{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression with a Neural Network in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to your first programming assignment! \n",
    "In this assignment you will build your first classification and regression neural networks.\n",
    "This assignment is a 'step-through' guide to implement a simple fully-connected neural network in Pytorch.\n",
    "\n",
    "* In the first part of this exercise, you will implement a neural network with a 2 dimensional input. Your dataset is based on a two <a href=https://en.wikipedia.org/wiki/Crescent> crescent </a> moon-shaped groups for classification.\n",
    "\n",
    "* Then, in the second part of this exercise, you will implement a regression model for predicting the output of a two dimensional function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "Let's first import all the packages that you will need during this part of assignment.\n",
    "\n",
    "Feel free to use another libraries if you want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datargs\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import attr\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter your Israeli ID?205783277\n"
     ]
    }
   ],
   "source": [
    "id_num_str = input(\"Please Enter your Israeli ID?\")\n",
    "if (len(id_num_str) != 9):\n",
    "    print('ID should contain 9 digits')\n",
    "if (id_num_str.isdigit() is False):\n",
    "    print('ID should contain only digits')\n",
    "id_num = list(id_num_str[-3:])\n",
    "random_num = sum(list(map(int, id_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_num)\n",
    "torch.manual_seed(random_num)\n",
    "x, y = make_moons(500, noise=0.2, random_state=random_num)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=random_num)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, random_state=random_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the dataset using matplotlib:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2074734d4c8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "colormap = np.array(['g', 'b'])\n",
    "plt.scatter(x[:, 0], x[:, 1], s=10, c=colormap[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you are required to do:\n",
    "\n",
    "1. Implement a 2-class classification neural network with a zero hidden layer.\n",
    "2. Plot loss vs epoch.\n",
    "3. Plot AUC vs epoch for train and test sets. \n",
    "4. Plot ROC curve and calculate AUC for the test set.\n",
    "5. Plot the learned decision boundary.\n",
    "6. Briefly interpret graph's results.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How you will do it:\n",
    "\n",
    "1. Prepare the Data.\n",
    "2. Define the Model.\n",
    "3. Train the Model.\n",
    "4. Evaluate the Model.\n",
    "5. Visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Args for the assigment\"\"\"\n",
    "\n",
    "# NN architecture args\n",
    "num_net_channels_part_a = [2, 1]\n",
    "num_net_channels_part_b = [2, 16, 1]\n",
    "num_net_channels_part_c_1 = [2, 128, 64, 32, 1]\n",
    "num_net_channels_part_c_2 = [2, 512, 256, 128, 64, 32, 1]\n",
    "# NN training args\n",
    "tr_batch_size = 50\n",
    "val_test_batch_size = 50\n",
    "num_epochs = 2000\n",
    "lr = 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Help function for the Nerual Network:**\n",
    "1. EarlyStopping\n",
    "2. TabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, features, labels):\n",
    "        'Initialization'\n",
    "        self.features = torch.Tensor(features).to(torch.float32)\n",
    "        self.labels = torch.Tensor(labels).to(torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        X = self.features[index, :]\n",
    "\n",
    "        # Load data and get label\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Model:**\n",
    "With this model we gonna use in all parts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpNetwork(nn.Module):\n",
    "    \"\"\"Our neural network\n",
    "    in_features: the number of input features\n",
    "    num_features: the number of features used in intermmidiate layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, activation_func, classifier=False):\n",
    "        super(MlpNetwork, self).__init__()\n",
    "        model_layers = [(nn.Linear(num_features[0], num_features[1]))]\n",
    "        for i in range(2, len(num_features)):\n",
    "            model_layers.append(activation_func)\n",
    "            model_layers.append(nn.Linear(num_features[i-1], num_features[i]))\n",
    "        if classifier:\n",
    "            model_layers.append(activation_func)\n",
    "        self.fc_module = nn.Sequential(*model_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_module(x)\n",
    "        return torch.flatten(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utils function for the nerual network:**\n",
    "1. create_generators\n",
    "2. infer\n",
    "3. test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generators(x_train, y_train, x_val, y_val, x_test, y_test):\n",
    "    train_set = TabularDataset(x_train, y_train)\n",
    "    val_set = TabularDataset(x_val, y_val)\n",
    "    test_set = TabularDataset(x_test, y_test)\n",
    "\n",
    "    training_generator = DataLoader(train_set, x_train.shape[0])\n",
    "    validation_generator = DataLoader(val_set, x_val.shape[0])\n",
    "    test_generator = DataLoader(test_set, x_test.shape[0])\n",
    "    return training_generator, validation_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(net, dataloader, criterion_func=nn.BCELoss):\n",
    "    \"\"\"\n",
    "    check the network before training\n",
    "    \"\"\"\n",
    "    criterion = criterion_func()\n",
    "    net.eval()\n",
    "    running_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        with torch.no_grad():\n",
    "            pred = net(x)\n",
    "            loss = criterion(pred, y)\n",
    "        running_loss += loss\n",
    "    return round((running_loss / len(dataloader)).item(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(net, x_test, y_test):\n",
    "    y_pred_p = net(torch.from_numpy(x_test).to(torch.float32))\n",
    "    y_pred_p = y_pred_p.detach().numpy()\n",
    "    y_pred = np.where(y_pred_p > 0.5, 1, 0).flatten()\n",
    "    print(round(sum(y_test == y_pred) / y_test.shape[0], 2))\n",
    "    return y_pred_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "def train(net, training_generator, validation_generator, verbose=False, opt_func=torch.optim.Adam,\n",
    "          criterion_func=nn.BCELoss, epochs=100, batch_size=10, lr=0.01,check_early_stopping=False, check_auc_roc=False):\n",
    "    \"\"\"\n",
    "    train the model\n",
    "    \"\"\"\n",
    "    \n",
    "    opt = opt_func(net.parameters(), lr=lr)\n",
    "    criterion = criterion_func()\n",
    "    early_stopping = EarlyStopping(patience=30, verbose=False, delta=1e-5)\n",
    "    \n",
    "    counter = 0\n",
    "    train_losses_per_epoch = []\n",
    "    val_losses_per_epoch = []\n",
    "    roc_auc_per_train = []\n",
    "    roc_auc_per_val = []\n",
    "    for e in range(epochs):\n",
    "        net.train()\n",
    "        train_losses = []\n",
    "        roc_auc_list = []\n",
    "        for inputs, targets in training_generator:\n",
    "            counter += 1\n",
    "            # zero accumulated gradients\n",
    "            opt.zero_grad()\n",
    "            # Forward\n",
    "            output= net(inputs)\n",
    "            if check_auc_roc:\n",
    "                false_positive_rate, recall, thresholds = roc_curve(targets.detach().numpy(), output.detach().numpy())\n",
    "                roc_auc = auc(false_positive_rate, recall)\n",
    "                roc_auc_list.append(roc_auc)\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets)\n",
    "            train_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        train_losses_per_epoch.append(np.mean(train_losses))\n",
    "        if check_auc_roc:\n",
    "            roc_auc_per_train.append(np.mean(roc_auc_list))    \n",
    "\n",
    "        val_losses = [] \n",
    "        roc_auc_val = []\n",
    "        net.eval()\n",
    "        for inputs, targets in validation_generator:\n",
    "            with torch.no_grad():\n",
    "                output= net(inputs)\n",
    "                val_loss = criterion(output, targets)\n",
    "            if check_auc_roc:\n",
    "                false_positive_rate, recall, thresholds = roc_curve(targets.detach().numpy(), output.detach().numpy())\n",
    "                roc_auc = auc(false_positive_rate, recall)\n",
    "                roc_auc_val.append(roc_auc)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        val_losses_per_epoch.append(np.mean(val_losses))\n",
    "        if check_auc_roc:\n",
    "            roc_auc_per_val.append(np.mean(roc_auc_val))\n",
    "        \n",
    "        early_stopping(np.average(val_losses), net)\n",
    "        if early_stopping.early_stop and check_early_stopping:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        if verbose:\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.5f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.5f}\".format(np.mean(val_losses)))\n",
    "        \n",
    "    if check_auc_roc:\n",
    "        return train_losses_per_epoch, val_losses_per_epoch, roc_auc_per_train, roc_auc_per_val\n",
    "    return train_losses_per_epoch, val_losses_per_epoch    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize function:**\n",
    "1. Loss of train and validation vs epochs\n",
    "2. Auc of train and validation vs epochs\n",
    "3. Roc cruve\n",
    "4. Decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_vs_epochs(train_losess, val_losses):\n",
    "    epochs_list = list(range(1, len(train_losess) + 1))\n",
    "    plt.plot(epochs_list, train_losess, label=\"Train loss\")\n",
    "    plt.plot(epochs_list, val_losses, label=\"Val loss\")\n",
    "    plt.ylim(ymin=0)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc_vs_epochs(roc_auc_train, roc_auc_val):\n",
    "    epochs_list = list(range(1, len(roc_auc_train) + 1))\n",
    "    plt.plot(epochs_list, roc_auc_train, label=\"Train\")\n",
    "    plt.plot(epochs_list, roc_auc_val, label=\"Validation\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_cruve(y_test, y_pred_p):\n",
    "    false_positive_rate, recall, thresholds = roc_curve(y_test, y_pred_p)\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "    print(\"The test auc is:\", round(roc_auc, 2))\n",
    "    plt.figure()\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1], [0,1], 'r--')\n",
    "    plt.xlim([0.0,1.0])\n",
    "    plt.ylim([0.0,1.0])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Fall-out (1-Specificity)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, X_test, Y_test, net):    \n",
    "    X_test_t = torch.FloatTensor(X_test)\n",
    "    y_hat_test = net(X_test_t)\n",
    "    y_hat_test_class = np.where(y_hat_test.detach().numpy() > 0.5, 1, 0)\n",
    "    test_accuracy = np.sum(Y_test.flatten()==y_hat_test_class) / len(Y_test)\n",
    "    print(\"Test Accuracy {:.2f}\".format(test_accuracy))\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    # Determine grid range in x and y directions\n",
    "    x_min, x_max = X[:, 0].min()-0.1, X[:, 0].max()+0.1\n",
    "    y_min, y_max = X[:, 1].min()-0.1, X[:, 1].max()+0.1\n",
    "\n",
    "    # Set grid spacing parameter\n",
    "    spacing = min(x_max - x_min, y_max - y_min) / 100\n",
    "\n",
    "    # Create grid\n",
    "    XX, YY = np.meshgrid(np.arange(x_min, x_max, spacing),\n",
    "                   np.arange(y_min, y_max, spacing))\n",
    "\n",
    "    # Concatenate data to match input\n",
    "    data = np.hstack((XX.ravel().reshape(-1,1), \n",
    "                      YY.ravel().reshape(-1,1)))\n",
    "\n",
    "    # Pass data to predict method\n",
    "    data_t = torch.FloatTensor(data)\n",
    "    db_prob = net(data_t)\n",
    "\n",
    "    clf = np.where(db_prob<0.5,0,1)\n",
    "\n",
    "    Z = clf.reshape(XX.shape)\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.contourf(XX, YY, Z, cmap=plt.cm.Accent, alpha=0.5)\n",
    "    plt.scatter(X_test[:,0], X_test[:,1], c=Y_test, \n",
    "                cmap=plt.cm.Accent)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result for Simple Logistic Regression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator, validation_generator, test_generator = create_generators(x_train, y_train, x_val, y_val, x_test,y_test)\n",
    "net = MlpNetwork(num_net_channels_part_a, nn.Sigmoid(), classifier=True)\n",
    "opt = torch.optim.Adam\n",
    "criterion = nn.BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before train on the Test set:\n",
      "0.8874\n",
      "Accuracy before train:\n",
      "0.21\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss before train on the Test set:\")\n",
    "print(infer(net, test_generator))\n",
    "print(\"Accuracy before train:\")\n",
    "_ = test_accuracy(net, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2000... Step: 1... Loss: 0.90161... Val Loss: 0.84911\n",
      "Epoch: 2/2000... Step: 2... Loss: 0.89490... Val Loss: 0.84348\n",
      "Epoch: 3/2000... Step: 3... Loss: 0.88824... Val Loss: 0.83788\n",
      "Epoch: 4/2000... Step: 4... Loss: 0.88163... Val Loss: 0.83233\n",
      "Epoch: 5/2000... Step: 5... Loss: 0.87507... Val Loss: 0.82681\n",
      "Epoch: 6/2000... Step: 6... Loss: 0.86855... Val Loss: 0.82134\n",
      "Epoch: 7/2000... Step: 7... Loss: 0.86209... Val Loss: 0.81590\n",
      "Epoch: 8/2000... Step: 8... Loss: 0.85567... Val Loss: 0.81050\n",
      "Epoch: 9/2000... Step: 9... Loss: 0.84931... Val Loss: 0.80515\n",
      "Epoch: 10/2000... Step: 10... Loss: 0.84299... Val Loss: 0.79984\n",
      "Epoch: 11/2000... Step: 11... Loss: 0.83674... Val Loss: 0.79457\n",
      "Epoch: 12/2000... Step: 12... Loss: 0.83053... Val Loss: 0.78934\n",
      "Epoch: 13/2000... Step: 13... Loss: 0.82438... Val Loss: 0.78416\n",
      "Epoch: 14/2000... Step: 14... Loss: 0.81829... Val Loss: 0.77902\n",
      "Epoch: 15/2000... Step: 15... Loss: 0.81225... Val Loss: 0.77393\n",
      "Epoch: 16/2000... Step: 16... Loss: 0.80627... Val Loss: 0.76888\n",
      "Epoch: 17/2000... Step: 17... Loss: 0.80035... Val Loss: 0.76388\n",
      "Epoch: 18/2000... Step: 18... Loss: 0.79449... Val Loss: 0.75892\n",
      "Epoch: 19/2000... Step: 19... Loss: 0.78868... Val Loss: 0.75400\n",
      "Epoch: 20/2000... Step: 20... Loss: 0.78294... Val Loss: 0.74914\n",
      "Epoch: 21/2000... Step: 21... Loss: 0.77726... Val Loss: 0.74432\n",
      "Epoch: 22/2000... Step: 22... Loss: 0.77163... Val Loss: 0.73954\n",
      "Epoch: 23/2000... Step: 23... Loss: 0.76607... Val Loss: 0.73481\n",
      "Epoch: 24/2000... Step: 24... Loss: 0.76057... Val Loss: 0.73013\n",
      "Epoch: 25/2000... Step: 25... Loss: 0.75513... Val Loss: 0.72549\n",
      "Epoch: 26/2000... Step: 26... Loss: 0.74975... Val Loss: 0.72090\n",
      "Epoch: 27/2000... Step: 27... Loss: 0.74443... Val Loss: 0.71635\n",
      "Epoch: 28/2000... Step: 28... Loss: 0.73918... Val Loss: 0.71185\n",
      "Epoch: 29/2000... Step: 29... Loss: 0.73398... Val Loss: 0.70740\n",
      "Epoch: 30/2000... Step: 30... Loss: 0.72885... Val Loss: 0.70299\n",
      "Epoch: 31/2000... Step: 31... Loss: 0.72378... Val Loss: 0.69863\n",
      "Epoch: 32/2000... Step: 32... Loss: 0.71877... Val Loss: 0.69431\n",
      "Epoch: 33/2000... Step: 33... Loss: 0.71382... Val Loss: 0.69003\n",
      "Epoch: 34/2000... Step: 34... Loss: 0.70893... Val Loss: 0.68580\n",
      "Epoch: 35/2000... Step: 35... Loss: 0.70410... Val Loss: 0.68162\n",
      "Epoch: 36/2000... Step: 36... Loss: 0.69934... Val Loss: 0.67748\n",
      "Epoch: 37/2000... Step: 37... Loss: 0.69463... Val Loss: 0.67338\n",
      "Epoch: 38/2000... Step: 38... Loss: 0.68998... Val Loss: 0.66932\n",
      "Epoch: 39/2000... Step: 39... Loss: 0.68539... Val Loss: 0.66531\n",
      "Epoch: 40/2000... Step: 40... Loss: 0.68086... Val Loss: 0.66134\n",
      "Epoch: 41/2000... Step: 41... Loss: 0.67639... Val Loss: 0.65741\n",
      "Epoch: 42/2000... Step: 42... Loss: 0.67198... Val Loss: 0.65352\n",
      "Epoch: 43/2000... Step: 43... Loss: 0.66762... Val Loss: 0.64968\n",
      "Epoch: 44/2000... Step: 44... Loss: 0.66332... Val Loss: 0.64587\n",
      "Epoch: 45/2000... Step: 45... Loss: 0.65908... Val Loss: 0.64211\n",
      "Epoch: 46/2000... Step: 46... Loss: 0.65489... Val Loss: 0.63838\n",
      "Epoch: 47/2000... Step: 47... Loss: 0.65075... Val Loss: 0.63470\n",
      "Epoch: 48/2000... Step: 48... Loss: 0.64667... Val Loss: 0.63105\n",
      "Epoch: 49/2000... Step: 49... Loss: 0.64265... Val Loss: 0.62745\n",
      "Epoch: 50/2000... Step: 50... Loss: 0.63868... Val Loss: 0.62388\n",
      "Epoch: 51/2000... Step: 51... Loss: 0.63476... Val Loss: 0.62035\n",
      "Epoch: 52/2000... Step: 52... Loss: 0.63089... Val Loss: 0.61685\n",
      "Epoch: 53/2000... Step: 53... Loss: 0.62707... Val Loss: 0.61340\n",
      "Epoch: 54/2000... Step: 54... Loss: 0.62330... Val Loss: 0.60998\n",
      "Epoch: 55/2000... Step: 55... Loss: 0.61959... Val Loss: 0.60660\n",
      "Epoch: 56/2000... Step: 56... Loss: 0.61592... Val Loss: 0.60325\n",
      "Epoch: 57/2000... Step: 57... Loss: 0.61230... Val Loss: 0.59994\n",
      "Epoch: 58/2000... Step: 58... Loss: 0.60873... Val Loss: 0.59667\n",
      "Epoch: 59/2000... Step: 59... Loss: 0.60520... Val Loss: 0.59343\n",
      "Epoch: 60/2000... Step: 60... Loss: 0.60172... Val Loss: 0.59022\n",
      "Epoch: 61/2000... Step: 61... Loss: 0.59829... Val Loss: 0.58705\n",
      "Epoch: 62/2000... Step: 62... Loss: 0.59491... Val Loss: 0.58392\n",
      "Epoch: 63/2000... Step: 63... Loss: 0.59157... Val Loss: 0.58082\n",
      "Epoch: 64/2000... Step: 64... Loss: 0.58827... Val Loss: 0.57775\n",
      "Epoch: 65/2000... Step: 65... Loss: 0.58502... Val Loss: 0.57472\n",
      "Epoch: 66/2000... Step: 66... Loss: 0.58181... Val Loss: 0.57172\n",
      "Epoch: 67/2000... Step: 67... Loss: 0.57864... Val Loss: 0.56875\n",
      "Epoch: 68/2000... Step: 68... Loss: 0.57551... Val Loss: 0.56582\n",
      "Epoch: 69/2000... Step: 69... Loss: 0.57243... Val Loss: 0.56292\n",
      "Epoch: 70/2000... Step: 70... Loss: 0.56938... Val Loss: 0.56005\n",
      "Epoch: 71/2000... Step: 71... Loss: 0.56638... Val Loss: 0.55722\n",
      "Epoch: 72/2000... Step: 72... Loss: 0.56341... Val Loss: 0.55441\n",
      "Epoch: 73/2000... Step: 73... Loss: 0.56049... Val Loss: 0.55164\n",
      "Epoch: 74/2000... Step: 74... Loss: 0.55760... Val Loss: 0.54890\n",
      "Epoch: 75/2000... Step: 75... Loss: 0.55475... Val Loss: 0.54619\n",
      "Epoch: 76/2000... Step: 76... Loss: 0.55194... Val Loss: 0.54352\n",
      "Epoch: 77/2000... Step: 77... Loss: 0.54916... Val Loss: 0.54087\n",
      "Epoch: 78/2000... Step: 78... Loss: 0.54642... Val Loss: 0.53826\n",
      "Epoch: 79/2000... Step: 79... Loss: 0.54372... Val Loss: 0.53567\n",
      "Epoch: 80/2000... Step: 80... Loss: 0.54105... Val Loss: 0.53312\n",
      "Epoch: 81/2000... Step: 81... Loss: 0.53842... Val Loss: 0.53059\n",
      "Epoch: 82/2000... Step: 82... Loss: 0.53582... Val Loss: 0.52810\n",
      "Epoch: 83/2000... Step: 83... Loss: 0.53326... Val Loss: 0.52563\n",
      "Epoch: 84/2000... Step: 84... Loss: 0.53073... Val Loss: 0.52320\n",
      "Epoch: 85/2000... Step: 85... Loss: 0.52823... Val Loss: 0.52079\n",
      "Epoch: 86/2000... Step: 86... Loss: 0.52576... Val Loss: 0.51841\n",
      "Epoch: 87/2000... Step: 87... Loss: 0.52333... Val Loss: 0.51606\n",
      "Epoch: 88/2000... Step: 88... Loss: 0.52093... Val Loss: 0.51374\n",
      "Epoch: 89/2000... Step: 89... Loss: 0.51856... Val Loss: 0.51145\n",
      "Epoch: 90/2000... Step: 90... Loss: 0.51622... Val Loss: 0.50919\n",
      "Epoch: 91/2000... Step: 91... Loss: 0.51391... Val Loss: 0.50695\n",
      "Epoch: 92/2000... Step: 92... Loss: 0.51163... Val Loss: 0.50474\n",
      "Epoch: 93/2000... Step: 93... Loss: 0.50938... Val Loss: 0.50256\n",
      "Epoch: 94/2000... Step: 94... Loss: 0.50716... Val Loss: 0.50040\n",
      "Epoch: 95/2000... Step: 95... Loss: 0.50497... Val Loss: 0.49827\n",
      "Epoch: 96/2000... Step: 96... Loss: 0.50280... Val Loss: 0.49617\n",
      "Epoch: 97/2000... Step: 97... Loss: 0.50067... Val Loss: 0.49409\n",
      "Epoch: 98/2000... Step: 98... Loss: 0.49856... Val Loss: 0.49204\n",
      "Epoch: 99/2000... Step: 99... Loss: 0.49648... Val Loss: 0.49002\n",
      "Epoch: 100/2000... Step: 100... Loss: 0.49442... Val Loss: 0.48802\n",
      "Epoch: 101/2000... Step: 101... Loss: 0.49239... Val Loss: 0.48604\n",
      "Epoch: 102/2000... Step: 102... Loss: 0.49039... Val Loss: 0.48409\n",
      "Epoch: 103/2000... Step: 103... Loss: 0.48841... Val Loss: 0.48217\n",
      "Epoch: 104/2000... Step: 104... Loss: 0.48646... Val Loss: 0.48027\n",
      "Epoch: 105/2000... Step: 105... Loss: 0.48453... Val Loss: 0.47839\n",
      "Epoch: 106/2000... Step: 106... Loss: 0.48263... Val Loss: 0.47653\n",
      "Epoch: 107/2000... Step: 107... Loss: 0.48075... Val Loss: 0.47470\n",
      "Epoch: 108/2000... Step: 108... Loss: 0.47890... Val Loss: 0.47289\n",
      "Epoch: 109/2000... Step: 109... Loss: 0.47706... Val Loss: 0.47111\n",
      "Epoch: 110/2000... Step: 110... Loss: 0.47525... Val Loss: 0.46934\n",
      "Epoch: 111/2000... Step: 111... Loss: 0.47347... Val Loss: 0.46760\n",
      "Epoch: 112/2000... Step: 112... Loss: 0.47170... Val Loss: 0.46588\n",
      "Epoch: 113/2000... Step: 113... Loss: 0.46996... Val Loss: 0.46418\n",
      "Epoch: 114/2000... Step: 114... Loss: 0.46824... Val Loss: 0.46250\n",
      "Epoch: 115/2000... Step: 115... Loss: 0.46654... Val Loss: 0.46085\n",
      "Epoch: 116/2000... Step: 116... Loss: 0.46486... Val Loss: 0.45921\n",
      "Epoch: 117/2000... Step: 117... Loss: 0.46321... Val Loss: 0.45759\n",
      "Epoch: 118/2000... Step: 118... Loss: 0.46157... Val Loss: 0.45600\n",
      "Epoch: 119/2000... Step: 119... Loss: 0.45995... Val Loss: 0.45442\n",
      "Epoch: 120/2000... Step: 120... Loss: 0.45835... Val Loss: 0.45286\n",
      "Epoch: 121/2000... Step: 121... Loss: 0.45678... Val Loss: 0.45132\n",
      "Epoch: 122/2000... Step: 122... Loss: 0.45522... Val Loss: 0.44980\n",
      "Epoch: 123/2000... Step: 123... Loss: 0.45368... Val Loss: 0.44830\n",
      "Epoch: 124/2000... Step: 124... Loss: 0.45216... Val Loss: 0.44682\n",
      "Epoch: 125/2000... Step: 125... Loss: 0.45065... Val Loss: 0.44535\n",
      "Epoch: 126/2000... Step: 126... Loss: 0.44917... Val Loss: 0.44391\n",
      "Epoch: 127/2000... Step: 127... Loss: 0.44770... Val Loss: 0.44248\n",
      "Epoch: 128/2000... Step: 128... Loss: 0.44625... Val Loss: 0.44106\n",
      "Epoch: 129/2000... Step: 129... Loss: 0.44482... Val Loss: 0.43967\n",
      "Epoch: 130/2000... Step: 130... Loss: 0.44340... Val Loss: 0.43829\n",
      "Epoch: 131/2000... Step: 131... Loss: 0.44200... Val Loss: 0.43693\n",
      "Epoch: 132/2000... Step: 132... Loss: 0.44062... Val Loss: 0.43558\n",
      "Epoch: 133/2000... Step: 133... Loss: 0.43926... Val Loss: 0.43425\n",
      "Epoch: 134/2000... Step: 134... Loss: 0.43791... Val Loss: 0.43294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 135/2000... Step: 135... Loss: 0.43657... Val Loss: 0.43164\n",
      "Epoch: 136/2000... Step: 136... Loss: 0.43525... Val Loss: 0.43035\n",
      "Epoch: 137/2000... Step: 137... Loss: 0.43395... Val Loss: 0.42908\n",
      "Epoch: 138/2000... Step: 138... Loss: 0.43266... Val Loss: 0.42783\n",
      "Epoch: 139/2000... Step: 139... Loss: 0.43139... Val Loss: 0.42659\n",
      "Epoch: 140/2000... Step: 140... Loss: 0.43013... Val Loss: 0.42536\n",
      "Epoch: 141/2000... Step: 141... Loss: 0.42889... Val Loss: 0.42415\n",
      "Epoch: 142/2000... Step: 142... Loss: 0.42766... Val Loss: 0.42295\n",
      "Epoch: 143/2000... Step: 143... Loss: 0.42644... Val Loss: 0.42177\n",
      "Epoch: 144/2000... Step: 144... Loss: 0.42524... Val Loss: 0.42060\n",
      "Epoch: 145/2000... Step: 145... Loss: 0.42405... Val Loss: 0.41944\n",
      "Epoch: 146/2000... Step: 146... Loss: 0.42287... Val Loss: 0.41830\n",
      "Epoch: 147/2000... Step: 147... Loss: 0.42171... Val Loss: 0.41717\n",
      "Epoch: 148/2000... Step: 148... Loss: 0.42056... Val Loss: 0.41605\n",
      "Epoch: 149/2000... Step: 149... Loss: 0.41942... Val Loss: 0.41494\n",
      "Epoch: 150/2000... Step: 150... Loss: 0.41830... Val Loss: 0.41385\n",
      "Epoch: 151/2000... Step: 151... Loss: 0.41719... Val Loss: 0.41277\n",
      "Epoch: 152/2000... Step: 152... Loss: 0.41609... Val Loss: 0.41170\n",
      "Epoch: 153/2000... Step: 153... Loss: 0.41500... Val Loss: 0.41064\n",
      "Epoch: 154/2000... Step: 154... Loss: 0.41393... Val Loss: 0.40959\n",
      "Epoch: 155/2000... Step: 155... Loss: 0.41286... Val Loss: 0.40856\n",
      "Epoch: 156/2000... Step: 156... Loss: 0.41181... Val Loss: 0.40753\n",
      "Epoch: 157/2000... Step: 157... Loss: 0.41077... Val Loss: 0.40652\n",
      "Epoch: 158/2000... Step: 158... Loss: 0.40974... Val Loss: 0.40552\n",
      "Epoch: 159/2000... Step: 159... Loss: 0.40872... Val Loss: 0.40453\n",
      "Epoch: 160/2000... Step: 160... Loss: 0.40771... Val Loss: 0.40355\n",
      "Epoch: 161/2000... Step: 161... Loss: 0.40672... Val Loss: 0.40257\n",
      "Epoch: 162/2000... Step: 162... Loss: 0.40573... Val Loss: 0.40161\n",
      "Epoch: 163/2000... Step: 163... Loss: 0.40475... Val Loss: 0.40066\n",
      "Epoch: 164/2000... Step: 164... Loss: 0.40379... Val Loss: 0.39972\n",
      "Epoch: 165/2000... Step: 165... Loss: 0.40283... Val Loss: 0.39879\n",
      "Epoch: 166/2000... Step: 166... Loss: 0.40188... Val Loss: 0.39787\n",
      "Epoch: 167/2000... Step: 167... Loss: 0.40095... Val Loss: 0.39696\n",
      "Epoch: 168/2000... Step: 168... Loss: 0.40002... Val Loss: 0.39606\n",
      "Epoch: 169/2000... Step: 169... Loss: 0.39911... Val Loss: 0.39517\n",
      "Epoch: 170/2000... Step: 170... Loss: 0.39820... Val Loss: 0.39429\n",
      "Epoch: 171/2000... Step: 171... Loss: 0.39730... Val Loss: 0.39341\n",
      "Epoch: 172/2000... Step: 172... Loss: 0.39641... Val Loss: 0.39255\n",
      "Epoch: 173/2000... Step: 173... Loss: 0.39553... Val Loss: 0.39169\n",
      "Epoch: 174/2000... Step: 174... Loss: 0.39466... Val Loss: 0.39084\n",
      "Epoch: 175/2000... Step: 175... Loss: 0.39380... Val Loss: 0.39000\n",
      "Epoch: 176/2000... Step: 176... Loss: 0.39294... Val Loss: 0.38917\n",
      "Epoch: 177/2000... Step: 177... Loss: 0.39210... Val Loss: 0.38835\n",
      "Epoch: 178/2000... Step: 178... Loss: 0.39126... Val Loss: 0.38753\n",
      "Epoch: 179/2000... Step: 179... Loss: 0.39043... Val Loss: 0.38673\n",
      "Epoch: 180/2000... Step: 180... Loss: 0.38961... Val Loss: 0.38593\n",
      "Epoch: 181/2000... Step: 181... Loss: 0.38880... Val Loss: 0.38514\n",
      "Epoch: 182/2000... Step: 182... Loss: 0.38799... Val Loss: 0.38435\n",
      "Epoch: 183/2000... Step: 183... Loss: 0.38720... Val Loss: 0.38358\n",
      "Epoch: 184/2000... Step: 184... Loss: 0.38641... Val Loss: 0.38281\n",
      "Epoch: 185/2000... Step: 185... Loss: 0.38563... Val Loss: 0.38205\n",
      "Epoch: 186/2000... Step: 186... Loss: 0.38485... Val Loss: 0.38129\n",
      "Epoch: 187/2000... Step: 187... Loss: 0.38409... Val Loss: 0.38055\n",
      "Epoch: 188/2000... Step: 188... Loss: 0.38333... Val Loss: 0.37981\n",
      "Epoch: 189/2000... Step: 189... Loss: 0.38257... Val Loss: 0.37907\n",
      "Epoch: 190/2000... Step: 190... Loss: 0.38183... Val Loss: 0.37835\n",
      "Epoch: 191/2000... Step: 191... Loss: 0.38109... Val Loss: 0.37763\n",
      "Epoch: 192/2000... Step: 192... Loss: 0.38036... Val Loss: 0.37692\n",
      "Epoch: 193/2000... Step: 193... Loss: 0.37964... Val Loss: 0.37621\n",
      "Epoch: 194/2000... Step: 194... Loss: 0.37892... Val Loss: 0.37551\n",
      "Epoch: 195/2000... Step: 195... Loss: 0.37821... Val Loss: 0.37482\n",
      "Epoch: 196/2000... Step: 196... Loss: 0.37750... Val Loss: 0.37413\n",
      "Epoch: 197/2000... Step: 197... Loss: 0.37680... Val Loss: 0.37345\n",
      "Epoch: 198/2000... Step: 198... Loss: 0.37611... Val Loss: 0.37278\n",
      "Epoch: 199/2000... Step: 199... Loss: 0.37543... Val Loss: 0.37211\n",
      "Epoch: 200/2000... Step: 200... Loss: 0.37475... Val Loss: 0.37145\n",
      "Epoch: 201/2000... Step: 201... Loss: 0.37408... Val Loss: 0.37079\n",
      "Epoch: 202/2000... Step: 202... Loss: 0.37341... Val Loss: 0.37014\n",
      "Epoch: 203/2000... Step: 203... Loss: 0.37275... Val Loss: 0.36950\n",
      "Epoch: 204/2000... Step: 204... Loss: 0.37209... Val Loss: 0.36886\n",
      "Epoch: 205/2000... Step: 205... Loss: 0.37144... Val Loss: 0.36823\n",
      "Epoch: 206/2000... Step: 206... Loss: 0.37080... Val Loss: 0.36760\n",
      "Epoch: 207/2000... Step: 207... Loss: 0.37016... Val Loss: 0.36698\n",
      "Epoch: 208/2000... Step: 208... Loss: 0.36953... Val Loss: 0.36636\n",
      "Epoch: 209/2000... Step: 209... Loss: 0.36890... Val Loss: 0.36575\n",
      "Epoch: 210/2000... Step: 210... Loss: 0.36828... Val Loss: 0.36514\n",
      "Epoch: 211/2000... Step: 211... Loss: 0.36766... Val Loss: 0.36454\n",
      "Epoch: 212/2000... Step: 212... Loss: 0.36705... Val Loss: 0.36395\n",
      "Epoch: 213/2000... Step: 213... Loss: 0.36645... Val Loss: 0.36335\n",
      "Epoch: 214/2000... Step: 214... Loss: 0.36585... Val Loss: 0.36277\n",
      "Epoch: 215/2000... Step: 215... Loss: 0.36525... Val Loss: 0.36219\n",
      "Epoch: 216/2000... Step: 216... Loss: 0.36466... Val Loss: 0.36161\n",
      "Epoch: 217/2000... Step: 217... Loss: 0.36407... Val Loss: 0.36104\n",
      "Epoch: 218/2000... Step: 218... Loss: 0.36349... Val Loss: 0.36047\n",
      "Epoch: 219/2000... Step: 219... Loss: 0.36292... Val Loss: 0.35991\n",
      "Epoch: 220/2000... Step: 220... Loss: 0.36235... Val Loss: 0.35935\n",
      "Epoch: 221/2000... Step: 221... Loss: 0.36178... Val Loss: 0.35880\n",
      "Epoch: 222/2000... Step: 222... Loss: 0.36122... Val Loss: 0.35825\n",
      "Epoch: 223/2000... Step: 223... Loss: 0.36066... Val Loss: 0.35771\n",
      "Epoch: 224/2000... Step: 224... Loss: 0.36011... Val Loss: 0.35717\n",
      "Epoch: 225/2000... Step: 225... Loss: 0.35956... Val Loss: 0.35663\n",
      "Epoch: 226/2000... Step: 226... Loss: 0.35902... Val Loss: 0.35610\n",
      "Epoch: 227/2000... Step: 227... Loss: 0.35848... Val Loss: 0.35558\n",
      "Epoch: 228/2000... Step: 228... Loss: 0.35794... Val Loss: 0.35505\n",
      "Epoch: 229/2000... Step: 229... Loss: 0.35741... Val Loss: 0.35454\n",
      "Epoch: 230/2000... Step: 230... Loss: 0.35689... Val Loss: 0.35402\n",
      "Epoch: 231/2000... Step: 231... Loss: 0.35636... Val Loss: 0.35351\n",
      "Epoch: 232/2000... Step: 232... Loss: 0.35585... Val Loss: 0.35300\n",
      "Epoch: 233/2000... Step: 233... Loss: 0.35533... Val Loss: 0.35250\n",
      "Epoch: 234/2000... Step: 234... Loss: 0.35482... Val Loss: 0.35200\n",
      "Epoch: 235/2000... Step: 235... Loss: 0.35432... Val Loss: 0.35151\n",
      "Epoch: 236/2000... Step: 236... Loss: 0.35381... Val Loss: 0.35102\n",
      "Epoch: 237/2000... Step: 237... Loss: 0.35331... Val Loss: 0.35053\n",
      "Epoch: 238/2000... Step: 238... Loss: 0.35282... Val Loss: 0.35005\n",
      "Epoch: 239/2000... Step: 239... Loss: 0.35233... Val Loss: 0.34957\n",
      "Epoch: 240/2000... Step: 240... Loss: 0.35184... Val Loss: 0.34909\n",
      "Epoch: 241/2000... Step: 241... Loss: 0.35136... Val Loss: 0.34862\n",
      "Epoch: 242/2000... Step: 242... Loss: 0.35088... Val Loss: 0.34815\n",
      "Epoch: 243/2000... Step: 243... Loss: 0.35040... Val Loss: 0.34768\n",
      "Epoch: 244/2000... Step: 244... Loss: 0.34993... Val Loss: 0.34722\n",
      "Epoch: 245/2000... Step: 245... Loss: 0.34946... Val Loss: 0.34676\n",
      "Epoch: 246/2000... Step: 246... Loss: 0.34900... Val Loss: 0.34630\n",
      "Epoch: 247/2000... Step: 247... Loss: 0.34853... Val Loss: 0.34585\n",
      "Epoch: 248/2000... Step: 248... Loss: 0.34807... Val Loss: 0.34540\n",
      "Epoch: 249/2000... Step: 249... Loss: 0.34762... Val Loss: 0.34495\n",
      "Epoch: 250/2000... Step: 250... Loss: 0.34717... Val Loss: 0.34451\n",
      "Epoch: 251/2000... Step: 251... Loss: 0.34672... Val Loss: 0.34407\n",
      "Epoch: 252/2000... Step: 252... Loss: 0.34627... Val Loss: 0.34363\n",
      "Epoch: 253/2000... Step: 253... Loss: 0.34583... Val Loss: 0.34320\n",
      "Epoch: 254/2000... Step: 254... Loss: 0.34539... Val Loss: 0.34277\n",
      "Epoch: 255/2000... Step: 255... Loss: 0.34495... Val Loss: 0.34234\n",
      "Epoch: 256/2000... Step: 256... Loss: 0.34452... Val Loss: 0.34192\n",
      "Epoch: 257/2000... Step: 257... Loss: 0.34409... Val Loss: 0.34150\n",
      "Epoch: 258/2000... Step: 258... Loss: 0.34366... Val Loss: 0.34108\n",
      "Epoch: 259/2000... Step: 259... Loss: 0.34324... Val Loss: 0.34066\n",
      "Epoch: 260/2000... Step: 260... Loss: 0.34282... Val Loss: 0.34025\n",
      "Epoch: 261/2000... Step: 261... Loss: 0.34240... Val Loss: 0.33984\n",
      "Epoch: 262/2000... Step: 262... Loss: 0.34199... Val Loss: 0.33943\n",
      "Epoch: 263/2000... Step: 263... Loss: 0.34157... Val Loss: 0.33902\n",
      "Epoch: 264/2000... Step: 264... Loss: 0.34116... Val Loss: 0.33862\n",
      "Epoch: 265/2000... Step: 265... Loss: 0.34076... Val Loss: 0.33822\n",
      "Epoch: 266/2000... Step: 266... Loss: 0.34035... Val Loss: 0.33783\n",
      "Epoch: 267/2000... Step: 267... Loss: 0.33995... Val Loss: 0.33743\n",
      "Epoch: 268/2000... Step: 268... Loss: 0.33955... Val Loss: 0.33704\n",
      "Epoch: 269/2000... Step: 269... Loss: 0.33916... Val Loss: 0.33665\n",
      "Epoch: 270/2000... Step: 270... Loss: 0.33877... Val Loss: 0.33626\n",
      "Epoch: 271/2000... Step: 271... Loss: 0.33838... Val Loss: 0.33588\n",
      "Epoch: 272/2000... Step: 272... Loss: 0.33799... Val Loss: 0.33550\n",
      "Epoch: 273/2000... Step: 273... Loss: 0.33760... Val Loss: 0.33512\n",
      "Epoch: 274/2000... Step: 274... Loss: 0.33722... Val Loss: 0.33474\n",
      "Epoch: 275/2000... Step: 275... Loss: 0.33684... Val Loss: 0.33437\n",
      "Epoch: 276/2000... Step: 276... Loss: 0.33646... Val Loss: 0.33399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 277/2000... Step: 277... Loss: 0.33609... Val Loss: 0.33362\n",
      "Epoch: 278/2000... Step: 278... Loss: 0.33571... Val Loss: 0.33326\n",
      "Epoch: 279/2000... Step: 279... Loss: 0.33534... Val Loss: 0.33289\n",
      "Epoch: 280/2000... Step: 280... Loss: 0.33497... Val Loss: 0.33253\n",
      "Epoch: 281/2000... Step: 281... Loss: 0.33461... Val Loss: 0.33217\n",
      "Epoch: 282/2000... Step: 282... Loss: 0.33424... Val Loss: 0.33181\n",
      "Epoch: 283/2000... Step: 283... Loss: 0.33388... Val Loss: 0.33145\n",
      "Epoch: 284/2000... Step: 284... Loss: 0.33352... Val Loss: 0.33110\n",
      "Epoch: 285/2000... Step: 285... Loss: 0.33317... Val Loss: 0.33075\n",
      "Epoch: 286/2000... Step: 286... Loss: 0.33281... Val Loss: 0.33040\n",
      "Epoch: 287/2000... Step: 287... Loss: 0.33246... Val Loss: 0.33005\n",
      "Epoch: 288/2000... Step: 288... Loss: 0.33211... Val Loss: 0.32970\n",
      "Epoch: 289/2000... Step: 289... Loss: 0.33176... Val Loss: 0.32936\n",
      "Epoch: 290/2000... Step: 290... Loss: 0.33142... Val Loss: 0.32902\n",
      "Epoch: 291/2000... Step: 291... Loss: 0.33107... Val Loss: 0.32868\n",
      "Epoch: 292/2000... Step: 292... Loss: 0.33073... Val Loss: 0.32834\n",
      "Epoch: 293/2000... Step: 293... Loss: 0.33039... Val Loss: 0.32800\n",
      "Epoch: 294/2000... Step: 294... Loss: 0.33006... Val Loss: 0.32767\n",
      "Epoch: 295/2000... Step: 295... Loss: 0.32972... Val Loss: 0.32734\n",
      "Epoch: 296/2000... Step: 296... Loss: 0.32939... Val Loss: 0.32701\n",
      "Epoch: 297/2000... Step: 297... Loss: 0.32906... Val Loss: 0.32668\n",
      "Epoch: 298/2000... Step: 298... Loss: 0.32873... Val Loss: 0.32635\n",
      "Epoch: 299/2000... Step: 299... Loss: 0.32840... Val Loss: 0.32603\n",
      "Epoch: 300/2000... Step: 300... Loss: 0.32808... Val Loss: 0.32571\n",
      "Epoch: 301/2000... Step: 301... Loss: 0.32775... Val Loss: 0.32539\n",
      "Epoch: 302/2000... Step: 302... Loss: 0.32743... Val Loss: 0.32507\n",
      "Epoch: 303/2000... Step: 303... Loss: 0.32711... Val Loss: 0.32475\n",
      "Epoch: 304/2000... Step: 304... Loss: 0.32679... Val Loss: 0.32444\n",
      "Epoch: 305/2000... Step: 305... Loss: 0.32648... Val Loss: 0.32412\n",
      "Epoch: 306/2000... Step: 306... Loss: 0.32616... Val Loss: 0.32381\n",
      "Epoch: 307/2000... Step: 307... Loss: 0.32585... Val Loss: 0.32350\n",
      "Epoch: 308/2000... Step: 308... Loss: 0.32554... Val Loss: 0.32319\n",
      "Epoch: 309/2000... Step: 309... Loss: 0.32523... Val Loss: 0.32288\n",
      "Epoch: 310/2000... Step: 310... Loss: 0.32492... Val Loss: 0.32258\n",
      "Epoch: 311/2000... Step: 311... Loss: 0.32462... Val Loss: 0.32228\n",
      "Epoch: 312/2000... Step: 312... Loss: 0.32432... Val Loss: 0.32197\n",
      "Epoch: 313/2000... Step: 313... Loss: 0.32401... Val Loss: 0.32167\n",
      "Epoch: 314/2000... Step: 314... Loss: 0.32371... Val Loss: 0.32137\n",
      "Epoch: 315/2000... Step: 315... Loss: 0.32342... Val Loss: 0.32108\n",
      "Epoch: 316/2000... Step: 316... Loss: 0.32312... Val Loss: 0.32078\n",
      "Epoch: 317/2000... Step: 317... Loss: 0.32282... Val Loss: 0.32049\n",
      "Epoch: 318/2000... Step: 318... Loss: 0.32253... Val Loss: 0.32019\n",
      "Epoch: 319/2000... Step: 319... Loss: 0.32224... Val Loss: 0.31990\n",
      "Epoch: 320/2000... Step: 320... Loss: 0.32195... Val Loss: 0.31961\n",
      "Epoch: 321/2000... Step: 321... Loss: 0.32166... Val Loss: 0.31933\n",
      "Epoch: 322/2000... Step: 322... Loss: 0.32137... Val Loss: 0.31904\n",
      "Epoch: 323/2000... Step: 323... Loss: 0.32109... Val Loss: 0.31875\n",
      "Epoch: 324/2000... Step: 324... Loss: 0.32081... Val Loss: 0.31847\n",
      "Epoch: 325/2000... Step: 325... Loss: 0.32052... Val Loss: 0.31819\n",
      "Epoch: 326/2000... Step: 326... Loss: 0.32024... Val Loss: 0.31791\n",
      "Epoch: 327/2000... Step: 327... Loss: 0.31996... Val Loss: 0.31763\n",
      "Epoch: 328/2000... Step: 328... Loss: 0.31969... Val Loss: 0.31735\n",
      "Epoch: 329/2000... Step: 329... Loss: 0.31941... Val Loss: 0.31707\n",
      "Epoch: 330/2000... Step: 330... Loss: 0.31913... Val Loss: 0.31680\n",
      "Epoch: 331/2000... Step: 331... Loss: 0.31886... Val Loss: 0.31652\n",
      "Epoch: 332/2000... Step: 332... Loss: 0.31859... Val Loss: 0.31625\n",
      "Epoch: 333/2000... Step: 333... Loss: 0.31832... Val Loss: 0.31598\n",
      "Epoch: 334/2000... Step: 334... Loss: 0.31805... Val Loss: 0.31571\n",
      "Epoch: 335/2000... Step: 335... Loss: 0.31778... Val Loss: 0.31544\n",
      "Epoch: 336/2000... Step: 336... Loss: 0.31752... Val Loss: 0.31518\n",
      "Epoch: 337/2000... Step: 337... Loss: 0.31725... Val Loss: 0.31491\n",
      "Epoch: 338/2000... Step: 338... Loss: 0.31699... Val Loss: 0.31464\n",
      "Epoch: 339/2000... Step: 339... Loss: 0.31673... Val Loss: 0.31438\n",
      "Epoch: 340/2000... Step: 340... Loss: 0.31646... Val Loss: 0.31412\n",
      "Epoch: 341/2000... Step: 341... Loss: 0.31621... Val Loss: 0.31386\n",
      "Epoch: 342/2000... Step: 342... Loss: 0.31595... Val Loss: 0.31360\n",
      "Epoch: 343/2000... Step: 343... Loss: 0.31569... Val Loss: 0.31334\n",
      "Epoch: 344/2000... Step: 344... Loss: 0.31544... Val Loss: 0.31308\n",
      "Epoch: 345/2000... Step: 345... Loss: 0.31518... Val Loss: 0.31283\n",
      "Epoch: 346/2000... Step: 346... Loss: 0.31493... Val Loss: 0.31257\n",
      "Epoch: 347/2000... Step: 347... Loss: 0.31468... Val Loss: 0.31232\n",
      "Epoch: 348/2000... Step: 348... Loss: 0.31443... Val Loss: 0.31207\n",
      "Epoch: 349/2000... Step: 349... Loss: 0.31418... Val Loss: 0.31181\n",
      "Epoch: 350/2000... Step: 350... Loss: 0.31393... Val Loss: 0.31156\n",
      "Epoch: 351/2000... Step: 351... Loss: 0.31368... Val Loss: 0.31131\n",
      "Epoch: 352/2000... Step: 352... Loss: 0.31344... Val Loss: 0.31107\n",
      "Epoch: 353/2000... Step: 353... Loss: 0.31319... Val Loss: 0.31082\n",
      "Epoch: 354/2000... Step: 354... Loss: 0.31295... Val Loss: 0.31057\n",
      "Epoch: 355/2000... Step: 355... Loss: 0.31271... Val Loss: 0.31033\n",
      "Epoch: 356/2000... Step: 356... Loss: 0.31247... Val Loss: 0.31009\n",
      "Epoch: 357/2000... Step: 357... Loss: 0.31223... Val Loss: 0.30984\n",
      "Epoch: 358/2000... Step: 358... Loss: 0.31199... Val Loss: 0.30960\n",
      "Epoch: 359/2000... Step: 359... Loss: 0.31175... Val Loss: 0.30936\n",
      "Epoch: 360/2000... Step: 360... Loss: 0.31152... Val Loss: 0.30912\n",
      "Epoch: 361/2000... Step: 361... Loss: 0.31128... Val Loss: 0.30888\n",
      "Epoch: 362/2000... Step: 362... Loss: 0.31105... Val Loss: 0.30865\n",
      "Epoch: 363/2000... Step: 363... Loss: 0.31082... Val Loss: 0.30841\n",
      "Epoch: 364/2000... Step: 364... Loss: 0.31059... Val Loss: 0.30818\n",
      "Epoch: 365/2000... Step: 365... Loss: 0.31035... Val Loss: 0.30794\n",
      "Epoch: 366/2000... Step: 366... Loss: 0.31013... Val Loss: 0.30771\n",
      "Epoch: 367/2000... Step: 367... Loss: 0.30990... Val Loss: 0.30748\n",
      "Epoch: 368/2000... Step: 368... Loss: 0.30967... Val Loss: 0.30725\n",
      "Epoch: 369/2000... Step: 369... Loss: 0.30944... Val Loss: 0.30702\n",
      "Epoch: 370/2000... Step: 370... Loss: 0.30922... Val Loss: 0.30679\n",
      "Epoch: 371/2000... Step: 371... Loss: 0.30900... Val Loss: 0.30656\n",
      "Epoch: 372/2000... Step: 372... Loss: 0.30877... Val Loss: 0.30633\n",
      "Epoch: 373/2000... Step: 373... Loss: 0.30855... Val Loss: 0.30610\n",
      "Epoch: 374/2000... Step: 374... Loss: 0.30833... Val Loss: 0.30588\n",
      "Epoch: 375/2000... Step: 375... Loss: 0.30811... Val Loss: 0.30565\n",
      "Epoch: 376/2000... Step: 376... Loss: 0.30789... Val Loss: 0.30543\n",
      "Epoch: 377/2000... Step: 377... Loss: 0.30767... Val Loss: 0.30521\n",
      "Epoch: 378/2000... Step: 378... Loss: 0.30746... Val Loss: 0.30499\n",
      "Epoch: 379/2000... Step: 379... Loss: 0.30724... Val Loss: 0.30477\n",
      "Epoch: 380/2000... Step: 380... Loss: 0.30703... Val Loss: 0.30455\n",
      "Epoch: 381/2000... Step: 381... Loss: 0.30681... Val Loss: 0.30433\n",
      "Epoch: 382/2000... Step: 382... Loss: 0.30660... Val Loss: 0.30411\n",
      "Epoch: 383/2000... Step: 383... Loss: 0.30639... Val Loss: 0.30389\n",
      "Epoch: 384/2000... Step: 384... Loss: 0.30618... Val Loss: 0.30368\n",
      "Epoch: 385/2000... Step: 385... Loss: 0.30597... Val Loss: 0.30346\n",
      "Epoch: 386/2000... Step: 386... Loss: 0.30576... Val Loss: 0.30325\n",
      "Epoch: 387/2000... Step: 387... Loss: 0.30555... Val Loss: 0.30303\n",
      "Epoch: 388/2000... Step: 388... Loss: 0.30534... Val Loss: 0.30282\n",
      "Epoch: 389/2000... Step: 389... Loss: 0.30514... Val Loss: 0.30261\n",
      "Epoch: 390/2000... Step: 390... Loss: 0.30493... Val Loss: 0.30240\n",
      "Epoch: 391/2000... Step: 391... Loss: 0.30473... Val Loss: 0.30219\n",
      "Epoch: 392/2000... Step: 392... Loss: 0.30453... Val Loss: 0.30198\n",
      "Epoch: 393/2000... Step: 393... Loss: 0.30432... Val Loss: 0.30177\n",
      "Epoch: 394/2000... Step: 394... Loss: 0.30412... Val Loss: 0.30156\n",
      "Epoch: 395/2000... Step: 395... Loss: 0.30392... Val Loss: 0.30135\n",
      "Epoch: 396/2000... Step: 396... Loss: 0.30372... Val Loss: 0.30115\n",
      "Epoch: 397/2000... Step: 397... Loss: 0.30352... Val Loss: 0.30094\n",
      "Epoch: 398/2000... Step: 398... Loss: 0.30332... Val Loss: 0.30074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 399/2000... Step: 399... Loss: 0.30313... Val Loss: 0.30053\n",
      "Epoch: 400/2000... Step: 400... Loss: 0.30293... Val Loss: 0.30033\n",
      "Epoch: 401/2000... Step: 401... Loss: 0.30273... Val Loss: 0.30013\n",
      "Epoch: 402/2000... Step: 402... Loss: 0.30254... Val Loss: 0.29993\n",
      "Epoch: 403/2000... Step: 403... Loss: 0.30235... Val Loss: 0.29973\n",
      "Epoch: 404/2000... Step: 404... Loss: 0.30215... Val Loss: 0.29953\n",
      "Epoch: 405/2000... Step: 405... Loss: 0.30196... Val Loss: 0.29933\n",
      "Epoch: 406/2000... Step: 406... Loss: 0.30177... Val Loss: 0.29913\n",
      "Epoch: 407/2000... Step: 407... Loss: 0.30158... Val Loss: 0.29893\n",
      "Epoch: 408/2000... Step: 408... Loss: 0.30139... Val Loss: 0.29873\n",
      "Epoch: 409/2000... Step: 409... Loss: 0.30120... Val Loss: 0.29854\n",
      "Epoch: 410/2000... Step: 410... Loss: 0.30101... Val Loss: 0.29834\n",
      "Epoch: 411/2000... Step: 411... Loss: 0.30083... Val Loss: 0.29815\n",
      "Epoch: 412/2000... Step: 412... Loss: 0.30064... Val Loss: 0.29795\n",
      "Epoch: 413/2000... Step: 413... Loss: 0.30045... Val Loss: 0.29776\n",
      "Epoch: 414/2000... Step: 414... Loss: 0.30027... Val Loss: 0.29757\n",
      "Epoch: 415/2000... Step: 415... Loss: 0.30008... Val Loss: 0.29738\n",
      "Epoch: 416/2000... Step: 416... Loss: 0.29990... Val Loss: 0.29719\n",
      "Epoch: 417/2000... Step: 417... Loss: 0.29972... Val Loss: 0.29699\n",
      "Epoch: 418/2000... Step: 418... Loss: 0.29954... Val Loss: 0.29681\n",
      "Epoch: 419/2000... Step: 419... Loss: 0.29935... Val Loss: 0.29662\n",
      "Epoch: 420/2000... Step: 420... Loss: 0.29917... Val Loss: 0.29643\n",
      "Epoch: 421/2000... Step: 421... Loss: 0.29900... Val Loss: 0.29624\n",
      "Epoch: 422/2000... Step: 422... Loss: 0.29882... Val Loss: 0.29605\n",
      "Epoch: 423/2000... Step: 423... Loss: 0.29864... Val Loss: 0.29587\n",
      "Epoch: 424/2000... Step: 424... Loss: 0.29846... Val Loss: 0.29568\n",
      "Epoch: 425/2000... Step: 425... Loss: 0.29828... Val Loss: 0.29550\n",
      "Epoch: 426/2000... Step: 426... Loss: 0.29811... Val Loss: 0.29531\n",
      "Epoch: 427/2000... Step: 427... Loss: 0.29793... Val Loss: 0.29513\n",
      "Epoch: 428/2000... Step: 428... Loss: 0.29776... Val Loss: 0.29494\n",
      "Epoch: 429/2000... Step: 429... Loss: 0.29759... Val Loss: 0.29476\n",
      "Epoch: 430/2000... Step: 430... Loss: 0.29741... Val Loss: 0.29458\n",
      "Epoch: 431/2000... Step: 431... Loss: 0.29724... Val Loss: 0.29440\n",
      "Epoch: 432/2000... Step: 432... Loss: 0.29707... Val Loss: 0.29422\n",
      "Epoch: 433/2000... Step: 433... Loss: 0.29690... Val Loss: 0.29404\n",
      "Epoch: 434/2000... Step: 434... Loss: 0.29673... Val Loss: 0.29386\n",
      "Epoch: 435/2000... Step: 435... Loss: 0.29656... Val Loss: 0.29368\n",
      "Epoch: 436/2000... Step: 436... Loss: 0.29639... Val Loss: 0.29350\n",
      "Epoch: 437/2000... Step: 437... Loss: 0.29622... Val Loss: 0.29333\n",
      "Epoch: 438/2000... Step: 438... Loss: 0.29605... Val Loss: 0.29315\n",
      "Epoch: 439/2000... Step: 439... Loss: 0.29589... Val Loss: 0.29297\n",
      "Epoch: 440/2000... Step: 440... Loss: 0.29572... Val Loss: 0.29280\n",
      "Epoch: 441/2000... Step: 441... Loss: 0.29555... Val Loss: 0.29262\n",
      "Epoch: 442/2000... Step: 442... Loss: 0.29539... Val Loss: 0.29245\n",
      "Epoch: 443/2000... Step: 443... Loss: 0.29523... Val Loss: 0.29227\n",
      "Epoch: 444/2000... Step: 444... Loss: 0.29506... Val Loss: 0.29210\n",
      "Epoch: 445/2000... Step: 445... Loss: 0.29490... Val Loss: 0.29193\n",
      "Epoch: 446/2000... Step: 446... Loss: 0.29474... Val Loss: 0.29176\n",
      "Epoch: 447/2000... Step: 447... Loss: 0.29458... Val Loss: 0.29159\n",
      "Epoch: 448/2000... Step: 448... Loss: 0.29441... Val Loss: 0.29142\n",
      "Epoch: 449/2000... Step: 449... Loss: 0.29425... Val Loss: 0.29125\n",
      "Epoch: 450/2000... Step: 450... Loss: 0.29409... Val Loss: 0.29108\n",
      "Epoch: 451/2000... Step: 451... Loss: 0.29394... Val Loss: 0.29091\n",
      "Epoch: 452/2000... Step: 452... Loss: 0.29378... Val Loss: 0.29074\n",
      "Epoch: 453/2000... Step: 453... Loss: 0.29362... Val Loss: 0.29057\n",
      "Epoch: 454/2000... Step: 454... Loss: 0.29346... Val Loss: 0.29040\n",
      "Epoch: 455/2000... Step: 455... Loss: 0.29331... Val Loss: 0.29024\n",
      "Epoch: 456/2000... Step: 456... Loss: 0.29315... Val Loss: 0.29007\n",
      "Epoch: 457/2000... Step: 457... Loss: 0.29299... Val Loss: 0.28990\n",
      "Epoch: 458/2000... Step: 458... Loss: 0.29284... Val Loss: 0.28974\n",
      "Epoch: 459/2000... Step: 459... Loss: 0.29269... Val Loss: 0.28957\n",
      "Epoch: 460/2000... Step: 460... Loss: 0.29253... Val Loss: 0.28941\n",
      "Epoch: 461/2000... Step: 461... Loss: 0.29238... Val Loss: 0.28925\n",
      "Epoch: 462/2000... Step: 462... Loss: 0.29223... Val Loss: 0.28908\n",
      "Epoch: 463/2000... Step: 463... Loss: 0.29208... Val Loss: 0.28892\n",
      "Epoch: 464/2000... Step: 464... Loss: 0.29192... Val Loss: 0.28876\n",
      "Epoch: 465/2000... Step: 465... Loss: 0.29177... Val Loss: 0.28860\n",
      "Epoch: 466/2000... Step: 466... Loss: 0.29162... Val Loss: 0.28844\n",
      "Epoch: 467/2000... Step: 467... Loss: 0.29147... Val Loss: 0.28828\n",
      "Epoch: 468/2000... Step: 468... Loss: 0.29133... Val Loss: 0.28812\n",
      "Epoch: 469/2000... Step: 469... Loss: 0.29118... Val Loss: 0.28796\n",
      "Epoch: 470/2000... Step: 470... Loss: 0.29103... Val Loss: 0.28780\n",
      "Epoch: 471/2000... Step: 471... Loss: 0.29088... Val Loss: 0.28764\n",
      "Epoch: 472/2000... Step: 472... Loss: 0.29074... Val Loss: 0.28748\n",
      "Epoch: 473/2000... Step: 473... Loss: 0.29059... Val Loss: 0.28733\n",
      "Epoch: 474/2000... Step: 474... Loss: 0.29044... Val Loss: 0.28717\n",
      "Epoch: 475/2000... Step: 475... Loss: 0.29030... Val Loss: 0.28701\n",
      "Epoch: 476/2000... Step: 476... Loss: 0.29015... Val Loss: 0.28686\n",
      "Epoch: 477/2000... Step: 477... Loss: 0.29001... Val Loss: 0.28670\n",
      "Epoch: 478/2000... Step: 478... Loss: 0.28987... Val Loss: 0.28655\n",
      "Epoch: 479/2000... Step: 479... Loss: 0.28973... Val Loss: 0.28639\n",
      "Epoch: 480/2000... Step: 480... Loss: 0.28958... Val Loss: 0.28624\n",
      "Epoch: 481/2000... Step: 481... Loss: 0.28944... Val Loss: 0.28609\n",
      "Epoch: 482/2000... Step: 482... Loss: 0.28930... Val Loss: 0.28593\n",
      "Epoch: 483/2000... Step: 483... Loss: 0.28916... Val Loss: 0.28578\n",
      "Epoch: 484/2000... Step: 484... Loss: 0.28902... Val Loss: 0.28563\n",
      "Epoch: 485/2000... Step: 485... Loss: 0.28888... Val Loss: 0.28548\n",
      "Epoch: 486/2000... Step: 486... Loss: 0.28874... Val Loss: 0.28533\n",
      "Epoch: 487/2000... Step: 487... Loss: 0.28860... Val Loss: 0.28518\n",
      "Epoch: 488/2000... Step: 488... Loss: 0.28846... Val Loss: 0.28503\n",
      "Epoch: 489/2000... Step: 489... Loss: 0.28833... Val Loss: 0.28488\n",
      "Epoch: 490/2000... Step: 490... Loss: 0.28819... Val Loss: 0.28473\n",
      "Epoch: 491/2000... Step: 491... Loss: 0.28805... Val Loss: 0.28458\n",
      "Epoch: 492/2000... Step: 492... Loss: 0.28792... Val Loss: 0.28443\n",
      "Epoch: 493/2000... Step: 493... Loss: 0.28778... Val Loss: 0.28429\n",
      "Epoch: 494/2000... Step: 494... Loss: 0.28765... Val Loss: 0.28414\n",
      "Epoch: 495/2000... Step: 495... Loss: 0.28751... Val Loss: 0.28399\n",
      "Epoch: 496/2000... Step: 496... Loss: 0.28738... Val Loss: 0.28385\n",
      "Epoch: 497/2000... Step: 497... Loss: 0.28725... Val Loss: 0.28370\n",
      "Epoch: 498/2000... Step: 498... Loss: 0.28711... Val Loss: 0.28356\n",
      "Epoch: 499/2000... Step: 499... Loss: 0.28698... Val Loss: 0.28341\n",
      "Epoch: 500/2000... Step: 500... Loss: 0.28685... Val Loss: 0.28327\n",
      "Epoch: 501/2000... Step: 501... Loss: 0.28672... Val Loss: 0.28312\n",
      "Epoch: 502/2000... Step: 502... Loss: 0.28659... Val Loss: 0.28298\n",
      "Epoch: 503/2000... Step: 503... Loss: 0.28646... Val Loss: 0.28284\n",
      "Epoch: 504/2000... Step: 504... Loss: 0.28633... Val Loss: 0.28269\n",
      "Epoch: 505/2000... Step: 505... Loss: 0.28620... Val Loss: 0.28255\n",
      "Epoch: 506/2000... Step: 506... Loss: 0.28607... Val Loss: 0.28241\n",
      "Epoch: 507/2000... Step: 507... Loss: 0.28594... Val Loss: 0.28227\n",
      "Epoch: 508/2000... Step: 508... Loss: 0.28581... Val Loss: 0.28213\n",
      "Epoch: 509/2000... Step: 509... Loss: 0.28568... Val Loss: 0.28199\n",
      "Epoch: 510/2000... Step: 510... Loss: 0.28556... Val Loss: 0.28185\n",
      "Epoch: 511/2000... Step: 511... Loss: 0.28543... Val Loss: 0.28171\n",
      "Epoch: 512/2000... Step: 512... Loss: 0.28530... Val Loss: 0.28157\n",
      "Epoch: 513/2000... Step: 513... Loss: 0.28518... Val Loss: 0.28143\n",
      "Epoch: 514/2000... Step: 514... Loss: 0.28505... Val Loss: 0.28129\n",
      "Epoch: 515/2000... Step: 515... Loss: 0.28493... Val Loss: 0.28115\n",
      "Epoch: 516/2000... Step: 516... Loss: 0.28480... Val Loss: 0.28102\n",
      "Epoch: 517/2000... Step: 517... Loss: 0.28468... Val Loss: 0.28088\n",
      "Epoch: 518/2000... Step: 518... Loss: 0.28455... Val Loss: 0.28074\n",
      "Epoch: 519/2000... Step: 519... Loss: 0.28443... Val Loss: 0.28061\n",
      "Epoch: 520/2000... Step: 520... Loss: 0.28431... Val Loss: 0.28047\n",
      "Epoch: 521/2000... Step: 521... Loss: 0.28419... Val Loss: 0.28033\n",
      "Epoch: 522/2000... Step: 522... Loss: 0.28406... Val Loss: 0.28020\n",
      "Epoch: 523/2000... Step: 523... Loss: 0.28394... Val Loss: 0.28007\n",
      "Epoch: 524/2000... Step: 524... Loss: 0.28382... Val Loss: 0.27993\n",
      "Epoch: 525/2000... Step: 525... Loss: 0.28370... Val Loss: 0.27980\n",
      "Epoch: 526/2000... Step: 526... Loss: 0.28358... Val Loss: 0.27966\n",
      "Epoch: 527/2000... Step: 527... Loss: 0.28346... Val Loss: 0.27953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 528/2000... Step: 528... Loss: 0.28334... Val Loss: 0.27940\n",
      "Epoch: 529/2000... Step: 529... Loss: 0.28322... Val Loss: 0.27927\n",
      "Epoch: 530/2000... Step: 530... Loss: 0.28311... Val Loss: 0.27913\n",
      "Epoch: 531/2000... Step: 531... Loss: 0.28299... Val Loss: 0.27900\n",
      "Epoch: 532/2000... Step: 532... Loss: 0.28287... Val Loss: 0.27887\n",
      "Epoch: 533/2000... Step: 533... Loss: 0.28275... Val Loss: 0.27874\n",
      "Epoch: 534/2000... Step: 534... Loss: 0.28264... Val Loss: 0.27861\n",
      "Epoch: 535/2000... Step: 535... Loss: 0.28252... Val Loss: 0.27848\n",
      "Epoch: 536/2000... Step: 536... Loss: 0.28241... Val Loss: 0.27835\n",
      "Epoch: 537/2000... Step: 537... Loss: 0.28229... Val Loss: 0.27822\n",
      "Epoch: 538/2000... Step: 538... Loss: 0.28218... Val Loss: 0.27809\n",
      "Epoch: 539/2000... Step: 539... Loss: 0.28206... Val Loss: 0.27797\n",
      "Epoch: 540/2000... Step: 540... Loss: 0.28195... Val Loss: 0.27784\n",
      "Epoch: 541/2000... Step: 541... Loss: 0.28183... Val Loss: 0.27771\n",
      "Epoch: 542/2000... Step: 542... Loss: 0.28172... Val Loss: 0.27758\n",
      "Epoch: 543/2000... Step: 543... Loss: 0.28161... Val Loss: 0.27746\n",
      "Epoch: 544/2000... Step: 544... Loss: 0.28149... Val Loss: 0.27733\n",
      "Epoch: 545/2000... Step: 545... Loss: 0.28138... Val Loss: 0.27720\n",
      "Epoch: 546/2000... Step: 546... Loss: 0.28127... Val Loss: 0.27708\n",
      "Epoch: 547/2000... Step: 547... Loss: 0.28116... Val Loss: 0.27695\n",
      "Epoch: 548/2000... Step: 548... Loss: 0.28105... Val Loss: 0.27683\n",
      "Epoch: 549/2000... Step: 549... Loss: 0.28094... Val Loss: 0.27670\n",
      "Epoch: 550/2000... Step: 550... Loss: 0.28083... Val Loss: 0.27658\n",
      "Epoch: 551/2000... Step: 551... Loss: 0.28072... Val Loss: 0.27646\n",
      "Epoch: 552/2000... Step: 552... Loss: 0.28061... Val Loss: 0.27633\n",
      "Epoch: 553/2000... Step: 553... Loss: 0.28050... Val Loss: 0.27621\n",
      "Epoch: 554/2000... Step: 554... Loss: 0.28039... Val Loss: 0.27609\n",
      "Epoch: 555/2000... Step: 555... Loss: 0.28028... Val Loss: 0.27596\n",
      "Epoch: 556/2000... Step: 556... Loss: 0.28017... Val Loss: 0.27584\n",
      "Epoch: 557/2000... Step: 557... Loss: 0.28007... Val Loss: 0.27572\n",
      "Epoch: 558/2000... Step: 558... Loss: 0.27996... Val Loss: 0.27560\n",
      "Epoch: 559/2000... Step: 559... Loss: 0.27985... Val Loss: 0.27548\n",
      "Epoch: 560/2000... Step: 560... Loss: 0.27975... Val Loss: 0.27536\n",
      "Epoch: 561/2000... Step: 561... Loss: 0.27964... Val Loss: 0.27524\n",
      "Epoch: 562/2000... Step: 562... Loss: 0.27953... Val Loss: 0.27512\n",
      "Epoch: 563/2000... Step: 563... Loss: 0.27943... Val Loss: 0.27500\n",
      "Epoch: 564/2000... Step: 564... Loss: 0.27932... Val Loss: 0.27488\n",
      "Epoch: 565/2000... Step: 565... Loss: 0.27922... Val Loss: 0.27476\n",
      "Epoch: 566/2000... Step: 566... Loss: 0.27912... Val Loss: 0.27464\n",
      "Epoch: 567/2000... Step: 567... Loss: 0.27901... Val Loss: 0.27452\n",
      "Epoch: 568/2000... Step: 568... Loss: 0.27891... Val Loss: 0.27440\n",
      "Epoch: 569/2000... Step: 569... Loss: 0.27881... Val Loss: 0.27429\n",
      "Epoch: 570/2000... Step: 570... Loss: 0.27870... Val Loss: 0.27417\n",
      "Epoch: 571/2000... Step: 571... Loss: 0.27860... Val Loss: 0.27405\n",
      "Epoch: 572/2000... Step: 572... Loss: 0.27850... Val Loss: 0.27394\n",
      "Epoch: 573/2000... Step: 573... Loss: 0.27840... Val Loss: 0.27382\n",
      "Epoch: 574/2000... Step: 574... Loss: 0.27830... Val Loss: 0.27370\n",
      "Epoch: 575/2000... Step: 575... Loss: 0.27819... Val Loss: 0.27359\n",
      "Epoch: 576/2000... Step: 576... Loss: 0.27809... Val Loss: 0.27347\n",
      "Epoch: 577/2000... Step: 577... Loss: 0.27799... Val Loss: 0.27336\n",
      "Epoch: 578/2000... Step: 578... Loss: 0.27789... Val Loss: 0.27324\n",
      "Epoch: 579/2000... Step: 579... Loss: 0.27779... Val Loss: 0.27313\n",
      "Epoch: 580/2000... Step: 580... Loss: 0.27770... Val Loss: 0.27302\n",
      "Epoch: 581/2000... Step: 581... Loss: 0.27760... Val Loss: 0.27290\n",
      "Epoch: 582/2000... Step: 582... Loss: 0.27750... Val Loss: 0.27279\n",
      "Epoch: 583/2000... Step: 583... Loss: 0.27740... Val Loss: 0.27268\n",
      "Epoch: 584/2000... Step: 584... Loss: 0.27730... Val Loss: 0.27256\n",
      "Epoch: 585/2000... Step: 585... Loss: 0.27720... Val Loss: 0.27245\n",
      "Epoch: 586/2000... Step: 586... Loss: 0.27711... Val Loss: 0.27234\n",
      "Epoch: 587/2000... Step: 587... Loss: 0.27701... Val Loss: 0.27223\n",
      "Epoch: 588/2000... Step: 588... Loss: 0.27691... Val Loss: 0.27212\n",
      "Epoch: 589/2000... Step: 589... Loss: 0.27682... Val Loss: 0.27201\n",
      "Epoch: 590/2000... Step: 590... Loss: 0.27672... Val Loss: 0.27190\n",
      "Epoch: 591/2000... Step: 591... Loss: 0.27663... Val Loss: 0.27179\n",
      "Epoch: 592/2000... Step: 592... Loss: 0.27653... Val Loss: 0.27168\n",
      "Epoch: 593/2000... Step: 593... Loss: 0.27644... Val Loss: 0.27157\n",
      "Epoch: 594/2000... Step: 594... Loss: 0.27634... Val Loss: 0.27146\n",
      "Epoch: 595/2000... Step: 595... Loss: 0.27625... Val Loss: 0.27135\n",
      "Epoch: 596/2000... Step: 596... Loss: 0.27615... Val Loss: 0.27124\n",
      "Epoch: 597/2000... Step: 597... Loss: 0.27606... Val Loss: 0.27113\n",
      "Epoch: 598/2000... Step: 598... Loss: 0.27597... Val Loss: 0.27102\n",
      "Epoch: 599/2000... Step: 599... Loss: 0.27587... Val Loss: 0.27091\n",
      "Epoch: 600/2000... Step: 600... Loss: 0.27578... Val Loss: 0.27081\n",
      "Epoch: 601/2000... Step: 601... Loss: 0.27569... Val Loss: 0.27070\n",
      "Epoch: 602/2000... Step: 602... Loss: 0.27560... Val Loss: 0.27059\n",
      "Epoch: 603/2000... Step: 603... Loss: 0.27551... Val Loss: 0.27049\n",
      "Epoch: 604/2000... Step: 604... Loss: 0.27541... Val Loss: 0.27038\n",
      "Epoch: 605/2000... Step: 605... Loss: 0.27532... Val Loss: 0.27027\n",
      "Epoch: 606/2000... Step: 606... Loss: 0.27523... Val Loss: 0.27017\n",
      "Epoch: 607/2000... Step: 607... Loss: 0.27514... Val Loss: 0.27006\n",
      "Epoch: 608/2000... Step: 608... Loss: 0.27505... Val Loss: 0.26996\n",
      "Epoch: 609/2000... Step: 609... Loss: 0.27496... Val Loss: 0.26985\n",
      "Epoch: 610/2000... Step: 610... Loss: 0.27487... Val Loss: 0.26975\n",
      "Epoch: 611/2000... Step: 611... Loss: 0.27478... Val Loss: 0.26964\n",
      "Epoch: 612/2000... Step: 612... Loss: 0.27470... Val Loss: 0.26954\n",
      "Epoch: 613/2000... Step: 613... Loss: 0.27461... Val Loss: 0.26944\n",
      "Epoch: 614/2000... Step: 614... Loss: 0.27452... Val Loss: 0.26933\n",
      "Epoch: 615/2000... Step: 615... Loss: 0.27443... Val Loss: 0.26923\n",
      "Epoch: 616/2000... Step: 616... Loss: 0.27434... Val Loss: 0.26913\n",
      "Epoch: 617/2000... Step: 617... Loss: 0.27426... Val Loss: 0.26902\n",
      "Epoch: 618/2000... Step: 618... Loss: 0.27417... Val Loss: 0.26892\n",
      "Epoch: 619/2000... Step: 619... Loss: 0.27408... Val Loss: 0.26882\n",
      "Epoch: 620/2000... Step: 620... Loss: 0.27400... Val Loss: 0.26872\n",
      "Epoch: 621/2000... Step: 621... Loss: 0.27391... Val Loss: 0.26862\n",
      "Epoch: 622/2000... Step: 622... Loss: 0.27382... Val Loss: 0.26852\n",
      "Epoch: 623/2000... Step: 623... Loss: 0.27374... Val Loss: 0.26841\n",
      "Epoch: 624/2000... Step: 624... Loss: 0.27365... Val Loss: 0.26831\n",
      "Epoch: 625/2000... Step: 625... Loss: 0.27357... Val Loss: 0.26821\n",
      "Epoch: 626/2000... Step: 626... Loss: 0.27348... Val Loss: 0.26811\n",
      "Epoch: 627/2000... Step: 627... Loss: 0.27340... Val Loss: 0.26801\n",
      "Epoch: 628/2000... Step: 628... Loss: 0.27331... Val Loss: 0.26791\n",
      "Epoch: 629/2000... Step: 629... Loss: 0.27323... Val Loss: 0.26782\n",
      "Epoch: 630/2000... Step: 630... Loss: 0.27315... Val Loss: 0.26772\n",
      "Epoch: 631/2000... Step: 631... Loss: 0.27306... Val Loss: 0.26762\n",
      "Epoch: 632/2000... Step: 632... Loss: 0.27298... Val Loss: 0.26752\n",
      "Epoch: 633/2000... Step: 633... Loss: 0.27290... Val Loss: 0.26742\n",
      "Epoch: 634/2000... Step: 634... Loss: 0.27282... Val Loss: 0.26732\n",
      "Epoch: 635/2000... Step: 635... Loss: 0.27273... Val Loss: 0.26723\n",
      "Epoch: 636/2000... Step: 636... Loss: 0.27265... Val Loss: 0.26713\n",
      "Epoch: 637/2000... Step: 637... Loss: 0.27257... Val Loss: 0.26703\n",
      "Epoch: 638/2000... Step: 638... Loss: 0.27249... Val Loss: 0.26694\n",
      "Epoch: 639/2000... Step: 639... Loss: 0.27241... Val Loss: 0.26684\n",
      "Epoch: 640/2000... Step: 640... Loss: 0.27233... Val Loss: 0.26674\n",
      "Epoch: 641/2000... Step: 641... Loss: 0.27225... Val Loss: 0.26665\n",
      "Epoch: 642/2000... Step: 642... Loss: 0.27217... Val Loss: 0.26655\n",
      "Epoch: 643/2000... Step: 643... Loss: 0.27209... Val Loss: 0.26646\n",
      "Epoch: 644/2000... Step: 644... Loss: 0.27201... Val Loss: 0.26636\n",
      "Epoch: 645/2000... Step: 645... Loss: 0.27193... Val Loss: 0.26627\n",
      "Epoch: 646/2000... Step: 646... Loss: 0.27185... Val Loss: 0.26617\n",
      "Epoch: 647/2000... Step: 647... Loss: 0.27177... Val Loss: 0.26608\n",
      "Epoch: 648/2000... Step: 648... Loss: 0.27169... Val Loss: 0.26598\n",
      "Epoch: 649/2000... Step: 649... Loss: 0.27161... Val Loss: 0.26589\n",
      "Epoch: 650/2000... Step: 650... Loss: 0.27153... Val Loss: 0.26580\n",
      "Epoch: 651/2000... Step: 651... Loss: 0.27146... Val Loss: 0.26570\n",
      "Epoch: 652/2000... Step: 652... Loss: 0.27138... Val Loss: 0.26561\n",
      "Epoch: 653/2000... Step: 653... Loss: 0.27130... Val Loss: 0.26552\n",
      "Epoch: 654/2000... Step: 654... Loss: 0.27122... Val Loss: 0.26543\n",
      "Epoch: 655/2000... Step: 655... Loss: 0.27115... Val Loss: 0.26533\n",
      "Epoch: 656/2000... Step: 656... Loss: 0.27107... Val Loss: 0.26524\n",
      "Epoch: 657/2000... Step: 657... Loss: 0.27099... Val Loss: 0.26515\n",
      "Epoch: 658/2000... Step: 658... Loss: 0.27092... Val Loss: 0.26506\n",
      "Epoch: 659/2000... Step: 659... Loss: 0.27084... Val Loss: 0.26497\n",
      "Epoch: 660/2000... Step: 660... Loss: 0.27077... Val Loss: 0.26488\n",
      "Epoch: 661/2000... Step: 661... Loss: 0.27069... Val Loss: 0.26478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 662/2000... Step: 662... Loss: 0.27062... Val Loss: 0.26469\n",
      "Epoch: 663/2000... Step: 663... Loss: 0.27054... Val Loss: 0.26460\n",
      "Epoch: 664/2000... Step: 664... Loss: 0.27047... Val Loss: 0.26451\n",
      "Epoch: 665/2000... Step: 665... Loss: 0.27039... Val Loss: 0.26442\n",
      "Epoch: 666/2000... Step: 666... Loss: 0.27032... Val Loss: 0.26434\n",
      "Epoch: 667/2000... Step: 667... Loss: 0.27025... Val Loss: 0.26425\n",
      "Epoch: 668/2000... Step: 668... Loss: 0.27017... Val Loss: 0.26416\n",
      "Epoch: 669/2000... Step: 669... Loss: 0.27010... Val Loss: 0.26407\n",
      "Epoch: 670/2000... Step: 670... Loss: 0.27003... Val Loss: 0.26398\n",
      "Epoch: 671/2000... Step: 671... Loss: 0.26995... Val Loss: 0.26389\n",
      "Epoch: 672/2000... Step: 672... Loss: 0.26988... Val Loss: 0.26380\n",
      "Epoch: 673/2000... Step: 673... Loss: 0.26981... Val Loss: 0.26372\n",
      "Epoch: 674/2000... Step: 674... Loss: 0.26974... Val Loss: 0.26363\n",
      "Epoch: 675/2000... Step: 675... Loss: 0.26966... Val Loss: 0.26354\n",
      "Epoch: 676/2000... Step: 676... Loss: 0.26959... Val Loss: 0.26345\n",
      "Epoch: 677/2000... Step: 677... Loss: 0.26952... Val Loss: 0.26337\n",
      "Epoch: 678/2000... Step: 678... Loss: 0.26945... Val Loss: 0.26328\n",
      "Epoch: 679/2000... Step: 679... Loss: 0.26938... Val Loss: 0.26320\n",
      "Epoch: 680/2000... Step: 680... Loss: 0.26931... Val Loss: 0.26311\n",
      "Epoch: 681/2000... Step: 681... Loss: 0.26924... Val Loss: 0.26302\n",
      "Epoch: 682/2000... Step: 682... Loss: 0.26917... Val Loss: 0.26294\n",
      "Epoch: 683/2000... Step: 683... Loss: 0.26910... Val Loss: 0.26285\n",
      "Epoch: 684/2000... Step: 684... Loss: 0.26903... Val Loss: 0.26277\n",
      "Epoch: 685/2000... Step: 685... Loss: 0.26896... Val Loss: 0.26268\n",
      "Epoch: 686/2000... Step: 686... Loss: 0.26889... Val Loss: 0.26260\n",
      "Epoch: 687/2000... Step: 687... Loss: 0.26882... Val Loss: 0.26251\n",
      "Epoch: 688/2000... Step: 688... Loss: 0.26875... Val Loss: 0.26243\n",
      "Epoch: 689/2000... Step: 689... Loss: 0.26868... Val Loss: 0.26235\n",
      "Epoch: 690/2000... Step: 690... Loss: 0.26861... Val Loss: 0.26226\n",
      "Epoch: 691/2000... Step: 691... Loss: 0.26855... Val Loss: 0.26218\n",
      "Epoch: 692/2000... Step: 692... Loss: 0.26848... Val Loss: 0.26209\n",
      "Epoch: 693/2000... Step: 693... Loss: 0.26841... Val Loss: 0.26201\n",
      "Epoch: 694/2000... Step: 694... Loss: 0.26834... Val Loss: 0.26193\n",
      "Epoch: 695/2000... Step: 695... Loss: 0.26828... Val Loss: 0.26185\n",
      "Epoch: 696/2000... Step: 696... Loss: 0.26821... Val Loss: 0.26176\n",
      "Epoch: 697/2000... Step: 697... Loss: 0.26814... Val Loss: 0.26168\n",
      "Epoch: 698/2000... Step: 698... Loss: 0.26808... Val Loss: 0.26160\n",
      "Epoch: 699/2000... Step: 699... Loss: 0.26801... Val Loss: 0.26152\n",
      "Epoch: 700/2000... Step: 700... Loss: 0.26794... Val Loss: 0.26144\n",
      "Epoch: 701/2000... Step: 701... Loss: 0.26788... Val Loss: 0.26136\n",
      "Epoch: 702/2000... Step: 702... Loss: 0.26781... Val Loss: 0.26128\n",
      "Epoch: 703/2000... Step: 703... Loss: 0.26775... Val Loss: 0.26119\n",
      "Epoch: 704/2000... Step: 704... Loss: 0.26768... Val Loss: 0.26111\n",
      "Epoch: 705/2000... Step: 705... Loss: 0.26762... Val Loss: 0.26103\n",
      "Epoch: 706/2000... Step: 706... Loss: 0.26755... Val Loss: 0.26095\n",
      "Epoch: 707/2000... Step: 707... Loss: 0.26749... Val Loss: 0.26087\n",
      "Epoch: 708/2000... Step: 708... Loss: 0.26742... Val Loss: 0.26079\n",
      "Epoch: 709/2000... Step: 709... Loss: 0.26736... Val Loss: 0.26071\n",
      "Epoch: 710/2000... Step: 710... Loss: 0.26729... Val Loss: 0.26064\n",
      "Epoch: 711/2000... Step: 711... Loss: 0.26723... Val Loss: 0.26056\n",
      "Epoch: 712/2000... Step: 712... Loss: 0.26717... Val Loss: 0.26048\n",
      "Epoch: 713/2000... Step: 713... Loss: 0.26710... Val Loss: 0.26040\n",
      "Epoch: 714/2000... Step: 714... Loss: 0.26704... Val Loss: 0.26032\n",
      "Epoch: 715/2000... Step: 715... Loss: 0.26698... Val Loss: 0.26024\n",
      "Epoch: 716/2000... Step: 716... Loss: 0.26692... Val Loss: 0.26016\n",
      "Epoch: 717/2000... Step: 717... Loss: 0.26685... Val Loss: 0.26009\n",
      "Epoch: 718/2000... Step: 718... Loss: 0.26679... Val Loss: 0.26001\n",
      "Epoch: 719/2000... Step: 719... Loss: 0.26673... Val Loss: 0.25993\n",
      "Epoch: 720/2000... Step: 720... Loss: 0.26667... Val Loss: 0.25986\n",
      "Epoch: 721/2000... Step: 721... Loss: 0.26660... Val Loss: 0.25978\n",
      "Epoch: 722/2000... Step: 722... Loss: 0.26654... Val Loss: 0.25970\n",
      "Epoch: 723/2000... Step: 723... Loss: 0.26648... Val Loss: 0.25963\n",
      "Epoch: 724/2000... Step: 724... Loss: 0.26642... Val Loss: 0.25955\n",
      "Epoch: 725/2000... Step: 725... Loss: 0.26636... Val Loss: 0.25947\n",
      "Epoch: 726/2000... Step: 726... Loss: 0.26630... Val Loss: 0.25940\n",
      "Epoch: 727/2000... Step: 727... Loss: 0.26624... Val Loss: 0.25932\n",
      "Epoch: 728/2000... Step: 728... Loss: 0.26618... Val Loss: 0.25925\n",
      "Epoch: 729/2000... Step: 729... Loss: 0.26612... Val Loss: 0.25917\n",
      "Epoch: 730/2000... Step: 730... Loss: 0.26606... Val Loss: 0.25910\n",
      "Epoch: 731/2000... Step: 731... Loss: 0.26600... Val Loss: 0.25902\n",
      "Epoch: 732/2000... Step: 732... Loss: 0.26594... Val Loss: 0.25895\n",
      "Epoch: 733/2000... Step: 733... Loss: 0.26588... Val Loss: 0.25887\n",
      "Epoch: 734/2000... Step: 734... Loss: 0.26582... Val Loss: 0.25880\n",
      "Epoch: 735/2000... Step: 735... Loss: 0.26576... Val Loss: 0.25872\n",
      "Epoch: 736/2000... Step: 736... Loss: 0.26570... Val Loss: 0.25865\n",
      "Epoch: 737/2000... Step: 737... Loss: 0.26565... Val Loss: 0.25858\n",
      "Epoch: 738/2000... Step: 738... Loss: 0.26559... Val Loss: 0.25850\n",
      "Epoch: 739/2000... Step: 739... Loss: 0.26553... Val Loss: 0.25843\n",
      "Epoch: 740/2000... Step: 740... Loss: 0.26547... Val Loss: 0.25836\n",
      "Epoch: 741/2000... Step: 741... Loss: 0.26541... Val Loss: 0.25828\n",
      "Epoch: 742/2000... Step: 742... Loss: 0.26536... Val Loss: 0.25821\n",
      "Epoch: 743/2000... Step: 743... Loss: 0.26530... Val Loss: 0.25814\n",
      "Epoch: 744/2000... Step: 744... Loss: 0.26524... Val Loss: 0.25807\n",
      "Epoch: 745/2000... Step: 745... Loss: 0.26518... Val Loss: 0.25800\n",
      "Epoch: 746/2000... Step: 746... Loss: 0.26513... Val Loss: 0.25792\n",
      "Epoch: 747/2000... Step: 747... Loss: 0.26507... Val Loss: 0.25785\n",
      "Epoch: 748/2000... Step: 748... Loss: 0.26501... Val Loss: 0.25778\n",
      "Epoch: 749/2000... Step: 749... Loss: 0.26496... Val Loss: 0.25771\n",
      "Epoch: 750/2000... Step: 750... Loss: 0.26490... Val Loss: 0.25764\n",
      "Epoch: 751/2000... Step: 751... Loss: 0.26485... Val Loss: 0.25757\n",
      "Epoch: 752/2000... Step: 752... Loss: 0.26479... Val Loss: 0.25750\n",
      "Epoch: 753/2000... Step: 753... Loss: 0.26473... Val Loss: 0.25743\n",
      "Epoch: 754/2000... Step: 754... Loss: 0.26468... Val Loss: 0.25736\n",
      "Epoch: 755/2000... Step: 755... Loss: 0.26462... Val Loss: 0.25729\n",
      "Epoch: 756/2000... Step: 756... Loss: 0.26457... Val Loss: 0.25722\n",
      "Epoch: 757/2000... Step: 757... Loss: 0.26451... Val Loss: 0.25715\n",
      "Epoch: 758/2000... Step: 758... Loss: 0.26446... Val Loss: 0.25708\n",
      "Epoch: 759/2000... Step: 759... Loss: 0.26441... Val Loss: 0.25701\n",
      "Epoch: 760/2000... Step: 760... Loss: 0.26435... Val Loss: 0.25694\n",
      "Epoch: 761/2000... Step: 761... Loss: 0.26430... Val Loss: 0.25687\n",
      "Epoch: 762/2000... Step: 762... Loss: 0.26424... Val Loss: 0.25680\n",
      "Epoch: 763/2000... Step: 763... Loss: 0.26419... Val Loss: 0.25673\n",
      "Epoch: 764/2000... Step: 764... Loss: 0.26414... Val Loss: 0.25666\n",
      "Epoch: 765/2000... Step: 765... Loss: 0.26408... Val Loss: 0.25660\n",
      "Epoch: 766/2000... Step: 766... Loss: 0.26403... Val Loss: 0.25653\n",
      "Epoch: 767/2000... Step: 767... Loss: 0.26398... Val Loss: 0.25646\n",
      "Epoch: 768/2000... Step: 768... Loss: 0.26392... Val Loss: 0.25639\n",
      "Epoch: 769/2000... Step: 769... Loss: 0.26387... Val Loss: 0.25633\n",
      "Epoch: 770/2000... Step: 770... Loss: 0.26382... Val Loss: 0.25626\n",
      "Epoch: 771/2000... Step: 771... Loss: 0.26377... Val Loss: 0.25619\n",
      "Epoch: 772/2000... Step: 772... Loss: 0.26371... Val Loss: 0.25612\n",
      "Epoch: 773/2000... Step: 773... Loss: 0.26366... Val Loss: 0.25606\n",
      "Epoch: 774/2000... Step: 774... Loss: 0.26361... Val Loss: 0.25599\n",
      "Epoch: 775/2000... Step: 775... Loss: 0.26356... Val Loss: 0.25593\n",
      "Epoch: 776/2000... Step: 776... Loss: 0.26351... Val Loss: 0.25586\n",
      "Epoch: 777/2000... Step: 777... Loss: 0.26346... Val Loss: 0.25579\n",
      "Epoch: 778/2000... Step: 778... Loss: 0.26340... Val Loss: 0.25573\n",
      "Epoch: 779/2000... Step: 779... Loss: 0.26335... Val Loss: 0.25566\n",
      "Epoch: 780/2000... Step: 780... Loss: 0.26330... Val Loss: 0.25560\n",
      "Epoch: 781/2000... Step: 781... Loss: 0.26325... Val Loss: 0.25553\n",
      "Epoch: 782/2000... Step: 782... Loss: 0.26320... Val Loss: 0.25547\n",
      "Epoch: 783/2000... Step: 783... Loss: 0.26315... Val Loss: 0.25540\n",
      "Epoch: 784/2000... Step: 784... Loss: 0.26310... Val Loss: 0.25534\n",
      "Epoch: 785/2000... Step: 785... Loss: 0.26305... Val Loss: 0.25527\n",
      "Epoch: 786/2000... Step: 786... Loss: 0.26300... Val Loss: 0.25521\n",
      "Epoch: 787/2000... Step: 787... Loss: 0.26295... Val Loss: 0.25514\n",
      "Epoch: 788/2000... Step: 788... Loss: 0.26290... Val Loss: 0.25508\n",
      "Epoch: 789/2000... Step: 789... Loss: 0.26285... Val Loss: 0.25502\n",
      "Epoch: 790/2000... Step: 790... Loss: 0.26280... Val Loss: 0.25495\n",
      "Epoch: 791/2000... Step: 791... Loss: 0.26275... Val Loss: 0.25489\n",
      "Epoch: 792/2000... Step: 792... Loss: 0.26271... Val Loss: 0.25482\n",
      "Epoch: 793/2000... Step: 793... Loss: 0.26266... Val Loss: 0.25476\n",
      "Epoch: 794/2000... Step: 794... Loss: 0.26261... Val Loss: 0.25470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 795/2000... Step: 795... Loss: 0.26256... Val Loss: 0.25464\n",
      "Epoch: 796/2000... Step: 796... Loss: 0.26251... Val Loss: 0.25457\n",
      "Epoch: 797/2000... Step: 797... Loss: 0.26246... Val Loss: 0.25451\n",
      "Epoch: 798/2000... Step: 798... Loss: 0.26242... Val Loss: 0.25445\n",
      "Epoch: 799/2000... Step: 799... Loss: 0.26237... Val Loss: 0.25439\n",
      "Epoch: 800/2000... Step: 800... Loss: 0.26232... Val Loss: 0.25432\n",
      "Epoch: 801/2000... Step: 801... Loss: 0.26227... Val Loss: 0.25426\n",
      "Epoch: 802/2000... Step: 802... Loss: 0.26223... Val Loss: 0.25420\n",
      "Epoch: 803/2000... Step: 803... Loss: 0.26218... Val Loss: 0.25414\n",
      "Epoch: 804/2000... Step: 804... Loss: 0.26213... Val Loss: 0.25408\n",
      "Epoch: 805/2000... Step: 805... Loss: 0.26208... Val Loss: 0.25402\n",
      "Epoch: 806/2000... Step: 806... Loss: 0.26204... Val Loss: 0.25396\n",
      "Epoch: 807/2000... Step: 807... Loss: 0.26199... Val Loss: 0.25390\n",
      "Epoch: 808/2000... Step: 808... Loss: 0.26194... Val Loss: 0.25383\n",
      "Epoch: 809/2000... Step: 809... Loss: 0.26190... Val Loss: 0.25377\n",
      "Epoch: 810/2000... Step: 810... Loss: 0.26185... Val Loss: 0.25371\n",
      "Epoch: 811/2000... Step: 811... Loss: 0.26181... Val Loss: 0.25365\n",
      "Epoch: 812/2000... Step: 812... Loss: 0.26176... Val Loss: 0.25359\n",
      "Epoch: 813/2000... Step: 813... Loss: 0.26171... Val Loss: 0.25353\n",
      "Epoch: 814/2000... Step: 814... Loss: 0.26167... Val Loss: 0.25347\n",
      "Epoch: 815/2000... Step: 815... Loss: 0.26162... Val Loss: 0.25342\n",
      "Epoch: 816/2000... Step: 816... Loss: 0.26158... Val Loss: 0.25336\n",
      "Epoch: 817/2000... Step: 817... Loss: 0.26153... Val Loss: 0.25330\n",
      "Epoch: 818/2000... Step: 818... Loss: 0.26149... Val Loss: 0.25324\n",
      "Epoch: 819/2000... Step: 819... Loss: 0.26144... Val Loss: 0.25318\n",
      "Epoch: 820/2000... Step: 820... Loss: 0.26140... Val Loss: 0.25312\n",
      "Epoch: 821/2000... Step: 821... Loss: 0.26135... Val Loss: 0.25306\n",
      "Epoch: 822/2000... Step: 822... Loss: 0.26131... Val Loss: 0.25300\n",
      "Epoch: 823/2000... Step: 823... Loss: 0.26127... Val Loss: 0.25294\n",
      "Epoch: 824/2000... Step: 824... Loss: 0.26122... Val Loss: 0.25289\n",
      "Epoch: 825/2000... Step: 825... Loss: 0.26118... Val Loss: 0.25283\n",
      "Epoch: 826/2000... Step: 826... Loss: 0.26113... Val Loss: 0.25277\n",
      "Epoch: 827/2000... Step: 827... Loss: 0.26109... Val Loss: 0.25271\n",
      "Epoch: 828/2000... Step: 828... Loss: 0.26105... Val Loss: 0.25266\n",
      "Epoch: 829/2000... Step: 829... Loss: 0.26100... Val Loss: 0.25260\n",
      "Epoch: 830/2000... Step: 830... Loss: 0.26096... Val Loss: 0.25254\n",
      "Epoch: 831/2000... Step: 831... Loss: 0.26092... Val Loss: 0.25248\n",
      "Epoch: 832/2000... Step: 832... Loss: 0.26087... Val Loss: 0.25243\n",
      "Epoch: 833/2000... Step: 833... Loss: 0.26083... Val Loss: 0.25237\n",
      "Epoch: 834/2000... Step: 834... Loss: 0.26079... Val Loss: 0.25231\n",
      "Epoch: 835/2000... Step: 835... Loss: 0.26075... Val Loss: 0.25226\n",
      "Epoch: 836/2000... Step: 836... Loss: 0.26070... Val Loss: 0.25220\n",
      "Epoch: 837/2000... Step: 837... Loss: 0.26066... Val Loss: 0.25215\n",
      "Epoch: 838/2000... Step: 838... Loss: 0.26062... Val Loss: 0.25209\n",
      "Epoch: 839/2000... Step: 839... Loss: 0.26058... Val Loss: 0.25203\n",
      "Epoch: 840/2000... Step: 840... Loss: 0.26054... Val Loss: 0.25198\n",
      "Epoch: 841/2000... Step: 841... Loss: 0.26049... Val Loss: 0.25192\n",
      "Epoch: 842/2000... Step: 842... Loss: 0.26045... Val Loss: 0.25187\n",
      "Epoch: 843/2000... Step: 843... Loss: 0.26041... Val Loss: 0.25181\n",
      "Epoch: 844/2000... Step: 844... Loss: 0.26037... Val Loss: 0.25176\n",
      "Epoch: 845/2000... Step: 845... Loss: 0.26033... Val Loss: 0.25170\n",
      "Epoch: 846/2000... Step: 846... Loss: 0.26029... Val Loss: 0.25165\n",
      "Epoch: 847/2000... Step: 847... Loss: 0.26025... Val Loss: 0.25159\n",
      "Epoch: 848/2000... Step: 848... Loss: 0.26021... Val Loss: 0.25154\n",
      "Epoch: 849/2000... Step: 849... Loss: 0.26017... Val Loss: 0.25149\n",
      "Epoch: 850/2000... Step: 850... Loss: 0.26012... Val Loss: 0.25143\n",
      "Epoch: 851/2000... Step: 851... Loss: 0.26008... Val Loss: 0.25138\n",
      "Epoch: 852/2000... Step: 852... Loss: 0.26004... Val Loss: 0.25132\n",
      "Epoch: 853/2000... Step: 853... Loss: 0.26000... Val Loss: 0.25127\n",
      "Epoch: 854/2000... Step: 854... Loss: 0.25996... Val Loss: 0.25122\n",
      "Epoch: 855/2000... Step: 855... Loss: 0.25992... Val Loss: 0.25116\n",
      "Epoch: 856/2000... Step: 856... Loss: 0.25988... Val Loss: 0.25111\n",
      "Epoch: 857/2000... Step: 857... Loss: 0.25984... Val Loss: 0.25106\n",
      "Epoch: 858/2000... Step: 858... Loss: 0.25981... Val Loss: 0.25100\n",
      "Epoch: 859/2000... Step: 859... Loss: 0.25977... Val Loss: 0.25095\n",
      "Epoch: 860/2000... Step: 860... Loss: 0.25973... Val Loss: 0.25090\n",
      "Epoch: 861/2000... Step: 861... Loss: 0.25969... Val Loss: 0.25085\n",
      "Epoch: 862/2000... Step: 862... Loss: 0.25965... Val Loss: 0.25079\n",
      "Epoch: 863/2000... Step: 863... Loss: 0.25961... Val Loss: 0.25074\n",
      "Epoch: 864/2000... Step: 864... Loss: 0.25957... Val Loss: 0.25069\n",
      "Epoch: 865/2000... Step: 865... Loss: 0.25953... Val Loss: 0.25064\n",
      "Epoch: 866/2000... Step: 866... Loss: 0.25949... Val Loss: 0.25058\n",
      "Epoch: 867/2000... Step: 867... Loss: 0.25946... Val Loss: 0.25053\n",
      "Epoch: 868/2000... Step: 868... Loss: 0.25942... Val Loss: 0.25048\n",
      "Epoch: 869/2000... Step: 869... Loss: 0.25938... Val Loss: 0.25043\n",
      "Epoch: 870/2000... Step: 870... Loss: 0.25934... Val Loss: 0.25038\n",
      "Epoch: 871/2000... Step: 871... Loss: 0.25930... Val Loss: 0.25033\n",
      "Epoch: 872/2000... Step: 872... Loss: 0.25927... Val Loss: 0.25028\n",
      "Epoch: 873/2000... Step: 873... Loss: 0.25923... Val Loss: 0.25023\n",
      "Epoch: 874/2000... Step: 874... Loss: 0.25919... Val Loss: 0.25017\n",
      "Epoch: 875/2000... Step: 875... Loss: 0.25915... Val Loss: 0.25012\n",
      "Epoch: 876/2000... Step: 876... Loss: 0.25912... Val Loss: 0.25007\n",
      "Epoch: 877/2000... Step: 877... Loss: 0.25908... Val Loss: 0.25002\n",
      "Epoch: 878/2000... Step: 878... Loss: 0.25904... Val Loss: 0.24997\n",
      "Epoch: 879/2000... Step: 879... Loss: 0.25900... Val Loss: 0.24992\n",
      "Epoch: 880/2000... Step: 880... Loss: 0.25897... Val Loss: 0.24987\n",
      "Epoch: 881/2000... Step: 881... Loss: 0.25893... Val Loss: 0.24982\n",
      "Epoch: 882/2000... Step: 882... Loss: 0.25889... Val Loss: 0.24977\n",
      "Epoch: 883/2000... Step: 883... Loss: 0.25886... Val Loss: 0.24972\n",
      "Epoch: 884/2000... Step: 884... Loss: 0.25882... Val Loss: 0.24967\n",
      "Epoch: 885/2000... Step: 885... Loss: 0.25879... Val Loss: 0.24963\n",
      "Epoch: 886/2000... Step: 886... Loss: 0.25875... Val Loss: 0.24958\n",
      "Epoch: 887/2000... Step: 887... Loss: 0.25871... Val Loss: 0.24953\n",
      "Epoch: 888/2000... Step: 888... Loss: 0.25868... Val Loss: 0.24948\n",
      "Epoch: 889/2000... Step: 889... Loss: 0.25864... Val Loss: 0.24943\n",
      "Epoch: 890/2000... Step: 890... Loss: 0.25861... Val Loss: 0.24938\n",
      "Epoch: 891/2000... Step: 891... Loss: 0.25857... Val Loss: 0.24933\n",
      "Epoch: 892/2000... Step: 892... Loss: 0.25853... Val Loss: 0.24928\n",
      "Epoch: 893/2000... Step: 893... Loss: 0.25850... Val Loss: 0.24924\n",
      "Epoch: 894/2000... Step: 894... Loss: 0.25846... Val Loss: 0.24919\n",
      "Epoch: 895/2000... Step: 895... Loss: 0.25843... Val Loss: 0.24914\n",
      "Epoch: 896/2000... Step: 896... Loss: 0.25839... Val Loss: 0.24909\n",
      "Epoch: 897/2000... Step: 897... Loss: 0.25836... Val Loss: 0.24904\n",
      "Epoch: 898/2000... Step: 898... Loss: 0.25832... Val Loss: 0.24900\n",
      "Epoch: 899/2000... Step: 899... Loss: 0.25829... Val Loss: 0.24895\n",
      "Epoch: 900/2000... Step: 900... Loss: 0.25826... Val Loss: 0.24890\n",
      "Epoch: 901/2000... Step: 901... Loss: 0.25822... Val Loss: 0.24885\n",
      "Epoch: 902/2000... Step: 902... Loss: 0.25819... Val Loss: 0.24881\n",
      "Epoch: 903/2000... Step: 903... Loss: 0.25815... Val Loss: 0.24876\n",
      "Epoch: 904/2000... Step: 904... Loss: 0.25812... Val Loss: 0.24871\n",
      "Epoch: 905/2000... Step: 905... Loss: 0.25808... Val Loss: 0.24867\n",
      "Epoch: 906/2000... Step: 906... Loss: 0.25805... Val Loss: 0.24862\n",
      "Epoch: 907/2000... Step: 907... Loss: 0.25802... Val Loss: 0.24857\n",
      "Epoch: 908/2000... Step: 908... Loss: 0.25798... Val Loss: 0.24853\n",
      "Epoch: 909/2000... Step: 909... Loss: 0.25795... Val Loss: 0.24848\n",
      "Epoch: 910/2000... Step: 910... Loss: 0.25792... Val Loss: 0.24844\n",
      "Epoch: 911/2000... Step: 911... Loss: 0.25788... Val Loss: 0.24839\n",
      "Epoch: 912/2000... Step: 912... Loss: 0.25785... Val Loss: 0.24834\n",
      "Epoch: 913/2000... Step: 913... Loss: 0.25782... Val Loss: 0.24830\n",
      "Epoch: 914/2000... Step: 914... Loss: 0.25778... Val Loss: 0.24825\n",
      "Epoch: 915/2000... Step: 915... Loss: 0.25775... Val Loss: 0.24821\n",
      "Epoch: 916/2000... Step: 916... Loss: 0.25772... Val Loss: 0.24816\n",
      "Epoch: 917/2000... Step: 917... Loss: 0.25769... Val Loss: 0.24812\n",
      "Epoch: 918/2000... Step: 918... Loss: 0.25765... Val Loss: 0.24807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 919/2000... Step: 919... Loss: 0.25762... Val Loss: 0.24803\n",
      "Epoch: 920/2000... Step: 920... Loss: 0.25759... Val Loss: 0.24798\n",
      "Epoch: 921/2000... Step: 921... Loss: 0.25756... Val Loss: 0.24794\n",
      "Epoch: 922/2000... Step: 922... Loss: 0.25752... Val Loss: 0.24789\n",
      "Epoch: 923/2000... Step: 923... Loss: 0.25749... Val Loss: 0.24785\n",
      "Epoch: 924/2000... Step: 924... Loss: 0.25746... Val Loss: 0.24780\n",
      "Epoch: 925/2000... Step: 925... Loss: 0.25743... Val Loss: 0.24776\n",
      "Epoch: 926/2000... Step: 926... Loss: 0.25740... Val Loss: 0.24772\n",
      "Epoch: 927/2000... Step: 927... Loss: 0.25736... Val Loss: 0.24767\n",
      "Epoch: 928/2000... Step: 928... Loss: 0.25733... Val Loss: 0.24763\n",
      "Epoch: 929/2000... Step: 929... Loss: 0.25730... Val Loss: 0.24758\n",
      "Epoch: 930/2000... Step: 930... Loss: 0.25727... Val Loss: 0.24754\n",
      "Epoch: 931/2000... Step: 931... Loss: 0.25724... Val Loss: 0.24750\n",
      "Epoch: 932/2000... Step: 932... Loss: 0.25721... Val Loss: 0.24745\n",
      "Epoch: 933/2000... Step: 933... Loss: 0.25718... Val Loss: 0.24741\n",
      "Epoch: 934/2000... Step: 934... Loss: 0.25714... Val Loss: 0.24737\n",
      "Epoch: 935/2000... Step: 935... Loss: 0.25711... Val Loss: 0.24732\n",
      "Epoch: 936/2000... Step: 936... Loss: 0.25708... Val Loss: 0.24728\n",
      "Epoch: 937/2000... Step: 937... Loss: 0.25705... Val Loss: 0.24724\n",
      "Epoch: 938/2000... Step: 938... Loss: 0.25702... Val Loss: 0.24719\n",
      "Epoch: 939/2000... Step: 939... Loss: 0.25699... Val Loss: 0.24715\n",
      "Epoch: 940/2000... Step: 940... Loss: 0.25696... Val Loss: 0.24711\n",
      "Epoch: 941/2000... Step: 941... Loss: 0.25693... Val Loss: 0.24707\n",
      "Epoch: 942/2000... Step: 942... Loss: 0.25690... Val Loss: 0.24702\n",
      "Epoch: 943/2000... Step: 943... Loss: 0.25687... Val Loss: 0.24698\n",
      "Epoch: 944/2000... Step: 944... Loss: 0.25684... Val Loss: 0.24694\n",
      "Epoch: 945/2000... Step: 945... Loss: 0.25681... Val Loss: 0.24690\n",
      "Epoch: 946/2000... Step: 946... Loss: 0.25678... Val Loss: 0.24686\n",
      "Epoch: 947/2000... Step: 947... Loss: 0.25675... Val Loss: 0.24681\n",
      "Epoch: 948/2000... Step: 948... Loss: 0.25672... Val Loss: 0.24677\n",
      "Epoch: 949/2000... Step: 949... Loss: 0.25669... Val Loss: 0.24673\n",
      "Epoch: 950/2000... Step: 950... Loss: 0.25666... Val Loss: 0.24669\n",
      "Epoch: 951/2000... Step: 951... Loss: 0.25663... Val Loss: 0.24665\n",
      "Epoch: 952/2000... Step: 952... Loss: 0.25660... Val Loss: 0.24661\n",
      "Epoch: 953/2000... Step: 953... Loss: 0.25657... Val Loss: 0.24657\n",
      "Epoch: 954/2000... Step: 954... Loss: 0.25655... Val Loss: 0.24653\n",
      "Epoch: 955/2000... Step: 955... Loss: 0.25652... Val Loss: 0.24648\n",
      "Epoch: 956/2000... Step: 956... Loss: 0.25649... Val Loss: 0.24644\n",
      "Epoch: 957/2000... Step: 957... Loss: 0.25646... Val Loss: 0.24640\n",
      "Epoch: 958/2000... Step: 958... Loss: 0.25643... Val Loss: 0.24636\n",
      "Epoch: 959/2000... Step: 959... Loss: 0.25640... Val Loss: 0.24632\n",
      "Epoch: 960/2000... Step: 960... Loss: 0.25637... Val Loss: 0.24628\n",
      "Epoch: 961/2000... Step: 961... Loss: 0.25634... Val Loss: 0.24624\n",
      "Epoch: 962/2000... Step: 962... Loss: 0.25632... Val Loss: 0.24620\n",
      "Epoch: 963/2000... Step: 963... Loss: 0.25629... Val Loss: 0.24616\n",
      "Epoch: 964/2000... Step: 964... Loss: 0.25626... Val Loss: 0.24612\n",
      "Epoch: 965/2000... Step: 965... Loss: 0.25623... Val Loss: 0.24608\n",
      "Epoch: 966/2000... Step: 966... Loss: 0.25620... Val Loss: 0.24604\n",
      "Epoch: 967/2000... Step: 967... Loss: 0.25618... Val Loss: 0.24600\n",
      "Epoch: 968/2000... Step: 968... Loss: 0.25615... Val Loss: 0.24596\n",
      "Epoch: 969/2000... Step: 969... Loss: 0.25612... Val Loss: 0.24592\n",
      "Epoch: 970/2000... Step: 970... Loss: 0.25609... Val Loss: 0.24588\n",
      "Epoch: 971/2000... Step: 971... Loss: 0.25607... Val Loss: 0.24584\n",
      "Epoch: 972/2000... Step: 972... Loss: 0.25604... Val Loss: 0.24581\n",
      "Epoch: 973/2000... Step: 973... Loss: 0.25601... Val Loss: 0.24577\n",
      "Epoch: 974/2000... Step: 974... Loss: 0.25598... Val Loss: 0.24573\n",
      "Epoch: 975/2000... Step: 975... Loss: 0.25596... Val Loss: 0.24569\n",
      "Epoch: 976/2000... Step: 976... Loss: 0.25593... Val Loss: 0.24565\n",
      "Epoch: 977/2000... Step: 977... Loss: 0.25590... Val Loss: 0.24561\n",
      "Epoch: 978/2000... Step: 978... Loss: 0.25587... Val Loss: 0.24557\n",
      "Epoch: 979/2000... Step: 979... Loss: 0.25585... Val Loss: 0.24554\n",
      "Epoch: 980/2000... Step: 980... Loss: 0.25582... Val Loss: 0.24550\n",
      "Epoch: 981/2000... Step: 981... Loss: 0.25579... Val Loss: 0.24546\n",
      "Epoch: 982/2000... Step: 982... Loss: 0.25577... Val Loss: 0.24542\n",
      "Epoch: 983/2000... Step: 983... Loss: 0.25574... Val Loss: 0.24538\n",
      "Epoch: 984/2000... Step: 984... Loss: 0.25571... Val Loss: 0.24534\n",
      "Epoch: 985/2000... Step: 985... Loss: 0.25569... Val Loss: 0.24531\n",
      "Epoch: 986/2000... Step: 986... Loss: 0.25566... Val Loss: 0.24527\n",
      "Epoch: 987/2000... Step: 987... Loss: 0.25564... Val Loss: 0.24523\n",
      "Epoch: 988/2000... Step: 988... Loss: 0.25561... Val Loss: 0.24519\n",
      "Epoch: 989/2000... Step: 989... Loss: 0.25558... Val Loss: 0.24516\n",
      "Epoch: 990/2000... Step: 990... Loss: 0.25556... Val Loss: 0.24512\n",
      "Epoch: 991/2000... Step: 991... Loss: 0.25553... Val Loss: 0.24508\n",
      "Epoch: 992/2000... Step: 992... Loss: 0.25551... Val Loss: 0.24505\n",
      "Epoch: 993/2000... Step: 993... Loss: 0.25548... Val Loss: 0.24501\n",
      "Epoch: 994/2000... Step: 994... Loss: 0.25545... Val Loss: 0.24497\n",
      "Epoch: 995/2000... Step: 995... Loss: 0.25543... Val Loss: 0.24493\n",
      "Epoch: 996/2000... Step: 996... Loss: 0.25540... Val Loss: 0.24490\n",
      "Epoch: 997/2000... Step: 997... Loss: 0.25538... Val Loss: 0.24486\n",
      "Epoch: 998/2000... Step: 998... Loss: 0.25535... Val Loss: 0.24483\n",
      "Epoch: 999/2000... Step: 999... Loss: 0.25533... Val Loss: 0.24479\n",
      "Epoch: 1000/2000... Step: 1000... Loss: 0.25530... Val Loss: 0.24475\n",
      "Epoch: 1001/2000... Step: 1001... Loss: 0.25528... Val Loss: 0.24472\n",
      "Epoch: 1002/2000... Step: 1002... Loss: 0.25525... Val Loss: 0.24468\n",
      "Epoch: 1003/2000... Step: 1003... Loss: 0.25523... Val Loss: 0.24464\n",
      "Epoch: 1004/2000... Step: 1004... Loss: 0.25520... Val Loss: 0.24461\n",
      "Epoch: 1005/2000... Step: 1005... Loss: 0.25518... Val Loss: 0.24457\n",
      "Epoch: 1006/2000... Step: 1006... Loss: 0.25515... Val Loss: 0.24454\n",
      "Epoch: 1007/2000... Step: 1007... Loss: 0.25513... Val Loss: 0.24450\n",
      "Epoch: 1008/2000... Step: 1008... Loss: 0.25510... Val Loss: 0.24447\n",
      "Epoch: 1009/2000... Step: 1009... Loss: 0.25508... Val Loss: 0.24443\n",
      "Epoch: 1010/2000... Step: 1010... Loss: 0.25506... Val Loss: 0.24439\n",
      "Epoch: 1011/2000... Step: 1011... Loss: 0.25503... Val Loss: 0.24436\n",
      "Epoch: 1012/2000... Step: 1012... Loss: 0.25501... Val Loss: 0.24432\n",
      "Epoch: 1013/2000... Step: 1013... Loss: 0.25498... Val Loss: 0.24429\n",
      "Epoch: 1014/2000... Step: 1014... Loss: 0.25496... Val Loss: 0.24425\n",
      "Epoch: 1015/2000... Step: 1015... Loss: 0.25494... Val Loss: 0.24422\n",
      "Epoch: 1016/2000... Step: 1016... Loss: 0.25491... Val Loss: 0.24418\n",
      "Epoch: 1017/2000... Step: 1017... Loss: 0.25489... Val Loss: 0.24415\n",
      "Epoch: 1018/2000... Step: 1018... Loss: 0.25486... Val Loss: 0.24412\n",
      "Epoch: 1019/2000... Step: 1019... Loss: 0.25484... Val Loss: 0.24408\n",
      "Epoch: 1020/2000... Step: 1020... Loss: 0.25482... Val Loss: 0.24405\n",
      "Epoch: 1021/2000... Step: 1021... Loss: 0.25479... Val Loss: 0.24401\n",
      "Epoch: 1022/2000... Step: 1022... Loss: 0.25477... Val Loss: 0.24398\n",
      "Epoch: 1023/2000... Step: 1023... Loss: 0.25475... Val Loss: 0.24394\n",
      "Epoch: 1024/2000... Step: 1024... Loss: 0.25472... Val Loss: 0.24391\n",
      "Epoch: 1025/2000... Step: 1025... Loss: 0.25470... Val Loss: 0.24388\n",
      "Epoch: 1026/2000... Step: 1026... Loss: 0.25468... Val Loss: 0.24384\n",
      "Epoch: 1027/2000... Step: 1027... Loss: 0.25465... Val Loss: 0.24381\n",
      "Epoch: 1028/2000... Step: 1028... Loss: 0.25463... Val Loss: 0.24377\n",
      "Epoch: 1029/2000... Step: 1029... Loss: 0.25461... Val Loss: 0.24374\n",
      "Epoch: 1030/2000... Step: 1030... Loss: 0.25459... Val Loss: 0.24371\n",
      "Epoch: 1031/2000... Step: 1031... Loss: 0.25456... Val Loss: 0.24367\n",
      "Epoch: 1032/2000... Step: 1032... Loss: 0.25454... Val Loss: 0.24364\n",
      "Epoch: 1033/2000... Step: 1033... Loss: 0.25452... Val Loss: 0.24361\n",
      "Epoch: 1034/2000... Step: 1034... Loss: 0.25449... Val Loss: 0.24357\n",
      "Epoch: 1035/2000... Step: 1035... Loss: 0.25447... Val Loss: 0.24354\n",
      "Epoch: 1036/2000... Step: 1036... Loss: 0.25445... Val Loss: 0.24351\n",
      "Epoch: 1037/2000... Step: 1037... Loss: 0.25443... Val Loss: 0.24347\n",
      "Epoch: 1038/2000... Step: 1038... Loss: 0.25440... Val Loss: 0.24344\n",
      "Epoch: 1039/2000... Step: 1039... Loss: 0.25438... Val Loss: 0.24341\n",
      "Epoch: 1040/2000... Step: 1040... Loss: 0.25436... Val Loss: 0.24338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1041/2000... Step: 1041... Loss: 0.25434... Val Loss: 0.24334\n",
      "Epoch: 1042/2000... Step: 1042... Loss: 0.25432... Val Loss: 0.24331\n",
      "Epoch: 1043/2000... Step: 1043... Loss: 0.25429... Val Loss: 0.24328\n",
      "Epoch: 1044/2000... Step: 1044... Loss: 0.25427... Val Loss: 0.24325\n",
      "Epoch: 1045/2000... Step: 1045... Loss: 0.25425... Val Loss: 0.24321\n",
      "Epoch: 1046/2000... Step: 1046... Loss: 0.25423... Val Loss: 0.24318\n",
      "Epoch: 1047/2000... Step: 1047... Loss: 0.25421... Val Loss: 0.24315\n",
      "Epoch: 1048/2000... Step: 1048... Loss: 0.25419... Val Loss: 0.24312\n",
      "Epoch: 1049/2000... Step: 1049... Loss: 0.25416... Val Loss: 0.24309\n",
      "Epoch: 1050/2000... Step: 1050... Loss: 0.25414... Val Loss: 0.24305\n",
      "Epoch: 1051/2000... Step: 1051... Loss: 0.25412... Val Loss: 0.24302\n",
      "Epoch: 1052/2000... Step: 1052... Loss: 0.25410... Val Loss: 0.24299\n",
      "Epoch: 1053/2000... Step: 1053... Loss: 0.25408... Val Loss: 0.24296\n",
      "Epoch: 1054/2000... Step: 1054... Loss: 0.25406... Val Loss: 0.24293\n",
      "Epoch: 1055/2000... Step: 1055... Loss: 0.25404... Val Loss: 0.24290\n",
      "Epoch: 1056/2000... Step: 1056... Loss: 0.25402... Val Loss: 0.24286\n",
      "Epoch: 1057/2000... Step: 1057... Loss: 0.25399... Val Loss: 0.24283\n",
      "Epoch: 1058/2000... Step: 1058... Loss: 0.25397... Val Loss: 0.24280\n",
      "Epoch: 1059/2000... Step: 1059... Loss: 0.25395... Val Loss: 0.24277\n",
      "Epoch: 1060/2000... Step: 1060... Loss: 0.25393... Val Loss: 0.24274\n",
      "Epoch: 1061/2000... Step: 1061... Loss: 0.25391... Val Loss: 0.24271\n",
      "Epoch: 1062/2000... Step: 1062... Loss: 0.25389... Val Loss: 0.24268\n",
      "Epoch: 1063/2000... Step: 1063... Loss: 0.25387... Val Loss: 0.24265\n",
      "Epoch: 1064/2000... Step: 1064... Loss: 0.25385... Val Loss: 0.24262\n",
      "Epoch: 1065/2000... Step: 1065... Loss: 0.25383... Val Loss: 0.24259\n",
      "Epoch: 1066/2000... Step: 1066... Loss: 0.25381... Val Loss: 0.24255\n",
      "Epoch: 1067/2000... Step: 1067... Loss: 0.25379... Val Loss: 0.24252\n",
      "Epoch: 1068/2000... Step: 1068... Loss: 0.25377... Val Loss: 0.24249\n",
      "Epoch: 1069/2000... Step: 1069... Loss: 0.25375... Val Loss: 0.24246\n",
      "Epoch: 1070/2000... Step: 1070... Loss: 0.25373... Val Loss: 0.24243\n",
      "Epoch: 1071/2000... Step: 1071... Loss: 0.25371... Val Loss: 0.24240\n",
      "Epoch: 1072/2000... Step: 1072... Loss: 0.25369... Val Loss: 0.24237\n",
      "Epoch: 1073/2000... Step: 1073... Loss: 0.25367... Val Loss: 0.24234\n",
      "Epoch: 1074/2000... Step: 1074... Loss: 0.25365... Val Loss: 0.24231\n",
      "Epoch: 1075/2000... Step: 1075... Loss: 0.25363... Val Loss: 0.24228\n",
      "Epoch: 1076/2000... Step: 1076... Loss: 0.25361... Val Loss: 0.24225\n",
      "Epoch: 1077/2000... Step: 1077... Loss: 0.25359... Val Loss: 0.24222\n",
      "Epoch: 1078/2000... Step: 1078... Loss: 0.25357... Val Loss: 0.24219\n",
      "Epoch: 1079/2000... Step: 1079... Loss: 0.25355... Val Loss: 0.24216\n",
      "Epoch: 1080/2000... Step: 1080... Loss: 0.25353... Val Loss: 0.24214\n",
      "Epoch: 1081/2000... Step: 1081... Loss: 0.25351... Val Loss: 0.24211\n",
      "Epoch: 1082/2000... Step: 1082... Loss: 0.25349... Val Loss: 0.24208\n",
      "Epoch: 1083/2000... Step: 1083... Loss: 0.25347... Val Loss: 0.24205\n",
      "Epoch: 1084/2000... Step: 1084... Loss: 0.25345... Val Loss: 0.24202\n",
      "Epoch: 1085/2000... Step: 1085... Loss: 0.25343... Val Loss: 0.24199\n",
      "Epoch: 1086/2000... Step: 1086... Loss: 0.25341... Val Loss: 0.24196\n",
      "Epoch: 1087/2000... Step: 1087... Loss: 0.25339... Val Loss: 0.24193\n",
      "Epoch: 1088/2000... Step: 1088... Loss: 0.25338... Val Loss: 0.24190\n",
      "Epoch: 1089/2000... Step: 1089... Loss: 0.25336... Val Loss: 0.24187\n",
      "Epoch: 1090/2000... Step: 1090... Loss: 0.25334... Val Loss: 0.24184\n",
      "Epoch: 1091/2000... Step: 1091... Loss: 0.25332... Val Loss: 0.24182\n",
      "Epoch: 1092/2000... Step: 1092... Loss: 0.25330... Val Loss: 0.24179\n",
      "Epoch: 1093/2000... Step: 1093... Loss: 0.25328... Val Loss: 0.24176\n",
      "Epoch: 1094/2000... Step: 1094... Loss: 0.25326... Val Loss: 0.24173\n",
      "Epoch: 1095/2000... Step: 1095... Loss: 0.25324... Val Loss: 0.24170\n",
      "Epoch: 1096/2000... Step: 1096... Loss: 0.25323... Val Loss: 0.24167\n",
      "Epoch: 1097/2000... Step: 1097... Loss: 0.25321... Val Loss: 0.24165\n",
      "Epoch: 1098/2000... Step: 1098... Loss: 0.25319... Val Loss: 0.24162\n",
      "Epoch: 1099/2000... Step: 1099... Loss: 0.25317... Val Loss: 0.24159\n",
      "Epoch: 1100/2000... Step: 1100... Loss: 0.25315... Val Loss: 0.24156\n",
      "Epoch: 1101/2000... Step: 1101... Loss: 0.25313... Val Loss: 0.24153\n",
      "Epoch: 1102/2000... Step: 1102... Loss: 0.25312... Val Loss: 0.24151\n",
      "Epoch: 1103/2000... Step: 1103... Loss: 0.25310... Val Loss: 0.24148\n",
      "Epoch: 1104/2000... Step: 1104... Loss: 0.25308... Val Loss: 0.24145\n",
      "Epoch: 1105/2000... Step: 1105... Loss: 0.25306... Val Loss: 0.24142\n",
      "Epoch: 1106/2000... Step: 1106... Loss: 0.25304... Val Loss: 0.24140\n",
      "Epoch: 1107/2000... Step: 1107... Loss: 0.25303... Val Loss: 0.24137\n",
      "Epoch: 1108/2000... Step: 1108... Loss: 0.25301... Val Loss: 0.24134\n",
      "Epoch: 1109/2000... Step: 1109... Loss: 0.25299... Val Loss: 0.24131\n",
      "Epoch: 1110/2000... Step: 1110... Loss: 0.25297... Val Loss: 0.24129\n",
      "Epoch: 1111/2000... Step: 1111... Loss: 0.25295... Val Loss: 0.24126\n",
      "Epoch: 1112/2000... Step: 1112... Loss: 0.25294... Val Loss: 0.24123\n",
      "Epoch: 1113/2000... Step: 1113... Loss: 0.25292... Val Loss: 0.24120\n",
      "Epoch: 1114/2000... Step: 1114... Loss: 0.25290... Val Loss: 0.24118\n",
      "Epoch: 1115/2000... Step: 1115... Loss: 0.25288... Val Loss: 0.24115\n",
      "Epoch: 1116/2000... Step: 1116... Loss: 0.25287... Val Loss: 0.24112\n",
      "Epoch: 1117/2000... Step: 1117... Loss: 0.25285... Val Loss: 0.24110\n",
      "Epoch: 1118/2000... Step: 1118... Loss: 0.25283... Val Loss: 0.24107\n",
      "Epoch: 1119/2000... Step: 1119... Loss: 0.25281... Val Loss: 0.24104\n",
      "Epoch: 1120/2000... Step: 1120... Loss: 0.25280... Val Loss: 0.24102\n",
      "Epoch: 1121/2000... Step: 1121... Loss: 0.25278... Val Loss: 0.24099\n",
      "Epoch: 1122/2000... Step: 1122... Loss: 0.25276... Val Loss: 0.24096\n",
      "Epoch: 1123/2000... Step: 1123... Loss: 0.25275... Val Loss: 0.24094\n",
      "Epoch: 1124/2000... Step: 1124... Loss: 0.25273... Val Loss: 0.24091\n",
      "Epoch: 1125/2000... Step: 1125... Loss: 0.25271... Val Loss: 0.24089\n",
      "Epoch: 1126/2000... Step: 1126... Loss: 0.25270... Val Loss: 0.24086\n",
      "Epoch: 1127/2000... Step: 1127... Loss: 0.25268... Val Loss: 0.24083\n",
      "Epoch: 1128/2000... Step: 1128... Loss: 0.25266... Val Loss: 0.24081\n",
      "Epoch: 1129/2000... Step: 1129... Loss: 0.25265... Val Loss: 0.24078\n",
      "Epoch: 1130/2000... Step: 1130... Loss: 0.25263... Val Loss: 0.24075\n",
      "Epoch: 1131/2000... Step: 1131... Loss: 0.25261... Val Loss: 0.24073\n",
      "Epoch: 1132/2000... Step: 1132... Loss: 0.25260... Val Loss: 0.24070\n",
      "Epoch: 1133/2000... Step: 1133... Loss: 0.25258... Val Loss: 0.24068\n",
      "Epoch: 1134/2000... Step: 1134... Loss: 0.25256... Val Loss: 0.24065\n",
      "Epoch: 1135/2000... Step: 1135... Loss: 0.25255... Val Loss: 0.24063\n",
      "Epoch: 1136/2000... Step: 1136... Loss: 0.25253... Val Loss: 0.24060\n",
      "Epoch: 1137/2000... Step: 1137... Loss: 0.25251... Val Loss: 0.24058\n",
      "Epoch: 1138/2000... Step: 1138... Loss: 0.25250... Val Loss: 0.24055\n",
      "Epoch: 1139/2000... Step: 1139... Loss: 0.25248... Val Loss: 0.24052\n",
      "Epoch: 1140/2000... Step: 1140... Loss: 0.25246... Val Loss: 0.24050\n",
      "Epoch: 1141/2000... Step: 1141... Loss: 0.25245... Val Loss: 0.24047\n",
      "Epoch: 1142/2000... Step: 1142... Loss: 0.25243... Val Loss: 0.24045\n",
      "Epoch: 1143/2000... Step: 1143... Loss: 0.25242... Val Loss: 0.24042\n",
      "Epoch: 1144/2000... Step: 1144... Loss: 0.25240... Val Loss: 0.24040\n",
      "Epoch: 1145/2000... Step: 1145... Loss: 0.25238... Val Loss: 0.24037\n",
      "Epoch: 1146/2000... Step: 1146... Loss: 0.25237... Val Loss: 0.24035\n",
      "Epoch: 1147/2000... Step: 1147... Loss: 0.25235... Val Loss: 0.24032\n",
      "Epoch: 1148/2000... Step: 1148... Loss: 0.25234... Val Loss: 0.24030\n",
      "Epoch: 1149/2000... Step: 1149... Loss: 0.25232... Val Loss: 0.24027\n",
      "Epoch: 1150/2000... Step: 1150... Loss: 0.25231... Val Loss: 0.24025\n",
      "Epoch: 1151/2000... Step: 1151... Loss: 0.25229... Val Loss: 0.24023\n",
      "Epoch: 1152/2000... Step: 1152... Loss: 0.25227... Val Loss: 0.24020\n",
      "Epoch: 1153/2000... Step: 1153... Loss: 0.25226... Val Loss: 0.24018\n",
      "Epoch: 1154/2000... Step: 1154... Loss: 0.25224... Val Loss: 0.24015\n",
      "Epoch: 1155/2000... Step: 1155... Loss: 0.25223... Val Loss: 0.24013\n",
      "Epoch: 1156/2000... Step: 1156... Loss: 0.25221... Val Loss: 0.24010\n",
      "Epoch: 1157/2000... Step: 1157... Loss: 0.25220... Val Loss: 0.24008\n",
      "Epoch: 1158/2000... Step: 1158... Loss: 0.25218... Val Loss: 0.24006\n",
      "Epoch: 1159/2000... Step: 1159... Loss: 0.25217... Val Loss: 0.24003\n",
      "Epoch: 1160/2000... Step: 1160... Loss: 0.25215... Val Loss: 0.24001\n",
      "Epoch: 1161/2000... Step: 1161... Loss: 0.25214... Val Loss: 0.23998\n",
      "Epoch: 1162/2000... Step: 1162... Loss: 0.25212... Val Loss: 0.23996\n",
      "Epoch: 1163/2000... Step: 1163... Loss: 0.25211... Val Loss: 0.23994\n",
      "Epoch: 1164/2000... Step: 1164... Loss: 0.25209... Val Loss: 0.23991\n",
      "Epoch: 1165/2000... Step: 1165... Loss: 0.25208... Val Loss: 0.23989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1166/2000... Step: 1166... Loss: 0.25206... Val Loss: 0.23986\n",
      "Epoch: 1167/2000... Step: 1167... Loss: 0.25205... Val Loss: 0.23984\n",
      "Epoch: 1168/2000... Step: 1168... Loss: 0.25203... Val Loss: 0.23982\n",
      "Epoch: 1169/2000... Step: 1169... Loss: 0.25202... Val Loss: 0.23979\n",
      "Epoch: 1170/2000... Step: 1170... Loss: 0.25200... Val Loss: 0.23977\n",
      "Epoch: 1171/2000... Step: 1171... Loss: 0.25199... Val Loss: 0.23975\n",
      "Epoch: 1172/2000... Step: 1172... Loss: 0.25197... Val Loss: 0.23972\n",
      "Epoch: 1173/2000... Step: 1173... Loss: 0.25196... Val Loss: 0.23970\n",
      "Epoch: 1174/2000... Step: 1174... Loss: 0.25195... Val Loss: 0.23968\n",
      "Epoch: 1175/2000... Step: 1175... Loss: 0.25193... Val Loss: 0.23965\n",
      "Epoch: 1176/2000... Step: 1176... Loss: 0.25192... Val Loss: 0.23963\n",
      "Epoch: 1177/2000... Step: 1177... Loss: 0.25190... Val Loss: 0.23961\n",
      "Epoch: 1178/2000... Step: 1178... Loss: 0.25189... Val Loss: 0.23959\n",
      "Epoch: 1179/2000... Step: 1179... Loss: 0.25187... Val Loss: 0.23956\n",
      "Epoch: 1180/2000... Step: 1180... Loss: 0.25186... Val Loss: 0.23954\n",
      "Epoch: 1181/2000... Step: 1181... Loss: 0.25184... Val Loss: 0.23952\n",
      "Epoch: 1182/2000... Step: 1182... Loss: 0.25183... Val Loss: 0.23949\n",
      "Epoch: 1183/2000... Step: 1183... Loss: 0.25182... Val Loss: 0.23947\n",
      "Epoch: 1184/2000... Step: 1184... Loss: 0.25180... Val Loss: 0.23945\n",
      "Epoch: 1185/2000... Step: 1185... Loss: 0.25179... Val Loss: 0.23943\n",
      "Epoch: 1186/2000... Step: 1186... Loss: 0.25177... Val Loss: 0.23940\n",
      "Epoch: 1187/2000... Step: 1187... Loss: 0.25176... Val Loss: 0.23938\n",
      "Epoch: 1188/2000... Step: 1188... Loss: 0.25175... Val Loss: 0.23936\n",
      "Epoch: 1189/2000... Step: 1189... Loss: 0.25173... Val Loss: 0.23934\n",
      "Epoch: 1190/2000... Step: 1190... Loss: 0.25172... Val Loss: 0.23931\n",
      "Epoch: 1191/2000... Step: 1191... Loss: 0.25171... Val Loss: 0.23929\n",
      "Epoch: 1192/2000... Step: 1192... Loss: 0.25169... Val Loss: 0.23927\n",
      "Epoch: 1193/2000... Step: 1193... Loss: 0.25168... Val Loss: 0.23925\n",
      "Epoch: 1194/2000... Step: 1194... Loss: 0.25166... Val Loss: 0.23923\n",
      "Epoch: 1195/2000... Step: 1195... Loss: 0.25165... Val Loss: 0.23920\n",
      "Epoch: 1196/2000... Step: 1196... Loss: 0.25164... Val Loss: 0.23918\n",
      "Epoch: 1197/2000... Step: 1197... Loss: 0.25162... Val Loss: 0.23916\n",
      "Epoch: 1198/2000... Step: 1198... Loss: 0.25161... Val Loss: 0.23914\n",
      "Epoch: 1199/2000... Step: 1199... Loss: 0.25160... Val Loss: 0.23912\n",
      "Epoch: 1200/2000... Step: 1200... Loss: 0.25158... Val Loss: 0.23910\n",
      "Epoch: 1201/2000... Step: 1201... Loss: 0.25157... Val Loss: 0.23907\n",
      "Epoch: 1202/2000... Step: 1202... Loss: 0.25156... Val Loss: 0.23905\n",
      "Epoch: 1203/2000... Step: 1203... Loss: 0.25154... Val Loss: 0.23903\n",
      "Epoch: 1204/2000... Step: 1204... Loss: 0.25153... Val Loss: 0.23901\n",
      "Epoch: 1205/2000... Step: 1205... Loss: 0.25152... Val Loss: 0.23899\n",
      "Epoch: 1206/2000... Step: 1206... Loss: 0.25150... Val Loss: 0.23897\n",
      "Epoch: 1207/2000... Step: 1207... Loss: 0.25149... Val Loss: 0.23895\n",
      "Epoch: 1208/2000... Step: 1208... Loss: 0.25148... Val Loss: 0.23892\n",
      "Epoch: 1209/2000... Step: 1209... Loss: 0.25147... Val Loss: 0.23890\n",
      "Epoch: 1210/2000... Step: 1210... Loss: 0.25145... Val Loss: 0.23888\n",
      "Epoch: 1211/2000... Step: 1211... Loss: 0.25144... Val Loss: 0.23886\n",
      "Epoch: 1212/2000... Step: 1212... Loss: 0.25143... Val Loss: 0.23884\n",
      "Epoch: 1213/2000... Step: 1213... Loss: 0.25141... Val Loss: 0.23882\n",
      "Epoch: 1214/2000... Step: 1214... Loss: 0.25140... Val Loss: 0.23880\n",
      "Epoch: 1215/2000... Step: 1215... Loss: 0.25139... Val Loss: 0.23878\n",
      "Epoch: 1216/2000... Step: 1216... Loss: 0.25138... Val Loss: 0.23876\n",
      "Epoch: 1217/2000... Step: 1217... Loss: 0.25136... Val Loss: 0.23873\n",
      "Epoch: 1218/2000... Step: 1218... Loss: 0.25135... Val Loss: 0.23871\n",
      "Epoch: 1219/2000... Step: 1219... Loss: 0.25134... Val Loss: 0.23869\n",
      "Epoch: 1220/2000... Step: 1220... Loss: 0.25133... Val Loss: 0.23867\n",
      "Epoch: 1221/2000... Step: 1221... Loss: 0.25131... Val Loss: 0.23865\n",
      "Epoch: 1222/2000... Step: 1222... Loss: 0.25130... Val Loss: 0.23863\n",
      "Epoch: 1223/2000... Step: 1223... Loss: 0.25129... Val Loss: 0.23861\n",
      "Epoch: 1224/2000... Step: 1224... Loss: 0.25128... Val Loss: 0.23859\n",
      "Epoch: 1225/2000... Step: 1225... Loss: 0.25126... Val Loss: 0.23857\n",
      "Epoch: 1226/2000... Step: 1226... Loss: 0.25125... Val Loss: 0.23855\n",
      "Epoch: 1227/2000... Step: 1227... Loss: 0.25124... Val Loss: 0.23853\n",
      "Epoch: 1228/2000... Step: 1228... Loss: 0.25123... Val Loss: 0.23851\n",
      "Epoch: 1229/2000... Step: 1229... Loss: 0.25121... Val Loss: 0.23849\n",
      "Epoch: 1230/2000... Step: 1230... Loss: 0.25120... Val Loss: 0.23847\n",
      "Epoch: 1231/2000... Step: 1231... Loss: 0.25119... Val Loss: 0.23845\n",
      "Epoch: 1232/2000... Step: 1232... Loss: 0.25118... Val Loss: 0.23843\n",
      "Epoch: 1233/2000... Step: 1233... Loss: 0.25117... Val Loss: 0.23841\n",
      "Epoch: 1234/2000... Step: 1234... Loss: 0.25115... Val Loss: 0.23839\n",
      "Epoch: 1235/2000... Step: 1235... Loss: 0.25114... Val Loss: 0.23837\n",
      "Epoch: 1236/2000... Step: 1236... Loss: 0.25113... Val Loss: 0.23835\n",
      "Epoch: 1237/2000... Step: 1237... Loss: 0.25112... Val Loss: 0.23833\n",
      "Epoch: 1238/2000... Step: 1238... Loss: 0.25111... Val Loss: 0.23831\n",
      "Epoch: 1239/2000... Step: 1239... Loss: 0.25109... Val Loss: 0.23829\n",
      "Epoch: 1240/2000... Step: 1240... Loss: 0.25108... Val Loss: 0.23827\n",
      "Epoch: 1241/2000... Step: 1241... Loss: 0.25107... Val Loss: 0.23825\n",
      "Epoch: 1242/2000... Step: 1242... Loss: 0.25106... Val Loss: 0.23823\n",
      "Epoch: 1243/2000... Step: 1243... Loss: 0.25105... Val Loss: 0.23821\n",
      "Epoch: 1244/2000... Step: 1244... Loss: 0.25104... Val Loss: 0.23819\n",
      "Epoch: 1245/2000... Step: 1245... Loss: 0.25102... Val Loss: 0.23817\n",
      "Epoch: 1246/2000... Step: 1246... Loss: 0.25101... Val Loss: 0.23815\n",
      "Epoch: 1247/2000... Step: 1247... Loss: 0.25100... Val Loss: 0.23813\n",
      "Epoch: 1248/2000... Step: 1248... Loss: 0.25099... Val Loss: 0.23812\n",
      "Epoch: 1249/2000... Step: 1249... Loss: 0.25098... Val Loss: 0.23810\n",
      "Epoch: 1250/2000... Step: 1250... Loss: 0.25097... Val Loss: 0.23808\n",
      "Epoch: 1251/2000... Step: 1251... Loss: 0.25096... Val Loss: 0.23806\n",
      "Epoch: 1252/2000... Step: 1252... Loss: 0.25094... Val Loss: 0.23804\n",
      "Epoch: 1253/2000... Step: 1253... Loss: 0.25093... Val Loss: 0.23802\n",
      "Epoch: 1254/2000... Step: 1254... Loss: 0.25092... Val Loss: 0.23800\n",
      "Epoch: 1255/2000... Step: 1255... Loss: 0.25091... Val Loss: 0.23798\n",
      "Epoch: 1256/2000... Step: 1256... Loss: 0.25090... Val Loss: 0.23796\n",
      "Epoch: 1257/2000... Step: 1257... Loss: 0.25089... Val Loss: 0.23794\n",
      "Epoch: 1258/2000... Step: 1258... Loss: 0.25088... Val Loss: 0.23793\n",
      "Epoch: 1259/2000... Step: 1259... Loss: 0.25087... Val Loss: 0.23791\n",
      "Epoch: 1260/2000... Step: 1260... Loss: 0.25086... Val Loss: 0.23789\n",
      "Epoch: 1261/2000... Step: 1261... Loss: 0.25084... Val Loss: 0.23787\n",
      "Epoch: 1262/2000... Step: 1262... Loss: 0.25083... Val Loss: 0.23785\n",
      "Epoch: 1263/2000... Step: 1263... Loss: 0.25082... Val Loss: 0.23783\n",
      "Epoch: 1264/2000... Step: 1264... Loss: 0.25081... Val Loss: 0.23781\n",
      "Epoch: 1265/2000... Step: 1265... Loss: 0.25080... Val Loss: 0.23780\n",
      "Epoch: 1266/2000... Step: 1266... Loss: 0.25079... Val Loss: 0.23778\n",
      "Epoch: 1267/2000... Step: 1267... Loss: 0.25078... Val Loss: 0.23776\n",
      "Epoch: 1268/2000... Step: 1268... Loss: 0.25077... Val Loss: 0.23774\n",
      "Epoch: 1269/2000... Step: 1269... Loss: 0.25076... Val Loss: 0.23772\n",
      "Epoch: 1270/2000... Step: 1270... Loss: 0.25075... Val Loss: 0.23770\n",
      "Epoch: 1271/2000... Step: 1271... Loss: 0.25074... Val Loss: 0.23769\n",
      "Epoch: 1272/2000... Step: 1272... Loss: 0.25073... Val Loss: 0.23767\n",
      "Epoch: 1273/2000... Step: 1273... Loss: 0.25072... Val Loss: 0.23765\n",
      "Epoch: 1274/2000... Step: 1274... Loss: 0.25070... Val Loss: 0.23763\n",
      "Epoch: 1275/2000... Step: 1275... Loss: 0.25069... Val Loss: 0.23761\n",
      "Epoch: 1276/2000... Step: 1276... Loss: 0.25068... Val Loss: 0.23760\n",
      "Epoch: 1277/2000... Step: 1277... Loss: 0.25067... Val Loss: 0.23758\n",
      "Epoch: 1278/2000... Step: 1278... Loss: 0.25066... Val Loss: 0.23756\n",
      "Epoch: 1279/2000... Step: 1279... Loss: 0.25065... Val Loss: 0.23754\n",
      "Epoch: 1280/2000... Step: 1280... Loss: 0.25064... Val Loss: 0.23752\n",
      "Epoch: 1281/2000... Step: 1281... Loss: 0.25063... Val Loss: 0.23751\n",
      "Epoch: 1282/2000... Step: 1282... Loss: 0.25062... Val Loss: 0.23749\n",
      "Epoch: 1283/2000... Step: 1283... Loss: 0.25061... Val Loss: 0.23747\n",
      "Epoch: 1284/2000... Step: 1284... Loss: 0.25060... Val Loss: 0.23745\n",
      "Epoch: 1285/2000... Step: 1285... Loss: 0.25059... Val Loss: 0.23744\n",
      "Epoch: 1286/2000... Step: 1286... Loss: 0.25058... Val Loss: 0.23742\n",
      "Epoch: 1287/2000... Step: 1287... Loss: 0.25057... Val Loss: 0.23740\n",
      "Epoch: 1288/2000... Step: 1288... Loss: 0.25056... Val Loss: 0.23738\n",
      "Epoch: 1289/2000... Step: 1289... Loss: 0.25055... Val Loss: 0.23737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1290/2000... Step: 1290... Loss: 0.25054... Val Loss: 0.23735\n",
      "Epoch: 1291/2000... Step: 1291... Loss: 0.25053... Val Loss: 0.23733\n",
      "Epoch: 1292/2000... Step: 1292... Loss: 0.25052... Val Loss: 0.23732\n",
      "Epoch: 1293/2000... Step: 1293... Loss: 0.25051... Val Loss: 0.23730\n",
      "Epoch: 1294/2000... Step: 1294... Loss: 0.25050... Val Loss: 0.23728\n",
      "Epoch: 1295/2000... Step: 1295... Loss: 0.25049... Val Loss: 0.23726\n",
      "Epoch: 1296/2000... Step: 1296... Loss: 0.25048... Val Loss: 0.23725\n",
      "Epoch: 1297/2000... Step: 1297... Loss: 0.25047... Val Loss: 0.23723\n",
      "Epoch: 1298/2000... Step: 1298... Loss: 0.25046... Val Loss: 0.23721\n",
      "Epoch: 1299/2000... Step: 1299... Loss: 0.25045... Val Loss: 0.23720\n",
      "Epoch: 1300/2000... Step: 1300... Loss: 0.25044... Val Loss: 0.23718\n",
      "Epoch: 1301/2000... Step: 1301... Loss: 0.25043... Val Loss: 0.23716\n",
      "Epoch: 1302/2000... Step: 1302... Loss: 0.25042... Val Loss: 0.23715\n",
      "Epoch: 1303/2000... Step: 1303... Loss: 0.25041... Val Loss: 0.23713\n",
      "Epoch: 1304/2000... Step: 1304... Loss: 0.25040... Val Loss: 0.23711\n",
      "Epoch: 1305/2000... Step: 1305... Loss: 0.25039... Val Loss: 0.23709\n",
      "Epoch: 1306/2000... Step: 1306... Loss: 0.25038... Val Loss: 0.23708\n",
      "Epoch: 1307/2000... Step: 1307... Loss: 0.25037... Val Loss: 0.23706\n",
      "Epoch: 1308/2000... Step: 1308... Loss: 0.25037... Val Loss: 0.23705\n",
      "Epoch: 1309/2000... Step: 1309... Loss: 0.25036... Val Loss: 0.23703\n",
      "Epoch: 1310/2000... Step: 1310... Loss: 0.25035... Val Loss: 0.23701\n",
      "Epoch: 1311/2000... Step: 1311... Loss: 0.25034... Val Loss: 0.23700\n",
      "Epoch: 1312/2000... Step: 1312... Loss: 0.25033... Val Loss: 0.23698\n",
      "Epoch: 1313/2000... Step: 1313... Loss: 0.25032... Val Loss: 0.23696\n",
      "Epoch: 1314/2000... Step: 1314... Loss: 0.25031... Val Loss: 0.23695\n",
      "Epoch: 1315/2000... Step: 1315... Loss: 0.25030... Val Loss: 0.23693\n",
      "Epoch: 1316/2000... Step: 1316... Loss: 0.25029... Val Loss: 0.23691\n",
      "Epoch: 1317/2000... Step: 1317... Loss: 0.25028... Val Loss: 0.23690\n",
      "Epoch: 1318/2000... Step: 1318... Loss: 0.25027... Val Loss: 0.23688\n",
      "Epoch: 1319/2000... Step: 1319... Loss: 0.25026... Val Loss: 0.23687\n",
      "Epoch: 1320/2000... Step: 1320... Loss: 0.25025... Val Loss: 0.23685\n",
      "Epoch: 1321/2000... Step: 1321... Loss: 0.25024... Val Loss: 0.23683\n",
      "Epoch: 1322/2000... Step: 1322... Loss: 0.25024... Val Loss: 0.23682\n",
      "Epoch: 1323/2000... Step: 1323... Loss: 0.25023... Val Loss: 0.23680\n",
      "Epoch: 1324/2000... Step: 1324... Loss: 0.25022... Val Loss: 0.23679\n",
      "Epoch: 1325/2000... Step: 1325... Loss: 0.25021... Val Loss: 0.23677\n",
      "Epoch: 1326/2000... Step: 1326... Loss: 0.25020... Val Loss: 0.23675\n",
      "Epoch: 1327/2000... Step: 1327... Loss: 0.25019... Val Loss: 0.23674\n",
      "Epoch: 1328/2000... Step: 1328... Loss: 0.25018... Val Loss: 0.23672\n",
      "Epoch: 1329/2000... Step: 1329... Loss: 0.25017... Val Loss: 0.23671\n",
      "Epoch: 1330/2000... Step: 1330... Loss: 0.25016... Val Loss: 0.23669\n",
      "Epoch: 1331/2000... Step: 1331... Loss: 0.25016... Val Loss: 0.23668\n",
      "Epoch: 1332/2000... Step: 1332... Loss: 0.25015... Val Loss: 0.23666\n",
      "Epoch: 1333/2000... Step: 1333... Loss: 0.25014... Val Loss: 0.23664\n",
      "Epoch: 1334/2000... Step: 1334... Loss: 0.25013... Val Loss: 0.23663\n",
      "Epoch: 1335/2000... Step: 1335... Loss: 0.25012... Val Loss: 0.23661\n",
      "Epoch: 1336/2000... Step: 1336... Loss: 0.25011... Val Loss: 0.23660\n",
      "Epoch: 1337/2000... Step: 1337... Loss: 0.25010... Val Loss: 0.23658\n",
      "Epoch: 1338/2000... Step: 1338... Loss: 0.25009... Val Loss: 0.23657\n",
      "Epoch: 1339/2000... Step: 1339... Loss: 0.25009... Val Loss: 0.23655\n",
      "Epoch: 1340/2000... Step: 1340... Loss: 0.25008... Val Loss: 0.23654\n",
      "Epoch: 1341/2000... Step: 1341... Loss: 0.25007... Val Loss: 0.23652\n",
      "Epoch: 1342/2000... Step: 1342... Loss: 0.25006... Val Loss: 0.23651\n",
      "Epoch: 1343/2000... Step: 1343... Loss: 0.25005... Val Loss: 0.23649\n",
      "Epoch: 1344/2000... Step: 1344... Loss: 0.25004... Val Loss: 0.23647\n",
      "Epoch: 1345/2000... Step: 1345... Loss: 0.25004... Val Loss: 0.23646\n",
      "Epoch: 1346/2000... Step: 1346... Loss: 0.25003... Val Loss: 0.23644\n",
      "Epoch: 1347/2000... Step: 1347... Loss: 0.25002... Val Loss: 0.23643\n",
      "Epoch: 1348/2000... Step: 1348... Loss: 0.25001... Val Loss: 0.23641\n",
      "Epoch: 1349/2000... Step: 1349... Loss: 0.25000... Val Loss: 0.23640\n",
      "Epoch: 1350/2000... Step: 1350... Loss: 0.24999... Val Loss: 0.23638\n",
      "Epoch: 1351/2000... Step: 1351... Loss: 0.24999... Val Loss: 0.23637\n",
      "Epoch: 1352/2000... Step: 1352... Loss: 0.24998... Val Loss: 0.23635\n",
      "Epoch: 1353/2000... Step: 1353... Loss: 0.24997... Val Loss: 0.23634\n",
      "Epoch: 1354/2000... Step: 1354... Loss: 0.24996... Val Loss: 0.23633\n",
      "Epoch: 1355/2000... Step: 1355... Loss: 0.24995... Val Loss: 0.23631\n",
      "Epoch: 1356/2000... Step: 1356... Loss: 0.24995... Val Loss: 0.23630\n",
      "Epoch: 1357/2000... Step: 1357... Loss: 0.24994... Val Loss: 0.23628\n",
      "Epoch: 1358/2000... Step: 1358... Loss: 0.24993... Val Loss: 0.23627\n",
      "Epoch: 1359/2000... Step: 1359... Loss: 0.24992... Val Loss: 0.23625\n",
      "Epoch: 1360/2000... Step: 1360... Loss: 0.24991... Val Loss: 0.23624\n",
      "Epoch: 1361/2000... Step: 1361... Loss: 0.24990... Val Loss: 0.23622\n",
      "Epoch: 1362/2000... Step: 1362... Loss: 0.24990... Val Loss: 0.23621\n",
      "Epoch: 1363/2000... Step: 1363... Loss: 0.24989... Val Loss: 0.23619\n",
      "Epoch: 1364/2000... Step: 1364... Loss: 0.24988... Val Loss: 0.23618\n",
      "Epoch: 1365/2000... Step: 1365... Loss: 0.24987... Val Loss: 0.23616\n",
      "Epoch: 1366/2000... Step: 1366... Loss: 0.24987... Val Loss: 0.23615\n",
      "Epoch: 1367/2000... Step: 1367... Loss: 0.24986... Val Loss: 0.23614\n",
      "Epoch: 1368/2000... Step: 1368... Loss: 0.24985... Val Loss: 0.23612\n",
      "Epoch: 1369/2000... Step: 1369... Loss: 0.24984... Val Loss: 0.23611\n",
      "Epoch: 1370/2000... Step: 1370... Loss: 0.24983... Val Loss: 0.23609\n",
      "Epoch: 1371/2000... Step: 1371... Loss: 0.24983... Val Loss: 0.23608\n",
      "Epoch: 1372/2000... Step: 1372... Loss: 0.24982... Val Loss: 0.23606\n",
      "Epoch: 1373/2000... Step: 1373... Loss: 0.24981... Val Loss: 0.23605\n",
      "Epoch: 1374/2000... Step: 1374... Loss: 0.24980... Val Loss: 0.23604\n",
      "Epoch: 1375/2000... Step: 1375... Loss: 0.24980... Val Loss: 0.23602\n",
      "Epoch: 1376/2000... Step: 1376... Loss: 0.24979... Val Loss: 0.23601\n",
      "Epoch: 1377/2000... Step: 1377... Loss: 0.24978... Val Loss: 0.23599\n",
      "Epoch: 1378/2000... Step: 1378... Loss: 0.24977... Val Loss: 0.23598\n",
      "Epoch: 1379/2000... Step: 1379... Loss: 0.24977... Val Loss: 0.23597\n",
      "Epoch: 1380/2000... Step: 1380... Loss: 0.24976... Val Loss: 0.23595\n",
      "Epoch: 1381/2000... Step: 1381... Loss: 0.24975... Val Loss: 0.23594\n",
      "Epoch: 1382/2000... Step: 1382... Loss: 0.24974... Val Loss: 0.23593\n",
      "Epoch: 1383/2000... Step: 1383... Loss: 0.24974... Val Loss: 0.23591\n",
      "Epoch: 1384/2000... Step: 1384... Loss: 0.24973... Val Loss: 0.23590\n",
      "Epoch: 1385/2000... Step: 1385... Loss: 0.24972... Val Loss: 0.23588\n",
      "Epoch: 1386/2000... Step: 1386... Loss: 0.24971... Val Loss: 0.23587\n",
      "Epoch: 1387/2000... Step: 1387... Loss: 0.24971... Val Loss: 0.23586\n",
      "Epoch: 1388/2000... Step: 1388... Loss: 0.24970... Val Loss: 0.23584\n",
      "Epoch: 1389/2000... Step: 1389... Loss: 0.24969... Val Loss: 0.23583\n",
      "Epoch: 1390/2000... Step: 1390... Loss: 0.24968... Val Loss: 0.23582\n",
      "Epoch: 1391/2000... Step: 1391... Loss: 0.24968... Val Loss: 0.23580\n",
      "Epoch: 1392/2000... Step: 1392... Loss: 0.24967... Val Loss: 0.23579\n",
      "Epoch: 1393/2000... Step: 1393... Loss: 0.24966... Val Loss: 0.23578\n",
      "Epoch: 1394/2000... Step: 1394... Loss: 0.24966... Val Loss: 0.23576\n",
      "Epoch: 1395/2000... Step: 1395... Loss: 0.24965... Val Loss: 0.23575\n",
      "Epoch: 1396/2000... Step: 1396... Loss: 0.24964... Val Loss: 0.23574\n",
      "Epoch: 1397/2000... Step: 1397... Loss: 0.24963... Val Loss: 0.23572\n",
      "Epoch: 1398/2000... Step: 1398... Loss: 0.24963... Val Loss: 0.23571\n",
      "Epoch: 1399/2000... Step: 1399... Loss: 0.24962... Val Loss: 0.23570\n",
      "Epoch: 1400/2000... Step: 1400... Loss: 0.24961... Val Loss: 0.23568\n",
      "Epoch: 1401/2000... Step: 1401... Loss: 0.24961... Val Loss: 0.23567\n",
      "Epoch: 1402/2000... Step: 1402... Loss: 0.24960... Val Loss: 0.23566\n",
      "Epoch: 1403/2000... Step: 1403... Loss: 0.24959... Val Loss: 0.23564\n",
      "Epoch: 1404/2000... Step: 1404... Loss: 0.24959... Val Loss: 0.23563\n",
      "Epoch: 1405/2000... Step: 1405... Loss: 0.24958... Val Loss: 0.23562\n",
      "Epoch: 1406/2000... Step: 1406... Loss: 0.24957... Val Loss: 0.23560\n",
      "Epoch: 1407/2000... Step: 1407... Loss: 0.24956... Val Loss: 0.23559\n",
      "Epoch: 1408/2000... Step: 1408... Loss: 0.24956... Val Loss: 0.23558\n",
      "Epoch: 1409/2000... Step: 1409... Loss: 0.24955... Val Loss: 0.23556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1410/2000... Step: 1410... Loss: 0.24954... Val Loss: 0.23555\n",
      "Epoch: 1411/2000... Step: 1411... Loss: 0.24954... Val Loss: 0.23554\n",
      "Epoch: 1412/2000... Step: 1412... Loss: 0.24953... Val Loss: 0.23553\n",
      "Epoch: 1413/2000... Step: 1413... Loss: 0.24952... Val Loss: 0.23551\n",
      "Epoch: 1414/2000... Step: 1414... Loss: 0.24952... Val Loss: 0.23550\n",
      "Epoch: 1415/2000... Step: 1415... Loss: 0.24951... Val Loss: 0.23549\n",
      "Epoch: 1416/2000... Step: 1416... Loss: 0.24950... Val Loss: 0.23547\n",
      "Epoch: 1417/2000... Step: 1417... Loss: 0.24950... Val Loss: 0.23546\n",
      "Epoch: 1418/2000... Step: 1418... Loss: 0.24949... Val Loss: 0.23545\n",
      "Epoch: 1419/2000... Step: 1419... Loss: 0.24948... Val Loss: 0.23544\n",
      "Epoch: 1420/2000... Step: 1420... Loss: 0.24948... Val Loss: 0.23542\n",
      "Epoch: 1421/2000... Step: 1421... Loss: 0.24947... Val Loss: 0.23541\n",
      "Epoch: 1422/2000... Step: 1422... Loss: 0.24946... Val Loss: 0.23540\n",
      "Epoch: 1423/2000... Step: 1423... Loss: 0.24946... Val Loss: 0.23539\n",
      "Epoch: 1424/2000... Step: 1424... Loss: 0.24945... Val Loss: 0.23537\n",
      "Epoch: 1425/2000... Step: 1425... Loss: 0.24944... Val Loss: 0.23536\n",
      "Epoch: 1426/2000... Step: 1426... Loss: 0.24944... Val Loss: 0.23535\n",
      "Epoch: 1427/2000... Step: 1427... Loss: 0.24943... Val Loss: 0.23534\n",
      "Epoch: 1428/2000... Step: 1428... Loss: 0.24943... Val Loss: 0.23532\n",
      "Epoch: 1429/2000... Step: 1429... Loss: 0.24942... Val Loss: 0.23531\n",
      "Epoch: 1430/2000... Step: 1430... Loss: 0.24941... Val Loss: 0.23530\n",
      "Epoch: 1431/2000... Step: 1431... Loss: 0.24941... Val Loss: 0.23529\n",
      "Epoch: 1432/2000... Step: 1432... Loss: 0.24940... Val Loss: 0.23528\n",
      "Epoch: 1433/2000... Step: 1433... Loss: 0.24939... Val Loss: 0.23526\n",
      "Epoch: 1434/2000... Step: 1434... Loss: 0.24939... Val Loss: 0.23525\n",
      "Epoch: 1435/2000... Step: 1435... Loss: 0.24938... Val Loss: 0.23524\n",
      "Epoch: 1436/2000... Step: 1436... Loss: 0.24937... Val Loss: 0.23523\n",
      "Epoch: 1437/2000... Step: 1437... Loss: 0.24937... Val Loss: 0.23521\n",
      "Epoch: 1438/2000... Step: 1438... Loss: 0.24936... Val Loss: 0.23520\n",
      "Epoch: 1439/2000... Step: 1439... Loss: 0.24936... Val Loss: 0.23519\n",
      "Epoch: 1440/2000... Step: 1440... Loss: 0.24935... Val Loss: 0.23518\n",
      "Epoch: 1441/2000... Step: 1441... Loss: 0.24934... Val Loss: 0.23517\n",
      "Epoch: 1442/2000... Step: 1442... Loss: 0.24934... Val Loss: 0.23515\n",
      "Epoch: 1443/2000... Step: 1443... Loss: 0.24933... Val Loss: 0.23514\n",
      "Epoch: 1444/2000... Step: 1444... Loss: 0.24933... Val Loss: 0.23513\n",
      "Epoch: 1445/2000... Step: 1445... Loss: 0.24932... Val Loss: 0.23512\n",
      "Epoch: 1446/2000... Step: 1446... Loss: 0.24931... Val Loss: 0.23511\n",
      "Epoch: 1447/2000... Step: 1447... Loss: 0.24931... Val Loss: 0.23510\n",
      "Epoch: 1448/2000... Step: 1448... Loss: 0.24930... Val Loss: 0.23508\n",
      "Epoch: 1449/2000... Step: 1449... Loss: 0.24930... Val Loss: 0.23507\n",
      "Epoch: 1450/2000... Step: 1450... Loss: 0.24929... Val Loss: 0.23506\n",
      "Epoch: 1451/2000... Step: 1451... Loss: 0.24928... Val Loss: 0.23505\n",
      "Epoch: 1452/2000... Step: 1452... Loss: 0.24928... Val Loss: 0.23504\n",
      "Epoch: 1453/2000... Step: 1453... Loss: 0.24927... Val Loss: 0.23502\n",
      "Epoch: 1454/2000... Step: 1454... Loss: 0.24927... Val Loss: 0.23501\n",
      "Epoch: 1455/2000... Step: 1455... Loss: 0.24926... Val Loss: 0.23500\n",
      "Epoch: 1456/2000... Step: 1456... Loss: 0.24925... Val Loss: 0.23499\n",
      "Epoch: 1457/2000... Step: 1457... Loss: 0.24925... Val Loss: 0.23498\n",
      "Epoch: 1458/2000... Step: 1458... Loss: 0.24924... Val Loss: 0.23497\n",
      "Epoch: 1459/2000... Step: 1459... Loss: 0.24924... Val Loss: 0.23496\n",
      "Epoch: 1460/2000... Step: 1460... Loss: 0.24923... Val Loss: 0.23494\n",
      "Epoch: 1461/2000... Step: 1461... Loss: 0.24922... Val Loss: 0.23493\n",
      "Epoch: 1462/2000... Step: 1462... Loss: 0.24922... Val Loss: 0.23492\n",
      "Epoch: 1463/2000... Step: 1463... Loss: 0.24921... Val Loss: 0.23491\n",
      "Epoch: 1464/2000... Step: 1464... Loss: 0.24921... Val Loss: 0.23490\n",
      "Epoch: 1465/2000... Step: 1465... Loss: 0.24920... Val Loss: 0.23489\n",
      "Epoch: 1466/2000... Step: 1466... Loss: 0.24920... Val Loss: 0.23488\n",
      "Epoch: 1467/2000... Step: 1467... Loss: 0.24919... Val Loss: 0.23486\n",
      "Epoch: 1468/2000... Step: 1468... Loss: 0.24919... Val Loss: 0.23485\n",
      "Epoch: 1469/2000... Step: 1469... Loss: 0.24918... Val Loss: 0.23484\n",
      "Epoch: 1470/2000... Step: 1470... Loss: 0.24917... Val Loss: 0.23483\n",
      "Epoch: 1471/2000... Step: 1471... Loss: 0.24917... Val Loss: 0.23482\n",
      "Epoch: 1472/2000... Step: 1472... Loss: 0.24916... Val Loss: 0.23481\n",
      "Epoch: 1473/2000... Step: 1473... Loss: 0.24916... Val Loss: 0.23480\n",
      "Epoch: 1474/2000... Step: 1474... Loss: 0.24915... Val Loss: 0.23479\n",
      "Epoch: 1475/2000... Step: 1475... Loss: 0.24915... Val Loss: 0.23478\n",
      "Epoch: 1476/2000... Step: 1476... Loss: 0.24914... Val Loss: 0.23476\n",
      "Epoch: 1477/2000... Step: 1477... Loss: 0.24914... Val Loss: 0.23475\n",
      "Epoch: 1478/2000... Step: 1478... Loss: 0.24913... Val Loss: 0.23474\n",
      "Epoch: 1479/2000... Step: 1479... Loss: 0.24912... Val Loss: 0.23473\n",
      "Epoch: 1480/2000... Step: 1480... Loss: 0.24912... Val Loss: 0.23472\n",
      "Epoch: 1481/2000... Step: 1481... Loss: 0.24911... Val Loss: 0.23471\n",
      "Epoch: 1482/2000... Step: 1482... Loss: 0.24911... Val Loss: 0.23470\n",
      "Epoch: 1483/2000... Step: 1483... Loss: 0.24910... Val Loss: 0.23469\n",
      "Epoch: 1484/2000... Step: 1484... Loss: 0.24910... Val Loss: 0.23468\n",
      "Epoch: 1485/2000... Step: 1485... Loss: 0.24909... Val Loss: 0.23467\n",
      "Epoch: 1486/2000... Step: 1486... Loss: 0.24909... Val Loss: 0.23466\n",
      "Epoch: 1487/2000... Step: 1487... Loss: 0.24908... Val Loss: 0.23465\n",
      "Epoch: 1488/2000... Step: 1488... Loss: 0.24908... Val Loss: 0.23463\n",
      "Epoch: 1489/2000... Step: 1489... Loss: 0.24907... Val Loss: 0.23462\n",
      "Epoch: 1490/2000... Step: 1490... Loss: 0.24907... Val Loss: 0.23461\n",
      "Epoch: 1491/2000... Step: 1491... Loss: 0.24906... Val Loss: 0.23460\n",
      "Epoch: 1492/2000... Step: 1492... Loss: 0.24906... Val Loss: 0.23459\n",
      "Epoch: 1493/2000... Step: 1493... Loss: 0.24905... Val Loss: 0.23458\n",
      "Epoch: 1494/2000... Step: 1494... Loss: 0.24905... Val Loss: 0.23457\n",
      "Epoch: 1495/2000... Step: 1495... Loss: 0.24904... Val Loss: 0.23456\n",
      "Epoch: 1496/2000... Step: 1496... Loss: 0.24904... Val Loss: 0.23455\n",
      "Epoch: 1497/2000... Step: 1497... Loss: 0.24903... Val Loss: 0.23454\n",
      "Epoch: 1498/2000... Step: 1498... Loss: 0.24902... Val Loss: 0.23453\n",
      "Epoch: 1499/2000... Step: 1499... Loss: 0.24902... Val Loss: 0.23452\n",
      "Epoch: 1500/2000... Step: 1500... Loss: 0.24901... Val Loss: 0.23451\n",
      "Epoch: 1501/2000... Step: 1501... Loss: 0.24901... Val Loss: 0.23450\n",
      "Epoch: 1502/2000... Step: 1502... Loss: 0.24900... Val Loss: 0.23449\n",
      "Epoch: 1503/2000... Step: 1503... Loss: 0.24900... Val Loss: 0.23448\n",
      "Epoch: 1504/2000... Step: 1504... Loss: 0.24899... Val Loss: 0.23447\n",
      "Epoch: 1505/2000... Step: 1505... Loss: 0.24899... Val Loss: 0.23446\n",
      "Epoch: 1506/2000... Step: 1506... Loss: 0.24898... Val Loss: 0.23445\n",
      "Epoch: 1507/2000... Step: 1507... Loss: 0.24898... Val Loss: 0.23444\n",
      "Epoch: 1508/2000... Step: 1508... Loss: 0.24897... Val Loss: 0.23443\n",
      "Epoch: 1509/2000... Step: 1509... Loss: 0.24897... Val Loss: 0.23442\n",
      "Epoch: 1510/2000... Step: 1510... Loss: 0.24897... Val Loss: 0.23441\n",
      "Epoch: 1511/2000... Step: 1511... Loss: 0.24896... Val Loss: 0.23440\n",
      "Epoch: 1512/2000... Step: 1512... Loss: 0.24896... Val Loss: 0.23439\n",
      "Epoch: 1513/2000... Step: 1513... Loss: 0.24895... Val Loss: 0.23438\n",
      "Epoch: 1514/2000... Step: 1514... Loss: 0.24895... Val Loss: 0.23437\n",
      "Epoch: 1515/2000... Step: 1515... Loss: 0.24894... Val Loss: 0.23436\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1516/2000... Step: 1516... Loss: 0.24894... Val Loss: 0.23435\n",
      "Epoch: 1517/2000... Step: 1517... Loss: 0.24893... Val Loss: 0.23434\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1518/2000... Step: 1518... Loss: 0.24893... Val Loss: 0.23433\n",
      "Epoch: 1519/2000... Step: 1519... Loss: 0.24892... Val Loss: 0.23432\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1520/2000... Step: 1520... Loss: 0.24892... Val Loss: 0.23431\n",
      "Epoch: 1521/2000... Step: 1521... Loss: 0.24891... Val Loss: 0.23430\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1522/2000... Step: 1522... Loss: 0.24891... Val Loss: 0.23429\n",
      "Epoch: 1523/2000... Step: 1523... Loss: 0.24890... Val Loss: 0.23428\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1524/2000... Step: 1524... Loss: 0.24890... Val Loss: 0.23427\n",
      "Epoch: 1525/2000... Step: 1525... Loss: 0.24889... Val Loss: 0.23426\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1526/2000... Step: 1526... Loss: 0.24889... Val Loss: 0.23425\n",
      "Epoch: 1527/2000... Step: 1527... Loss: 0.24888... Val Loss: 0.23424\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1528/2000... Step: 1528... Loss: 0.24888... Val Loss: 0.23423\n",
      "Epoch: 1529/2000... Step: 1529... Loss: 0.24888... Val Loss: 0.23422\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1530/2000... Step: 1530... Loss: 0.24887... Val Loss: 0.23421\n",
      "Epoch: 1531/2000... Step: 1531... Loss: 0.24887... Val Loss: 0.23420\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1532/2000... Step: 1532... Loss: 0.24886... Val Loss: 0.23419\n",
      "Epoch: 1533/2000... Step: 1533... Loss: 0.24886... Val Loss: 0.23418\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1534/2000... Step: 1534... Loss: 0.24885... Val Loss: 0.23417\n",
      "Epoch: 1535/2000... Step: 1535... Loss: 0.24885... Val Loss: 0.23416\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1536/2000... Step: 1536... Loss: 0.24884... Val Loss: 0.23415\n",
      "Epoch: 1537/2000... Step: 1537... Loss: 0.24884... Val Loss: 0.23414\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1538/2000... Step: 1538... Loss: 0.24883... Val Loss: 0.23413\n",
      "Epoch: 1539/2000... Step: 1539... Loss: 0.24883... Val Loss: 0.23412\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1540/2000... Step: 1540... Loss: 0.24883... Val Loss: 0.23411\n",
      "Epoch: 1541/2000... Step: 1541... Loss: 0.24882... Val Loss: 0.23410\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1542/2000... Step: 1542... Loss: 0.24882... Val Loss: 0.23409\n",
      "Epoch: 1543/2000... Step: 1543... Loss: 0.24881... Val Loss: 0.23408\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1544/2000... Step: 1544... Loss: 0.24881... Val Loss: 0.23407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1545/2000... Step: 1545... Loss: 0.24880... Val Loss: 0.23407\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1546/2000... Step: 1546... Loss: 0.24880... Val Loss: 0.23406\n",
      "Epoch: 1547/2000... Step: 1547... Loss: 0.24880... Val Loss: 0.23405\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1548/2000... Step: 1548... Loss: 0.24879... Val Loss: 0.23404\n",
      "Epoch: 1549/2000... Step: 1549... Loss: 0.24879... Val Loss: 0.23403\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1550/2000... Step: 1550... Loss: 0.24878... Val Loss: 0.23402\n",
      "Epoch: 1551/2000... Step: 1551... Loss: 0.24878... Val Loss: 0.23401\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1552/2000... Step: 1552... Loss: 0.24877... Val Loss: 0.23400\n",
      "Epoch: 1553/2000... Step: 1553... Loss: 0.24877... Val Loss: 0.23399\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1554/2000... Step: 1554... Loss: 0.24877... Val Loss: 0.23398\n",
      "Epoch: 1555/2000... Step: 1555... Loss: 0.24876... Val Loss: 0.23397\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1556/2000... Step: 1556... Loss: 0.24876... Val Loss: 0.23396\n",
      "Epoch: 1557/2000... Step: 1557... Loss: 0.24875... Val Loss: 0.23395\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1558/2000... Step: 1558... Loss: 0.24875... Val Loss: 0.23395\n",
      "Epoch: 1559/2000... Step: 1559... Loss: 0.24874... Val Loss: 0.23394\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1560/2000... Step: 1560... Loss: 0.24874... Val Loss: 0.23393\n",
      "Epoch: 1561/2000... Step: 1561... Loss: 0.24874... Val Loss: 0.23392\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1562/2000... Step: 1562... Loss: 0.24873... Val Loss: 0.23391\n",
      "Epoch: 1563/2000... Step: 1563... Loss: 0.24873... Val Loss: 0.23390\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1564/2000... Step: 1564... Loss: 0.24872... Val Loss: 0.23389\n",
      "Epoch: 1565/2000... Step: 1565... Loss: 0.24872... Val Loss: 0.23388\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1566/2000... Step: 1566... Loss: 0.24872... Val Loss: 0.23387\n",
      "Epoch: 1567/2000... Step: 1567... Loss: 0.24871... Val Loss: 0.23387\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1568/2000... Step: 1568... Loss: 0.24871... Val Loss: 0.23386\n",
      "Epoch: 1569/2000... Step: 1569... Loss: 0.24870... Val Loss: 0.23385\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1570/2000... Step: 1570... Loss: 0.24870... Val Loss: 0.23384\n",
      "Epoch: 1571/2000... Step: 1571... Loss: 0.24870... Val Loss: 0.23383\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1572/2000... Step: 1572... Loss: 0.24869... Val Loss: 0.23382\n",
      "Epoch: 1573/2000... Step: 1573... Loss: 0.24869... Val Loss: 0.23381\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1574/2000... Step: 1574... Loss: 0.24868... Val Loss: 0.23380\n",
      "Epoch: 1575/2000... Step: 1575... Loss: 0.24868... Val Loss: 0.23380\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1576/2000... Step: 1576... Loss: 0.24868... Val Loss: 0.23379\n",
      "Epoch: 1577/2000... Step: 1577... Loss: 0.24867... Val Loss: 0.23378\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1578/2000... Step: 1578... Loss: 0.24867... Val Loss: 0.23377\n",
      "Epoch: 1579/2000... Step: 1579... Loss: 0.24866... Val Loss: 0.23376\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1580/2000... Step: 1580... Loss: 0.24866... Val Loss: 0.23375\n",
      "Epoch: 1581/2000... Step: 1581... Loss: 0.24866... Val Loss: 0.23374\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1582/2000... Step: 1582... Loss: 0.24865... Val Loss: 0.23373\n",
      "Epoch: 1583/2000... Step: 1583... Loss: 0.24865... Val Loss: 0.23373\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1584/2000... Step: 1584... Loss: 0.24865... Val Loss: 0.23372\n",
      "Epoch: 1585/2000... Step: 1585... Loss: 0.24864... Val Loss: 0.23371\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1586/2000... Step: 1586... Loss: 0.24864... Val Loss: 0.23370\n",
      "Epoch: 1587/2000... Step: 1587... Loss: 0.24863... Val Loss: 0.23369\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1588/2000... Step: 1588... Loss: 0.24863... Val Loss: 0.23368\n",
      "Epoch: 1589/2000... Step: 1589... Loss: 0.24863... Val Loss: 0.23368\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1590/2000... Step: 1590... Loss: 0.24862... Val Loss: 0.23367\n",
      "Epoch: 1591/2000... Step: 1591... Loss: 0.24862... Val Loss: 0.23366\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1592/2000... Step: 1592... Loss: 0.24862... Val Loss: 0.23365\n",
      "Epoch: 1593/2000... Step: 1593... Loss: 0.24861... Val Loss: 0.23364\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1594/2000... Step: 1594... Loss: 0.24861... Val Loss: 0.23363\n",
      "Epoch: 1595/2000... Step: 1595... Loss: 0.24860... Val Loss: 0.23363\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1596/2000... Step: 1596... Loss: 0.24860... Val Loss: 0.23362\n",
      "Epoch: 1597/2000... Step: 1597... Loss: 0.24860... Val Loss: 0.23361\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1598/2000... Step: 1598... Loss: 0.24859... Val Loss: 0.23360\n",
      "Epoch: 1599/2000... Step: 1599... Loss: 0.24859... Val Loss: 0.23359\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1600/2000... Step: 1600... Loss: 0.24859... Val Loss: 0.23358\n",
      "Epoch: 1601/2000... Step: 1601... Loss: 0.24858... Val Loss: 0.23358\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1602/2000... Step: 1602... Loss: 0.24858... Val Loss: 0.23357\n",
      "Epoch: 1603/2000... Step: 1603... Loss: 0.24858... Val Loss: 0.23356\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1604/2000... Step: 1604... Loss: 0.24857... Val Loss: 0.23355\n",
      "Epoch: 1605/2000... Step: 1605... Loss: 0.24857... Val Loss: 0.23354\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1606/2000... Step: 1606... Loss: 0.24856... Val Loss: 0.23354\n",
      "Epoch: 1607/2000... Step: 1607... Loss: 0.24856... Val Loss: 0.23353\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1608/2000... Step: 1608... Loss: 0.24856... Val Loss: 0.23352\n",
      "Epoch: 1609/2000... Step: 1609... Loss: 0.24855... Val Loss: 0.23351\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1610/2000... Step: 1610... Loss: 0.24855... Val Loss: 0.23350\n",
      "Epoch: 1611/2000... Step: 1611... Loss: 0.24855... Val Loss: 0.23350\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1612/2000... Step: 1612... Loss: 0.24854... Val Loss: 0.23349\n",
      "Epoch: 1613/2000... Step: 1613... Loss: 0.24854... Val Loss: 0.23348\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1614/2000... Step: 1614... Loss: 0.24854... Val Loss: 0.23347\n",
      "Epoch: 1615/2000... Step: 1615... Loss: 0.24853... Val Loss: 0.23346\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1616/2000... Step: 1616... Loss: 0.24853... Val Loss: 0.23346\n",
      "Epoch: 1617/2000... Step: 1617... Loss: 0.24853... Val Loss: 0.23345\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1618/2000... Step: 1618... Loss: 0.24852... Val Loss: 0.23344\n",
      "Epoch: 1619/2000... Step: 1619... Loss: 0.24852... Val Loss: 0.23343\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1620/2000... Step: 1620... Loss: 0.24852... Val Loss: 0.23342\n",
      "Epoch: 1621/2000... Step: 1621... Loss: 0.24851... Val Loss: 0.23342\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1622/2000... Step: 1622... Loss: 0.24851... Val Loss: 0.23341\n",
      "Epoch: 1623/2000... Step: 1623... Loss: 0.24851... Val Loss: 0.23340\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1624/2000... Step: 1624... Loss: 0.24850... Val Loss: 0.23339\n",
      "Epoch: 1625/2000... Step: 1625... Loss: 0.24850... Val Loss: 0.23339\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1626/2000... Step: 1626... Loss: 0.24850... Val Loss: 0.23338\n",
      "Epoch: 1627/2000... Step: 1627... Loss: 0.24849... Val Loss: 0.23337\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1628/2000... Step: 1628... Loss: 0.24849... Val Loss: 0.23336\n",
      "Epoch: 1629/2000... Step: 1629... Loss: 0.24849... Val Loss: 0.23335\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1630/2000... Step: 1630... Loss: 0.24848... Val Loss: 0.23335\n",
      "Epoch: 1631/2000... Step: 1631... Loss: 0.24848... Val Loss: 0.23334\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1632/2000... Step: 1632... Loss: 0.24848... Val Loss: 0.23333\n",
      "Epoch: 1633/2000... Step: 1633... Loss: 0.24847... Val Loss: 0.23332\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1634/2000... Step: 1634... Loss: 0.24847... Val Loss: 0.23332\n",
      "Epoch: 1635/2000... Step: 1635... Loss: 0.24847... Val Loss: 0.23331\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1636/2000... Step: 1636... Loss: 0.24846... Val Loss: 0.23330\n",
      "Epoch: 1637/2000... Step: 1637... Loss: 0.24846... Val Loss: 0.23329\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1638/2000... Step: 1638... Loss: 0.24846... Val Loss: 0.23329\n",
      "Epoch: 1639/2000... Step: 1639... Loss: 0.24845... Val Loss: 0.23328\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1640/2000... Step: 1640... Loss: 0.24845... Val Loss: 0.23327\n",
      "Epoch: 1641/2000... Step: 1641... Loss: 0.24845... Val Loss: 0.23326\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1642/2000... Step: 1642... Loss: 0.24845... Val Loss: 0.23326\n",
      "Epoch: 1643/2000... Step: 1643... Loss: 0.24844... Val Loss: 0.23325\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1644/2000... Step: 1644... Loss: 0.24844... Val Loss: 0.23324\n",
      "Epoch: 1645/2000... Step: 1645... Loss: 0.24844... Val Loss: 0.23323\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1646/2000... Step: 1646... Loss: 0.24843... Val Loss: 0.23323\n",
      "Epoch: 1647/2000... Step: 1647... Loss: 0.24843... Val Loss: 0.23322\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1648/2000... Step: 1648... Loss: 0.24843... Val Loss: 0.23321\n",
      "Epoch: 1649/2000... Step: 1649... Loss: 0.24842... Val Loss: 0.23321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1650/2000... Step: 1650... Loss: 0.24842... Val Loss: 0.23320\n",
      "Epoch: 1651/2000... Step: 1651... Loss: 0.24842... Val Loss: 0.23319\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1652/2000... Step: 1652... Loss: 0.24841... Val Loss: 0.23318\n",
      "Epoch: 1653/2000... Step: 1653... Loss: 0.24841... Val Loss: 0.23318\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1654/2000... Step: 1654... Loss: 0.24841... Val Loss: 0.23317\n",
      "Epoch: 1655/2000... Step: 1655... Loss: 0.24841... Val Loss: 0.23316\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1656/2000... Step: 1656... Loss: 0.24840... Val Loss: 0.23315\n",
      "Epoch: 1657/2000... Step: 1657... Loss: 0.24840... Val Loss: 0.23315\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1658/2000... Step: 1658... Loss: 0.24840... Val Loss: 0.23314\n",
      "Epoch: 1659/2000... Step: 1659... Loss: 0.24839... Val Loss: 0.23313\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1660/2000... Step: 1660... Loss: 0.24839... Val Loss: 0.23313\n",
      "Epoch: 1661/2000... Step: 1661... Loss: 0.24839... Val Loss: 0.23312\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1662/2000... Step: 1662... Loss: 0.24839... Val Loss: 0.23311\n",
      "Epoch: 1663/2000... Step: 1663... Loss: 0.24838... Val Loss: 0.23311\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1664/2000... Step: 1664... Loss: 0.24838... Val Loss: 0.23310\n",
      "Epoch: 1665/2000... Step: 1665... Loss: 0.24838... Val Loss: 0.23309\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1666/2000... Step: 1666... Loss: 0.24837... Val Loss: 0.23308\n",
      "Epoch: 1667/2000... Step: 1667... Loss: 0.24837... Val Loss: 0.23308\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1668/2000... Step: 1668... Loss: 0.24837... Val Loss: 0.23307\n",
      "Epoch: 1669/2000... Step: 1669... Loss: 0.24836... Val Loss: 0.23306\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1670/2000... Step: 1670... Loss: 0.24836... Val Loss: 0.23306\n",
      "Epoch: 1671/2000... Step: 1671... Loss: 0.24836... Val Loss: 0.23305\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1672/2000... Step: 1672... Loss: 0.24836... Val Loss: 0.23304\n",
      "Epoch: 1673/2000... Step: 1673... Loss: 0.24835... Val Loss: 0.23304\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1674/2000... Step: 1674... Loss: 0.24835... Val Loss: 0.23303\n",
      "Epoch: 1675/2000... Step: 1675... Loss: 0.24835... Val Loss: 0.23302\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1676/2000... Step: 1676... Loss: 0.24835... Val Loss: 0.23301\n",
      "Epoch: 1677/2000... Step: 1677... Loss: 0.24834... Val Loss: 0.23301\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1678/2000... Step: 1678... Loss: 0.24834... Val Loss: 0.23300\n",
      "Epoch: 1679/2000... Step: 1679... Loss: 0.24834... Val Loss: 0.23299\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1680/2000... Step: 1680... Loss: 0.24833... Val Loss: 0.23299\n",
      "Epoch: 1681/2000... Step: 1681... Loss: 0.24833... Val Loss: 0.23298\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1682/2000... Step: 1682... Loss: 0.24833... Val Loss: 0.23297\n",
      "Epoch: 1683/2000... Step: 1683... Loss: 0.24833... Val Loss: 0.23297\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1684/2000... Step: 1684... Loss: 0.24832... Val Loss: 0.23296\n",
      "Epoch: 1685/2000... Step: 1685... Loss: 0.24832... Val Loss: 0.23295\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1686/2000... Step: 1686... Loss: 0.24832... Val Loss: 0.23295\n",
      "Epoch: 1687/2000... Step: 1687... Loss: 0.24832... Val Loss: 0.23294\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1688/2000... Step: 1688... Loss: 0.24831... Val Loss: 0.23293\n",
      "Epoch: 1689/2000... Step: 1689... Loss: 0.24831... Val Loss: 0.23293\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1690/2000... Step: 1690... Loss: 0.24831... Val Loss: 0.23292\n",
      "Epoch: 1691/2000... Step: 1691... Loss: 0.24830... Val Loss: 0.23291\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1692/2000... Step: 1692... Loss: 0.24830... Val Loss: 0.23291\n",
      "Epoch: 1693/2000... Step: 1693... Loss: 0.24830... Val Loss: 0.23290\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1694/2000... Step: 1694... Loss: 0.24830... Val Loss: 0.23289\n",
      "Epoch: 1695/2000... Step: 1695... Loss: 0.24829... Val Loss: 0.23289\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1696/2000... Step: 1696... Loss: 0.24829... Val Loss: 0.23288\n",
      "Epoch: 1697/2000... Step: 1697... Loss: 0.24829... Val Loss: 0.23287\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1698/2000... Step: 1698... Loss: 0.24829... Val Loss: 0.23287\n",
      "Epoch: 1699/2000... Step: 1699... Loss: 0.24828... Val Loss: 0.23286\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1700/2000... Step: 1700... Loss: 0.24828... Val Loss: 0.23285\n",
      "Epoch: 1701/2000... Step: 1701... Loss: 0.24828... Val Loss: 0.23285\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1702/2000... Step: 1702... Loss: 0.24828... Val Loss: 0.23284\n",
      "Epoch: 1703/2000... Step: 1703... Loss: 0.24827... Val Loss: 0.23284\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1704/2000... Step: 1704... Loss: 0.24827... Val Loss: 0.23283\n",
      "Epoch: 1705/2000... Step: 1705... Loss: 0.24827... Val Loss: 0.23282\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1706/2000... Step: 1706... Loss: 0.24827... Val Loss: 0.23282\n",
      "Epoch: 1707/2000... Step: 1707... Loss: 0.24826... Val Loss: 0.23281\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1708/2000... Step: 1708... Loss: 0.24826... Val Loss: 0.23280\n",
      "Epoch: 1709/2000... Step: 1709... Loss: 0.24826... Val Loss: 0.23280\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1710/2000... Step: 1710... Loss: 0.24826... Val Loss: 0.23279\n",
      "Epoch: 1711/2000... Step: 1711... Loss: 0.24825... Val Loss: 0.23278\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1712/2000... Step: 1712... Loss: 0.24825... Val Loss: 0.23278\n",
      "Epoch: 1713/2000... Step: 1713... Loss: 0.24825... Val Loss: 0.23277\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1714/2000... Step: 1714... Loss: 0.24825... Val Loss: 0.23277\n",
      "Epoch: 1715/2000... Step: 1715... Loss: 0.24824... Val Loss: 0.23276\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1716/2000... Step: 1716... Loss: 0.24824... Val Loss: 0.23275\n",
      "Epoch: 1717/2000... Step: 1717... Loss: 0.24824... Val Loss: 0.23275\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1718/2000... Step: 1718... Loss: 0.24824... Val Loss: 0.23274\n",
      "Epoch: 1719/2000... Step: 1719... Loss: 0.24823... Val Loss: 0.23273\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1720/2000... Step: 1720... Loss: 0.24823... Val Loss: 0.23273\n",
      "Epoch: 1721/2000... Step: 1721... Loss: 0.24823... Val Loss: 0.23272\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1722/2000... Step: 1722... Loss: 0.24823... Val Loss: 0.23272\n",
      "Epoch: 1723/2000... Step: 1723... Loss: 0.24823... Val Loss: 0.23271\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1724/2000... Step: 1724... Loss: 0.24822... Val Loss: 0.23270\n",
      "Epoch: 1725/2000... Step: 1725... Loss: 0.24822... Val Loss: 0.23270\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1726/2000... Step: 1726... Loss: 0.24822... Val Loss: 0.23269\n",
      "Epoch: 1727/2000... Step: 1727... Loss: 0.24822... Val Loss: 0.23269\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1728/2000... Step: 1728... Loss: 0.24821... Val Loss: 0.23268\n",
      "Epoch: 1729/2000... Step: 1729... Loss: 0.24821... Val Loss: 0.23267\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1730/2000... Step: 1730... Loss: 0.24821... Val Loss: 0.23267\n",
      "Epoch: 1731/2000... Step: 1731... Loss: 0.24821... Val Loss: 0.23266\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1732/2000... Step: 1732... Loss: 0.24820... Val Loss: 0.23266\n",
      "Epoch: 1733/2000... Step: 1733... Loss: 0.24820... Val Loss: 0.23265\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1734/2000... Step: 1734... Loss: 0.24820... Val Loss: 0.23264\n",
      "Epoch: 1735/2000... Step: 1735... Loss: 0.24820... Val Loss: 0.23264\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1736/2000... Step: 1736... Loss: 0.24820... Val Loss: 0.23263\n",
      "Epoch: 1737/2000... Step: 1737... Loss: 0.24819... Val Loss: 0.23263\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1738/2000... Step: 1738... Loss: 0.24819... Val Loss: 0.23262\n",
      "Epoch: 1739/2000... Step: 1739... Loss: 0.24819... Val Loss: 0.23261\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1740/2000... Step: 1740... Loss: 0.24819... Val Loss: 0.23261\n",
      "Epoch: 1741/2000... Step: 1741... Loss: 0.24818... Val Loss: 0.23260\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1742/2000... Step: 1742... Loss: 0.24818... Val Loss: 0.23260\n",
      "Epoch: 1743/2000... Step: 1743... Loss: 0.24818... Val Loss: 0.23259\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1744/2000... Step: 1744... Loss: 0.24818... Val Loss: 0.23258\n",
      "Epoch: 1745/2000... Step: 1745... Loss: 0.24818... Val Loss: 0.23258\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1746/2000... Step: 1746... Loss: 0.24817... Val Loss: 0.23257\n",
      "Epoch: 1747/2000... Step: 1747... Loss: 0.24817... Val Loss: 0.23257\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1748/2000... Step: 1748... Loss: 0.24817... Val Loss: 0.23256\n",
      "Epoch: 1749/2000... Step: 1749... Loss: 0.24817... Val Loss: 0.23256\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1750/2000... Step: 1750... Loss: 0.24817... Val Loss: 0.23255\n",
      "Epoch: 1751/2000... Step: 1751... Loss: 0.24816... Val Loss: 0.23254\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1752/2000... Step: 1752... Loss: 0.24816... Val Loss: 0.23254\n",
      "Epoch: 1753/2000... Step: 1753... Loss: 0.24816... Val Loss: 0.23253\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1754/2000... Step: 1754... Loss: 0.24816... Val Loss: 0.23253\n",
      "Epoch: 1755/2000... Step: 1755... Loss: 0.24815... Val Loss: 0.23252\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1756/2000... Step: 1756... Loss: 0.24815... Val Loss: 0.23252\n",
      "Epoch: 1757/2000... Step: 1757... Loss: 0.24815... Val Loss: 0.23251\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1758/2000... Step: 1758... Loss: 0.24815... Val Loss: 0.23250\n",
      "Epoch: 1759/2000... Step: 1759... Loss: 0.24815... Val Loss: 0.23250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1760/2000... Step: 1760... Loss: 0.24814... Val Loss: 0.23249\n",
      "Epoch: 1761/2000... Step: 1761... Loss: 0.24814... Val Loss: 0.23249\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1762/2000... Step: 1762... Loss: 0.24814... Val Loss: 0.23248\n",
      "Epoch: 1763/2000... Step: 1763... Loss: 0.24814... Val Loss: 0.23248\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1764/2000... Step: 1764... Loss: 0.24814... Val Loss: 0.23247\n",
      "Epoch: 1765/2000... Step: 1765... Loss: 0.24813... Val Loss: 0.23247\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1766/2000... Step: 1766... Loss: 0.24813... Val Loss: 0.23246\n",
      "Epoch: 1767/2000... Step: 1767... Loss: 0.24813... Val Loss: 0.23245\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1768/2000... Step: 1768... Loss: 0.24813... Val Loss: 0.23245\n",
      "Epoch: 1769/2000... Step: 1769... Loss: 0.24813... Val Loss: 0.23244\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1770/2000... Step: 1770... Loss: 0.24812... Val Loss: 0.23244\n",
      "Epoch: 1771/2000... Step: 1771... Loss: 0.24812... Val Loss: 0.23243\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1772/2000... Step: 1772... Loss: 0.24812... Val Loss: 0.23243\n",
      "Epoch: 1773/2000... Step: 1773... Loss: 0.24812... Val Loss: 0.23242\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1774/2000... Step: 1774... Loss: 0.24812... Val Loss: 0.23242\n",
      "Epoch: 1775/2000... Step: 1775... Loss: 0.24811... Val Loss: 0.23241\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1776/2000... Step: 1776... Loss: 0.24811... Val Loss: 0.23240\n",
      "Epoch: 1777/2000... Step: 1777... Loss: 0.24811... Val Loss: 0.23240\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1778/2000... Step: 1778... Loss: 0.24811... Val Loss: 0.23239\n",
      "Epoch: 1779/2000... Step: 1779... Loss: 0.24811... Val Loss: 0.23239\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1780/2000... Step: 1780... Loss: 0.24810... Val Loss: 0.23238\n",
      "Epoch: 1781/2000... Step: 1781... Loss: 0.24810... Val Loss: 0.23238\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1782/2000... Step: 1782... Loss: 0.24810... Val Loss: 0.23237\n",
      "Epoch: 1783/2000... Step: 1783... Loss: 0.24810... Val Loss: 0.23237\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1784/2000... Step: 1784... Loss: 0.24810... Val Loss: 0.23236\n",
      "Epoch: 1785/2000... Step: 1785... Loss: 0.24810... Val Loss: 0.23236\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1786/2000... Step: 1786... Loss: 0.24809... Val Loss: 0.23235\n",
      "Epoch: 1787/2000... Step: 1787... Loss: 0.24809... Val Loss: 0.23235\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1788/2000... Step: 1788... Loss: 0.24809... Val Loss: 0.23234\n",
      "Epoch: 1789/2000... Step: 1789... Loss: 0.24809... Val Loss: 0.23234\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1790/2000... Step: 1790... Loss: 0.24809... Val Loss: 0.23233\n",
      "Epoch: 1791/2000... Step: 1791... Loss: 0.24808... Val Loss: 0.23232\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1792/2000... Step: 1792... Loss: 0.24808... Val Loss: 0.23232\n",
      "Epoch: 1793/2000... Step: 1793... Loss: 0.24808... Val Loss: 0.23231\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1794/2000... Step: 1794... Loss: 0.24808... Val Loss: 0.23231\n",
      "Epoch: 1795/2000... Step: 1795... Loss: 0.24808... Val Loss: 0.23230\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1796/2000... Step: 1796... Loss: 0.24807... Val Loss: 0.23230\n",
      "Epoch: 1797/2000... Step: 1797... Loss: 0.24807... Val Loss: 0.23229\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1798/2000... Step: 1798... Loss: 0.24807... Val Loss: 0.23229\n",
      "Epoch: 1799/2000... Step: 1799... Loss: 0.24807... Val Loss: 0.23228\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1800/2000... Step: 1800... Loss: 0.24807... Val Loss: 0.23228\n",
      "Epoch: 1801/2000... Step: 1801... Loss: 0.24807... Val Loss: 0.23227\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1802/2000... Step: 1802... Loss: 0.24806... Val Loss: 0.23227\n",
      "Epoch: 1803/2000... Step: 1803... Loss: 0.24806... Val Loss: 0.23226\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1804/2000... Step: 1804... Loss: 0.24806... Val Loss: 0.23226\n",
      "Epoch: 1805/2000... Step: 1805... Loss: 0.24806... Val Loss: 0.23225\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1806/2000... Step: 1806... Loss: 0.24806... Val Loss: 0.23225\n",
      "Epoch: 1807/2000... Step: 1807... Loss: 0.24806... Val Loss: 0.23224\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1808/2000... Step: 1808... Loss: 0.24805... Val Loss: 0.23224\n",
      "Epoch: 1809/2000... Step: 1809... Loss: 0.24805... Val Loss: 0.23223\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1810/2000... Step: 1810... Loss: 0.24805... Val Loss: 0.23223\n",
      "Epoch: 1811/2000... Step: 1811... Loss: 0.24805... Val Loss: 0.23222\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1812/2000... Step: 1812... Loss: 0.24805... Val Loss: 0.23222\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1813/2000... Step: 1813... Loss: 0.24805... Val Loss: 0.23221\n",
      "Epoch: 1814/2000... Step: 1814... Loss: 0.24804... Val Loss: 0.23221\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1815/2000... Step: 1815... Loss: 0.24804... Val Loss: 0.23220\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1816/2000... Step: 1816... Loss: 0.24804... Val Loss: 0.23220\n",
      "Epoch: 1817/2000... Step: 1817... Loss: 0.24804... Val Loss: 0.23219\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1818/2000... Step: 1818... Loss: 0.24804... Val Loss: 0.23219\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1819/2000... Step: 1819... Loss: 0.24804... Val Loss: 0.23218\n",
      "Epoch: 1820/2000... Step: 1820... Loss: 0.24803... Val Loss: 0.23218\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1821/2000... Step: 1821... Loss: 0.24803... Val Loss: 0.23217\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1822/2000... Step: 1822... Loss: 0.24803... Val Loss: 0.23217\n",
      "Epoch: 1823/2000... Step: 1823... Loss: 0.24803... Val Loss: 0.23216\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1824/2000... Step: 1824... Loss: 0.24803... Val Loss: 0.23216\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1825/2000... Step: 1825... Loss: 0.24803... Val Loss: 0.23215\n",
      "Epoch: 1826/2000... Step: 1826... Loss: 0.24802... Val Loss: 0.23215\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1827/2000... Step: 1827... Loss: 0.24802... Val Loss: 0.23214\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1828/2000... Step: 1828... Loss: 0.24802... Val Loss: 0.23214\n",
      "Epoch: 1829/2000... Step: 1829... Loss: 0.24802... Val Loss: 0.23213\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1830/2000... Step: 1830... Loss: 0.24802... Val Loss: 0.23213\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1831/2000... Step: 1831... Loss: 0.24802... Val Loss: 0.23213\n",
      "Epoch: 1832/2000... Step: 1832... Loss: 0.24801... Val Loss: 0.23212\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1833/2000... Step: 1833... Loss: 0.24801... Val Loss: 0.23212\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1834/2000... Step: 1834... Loss: 0.24801... Val Loss: 0.23211\n",
      "Epoch: 1835/2000... Step: 1835... Loss: 0.24801... Val Loss: 0.23211\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1836/2000... Step: 1836... Loss: 0.24801... Val Loss: 0.23210\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1837/2000... Step: 1837... Loss: 0.24801... Val Loss: 0.23210\n",
      "Epoch: 1838/2000... Step: 1838... Loss: 0.24800... Val Loss: 0.23209\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1839/2000... Step: 1839... Loss: 0.24800... Val Loss: 0.23209\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1840/2000... Step: 1840... Loss: 0.24800... Val Loss: 0.23208\n",
      "Epoch: 1841/2000... Step: 1841... Loss: 0.24800... Val Loss: 0.23208\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1842/2000... Step: 1842... Loss: 0.24800... Val Loss: 0.23207\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1843/2000... Step: 1843... Loss: 0.24800... Val Loss: 0.23207\n",
      "Epoch: 1844/2000... Step: 1844... Loss: 0.24800... Val Loss: 0.23206\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1845/2000... Step: 1845... Loss: 0.24799... Val Loss: 0.23206\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1846/2000... Step: 1846... Loss: 0.24799... Val Loss: 0.23205\n",
      "Epoch: 1847/2000... Step: 1847... Loss: 0.24799... Val Loss: 0.23205\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1848/2000... Step: 1848... Loss: 0.24799... Val Loss: 0.23205\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1849/2000... Step: 1849... Loss: 0.24799... Val Loss: 0.23204\n",
      "Epoch: 1850/2000... Step: 1850... Loss: 0.24799... Val Loss: 0.23204\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1851/2000... Step: 1851... Loss: 0.24799... Val Loss: 0.23203\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1852/2000... Step: 1852... Loss: 0.24798... Val Loss: 0.23203\n",
      "Epoch: 1853/2000... Step: 1853... Loss: 0.24798... Val Loss: 0.23202\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1854/2000... Step: 1854... Loss: 0.24798... Val Loss: 0.23202\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1855/2000... Step: 1855... Loss: 0.24798... Val Loss: 0.23201\n",
      "Epoch: 1856/2000... Step: 1856... Loss: 0.24798... Val Loss: 0.23201\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1857/2000... Step: 1857... Loss: 0.24798... Val Loss: 0.23200\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1858/2000... Step: 1858... Loss: 0.24798... Val Loss: 0.23200\n",
      "Epoch: 1859/2000... Step: 1859... Loss: 0.24797... Val Loss: 0.23200\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1860/2000... Step: 1860... Loss: 0.24797... Val Loss: 0.23199\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1861/2000... Step: 1861... Loss: 0.24797... Val Loss: 0.23199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1862/2000... Step: 1862... Loss: 0.24797... Val Loss: 0.23198\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1863/2000... Step: 1863... Loss: 0.24797... Val Loss: 0.23198\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1864/2000... Step: 1864... Loss: 0.24797... Val Loss: 0.23197\n",
      "Epoch: 1865/2000... Step: 1865... Loss: 0.24797... Val Loss: 0.23197\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1866/2000... Step: 1866... Loss: 0.24796... Val Loss: 0.23197\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1867/2000... Step: 1867... Loss: 0.24796... Val Loss: 0.23196\n",
      "Epoch: 1868/2000... Step: 1868... Loss: 0.24796... Val Loss: 0.23196\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1869/2000... Step: 1869... Loss: 0.24796... Val Loss: 0.23195\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1870/2000... Step: 1870... Loss: 0.24796... Val Loss: 0.23195\n",
      "Epoch: 1871/2000... Step: 1871... Loss: 0.24796... Val Loss: 0.23194\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1872/2000... Step: 1872... Loss: 0.24796... Val Loss: 0.23194\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1873/2000... Step: 1873... Loss: 0.24795... Val Loss: 0.23193\n",
      "Epoch: 1874/2000... Step: 1874... Loss: 0.24795... Val Loss: 0.23193\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1875/2000... Step: 1875... Loss: 0.24795... Val Loss: 0.23193\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1876/2000... Step: 1876... Loss: 0.24795... Val Loss: 0.23192\n",
      "Epoch: 1877/2000... Step: 1877... Loss: 0.24795... Val Loss: 0.23192\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1878/2000... Step: 1878... Loss: 0.24795... Val Loss: 0.23191\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1879/2000... Step: 1879... Loss: 0.24795... Val Loss: 0.23191\n",
      "Epoch: 1880/2000... Step: 1880... Loss: 0.24794... Val Loss: 0.23190\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1881/2000... Step: 1881... Loss: 0.24794... Val Loss: 0.23190\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1882/2000... Step: 1882... Loss: 0.24794... Val Loss: 0.23190\n",
      "Epoch: 1883/2000... Step: 1883... Loss: 0.24794... Val Loss: 0.23189\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1884/2000... Step: 1884... Loss: 0.24794... Val Loss: 0.23189\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1885/2000... Step: 1885... Loss: 0.24794... Val Loss: 0.23188\n",
      "Epoch: 1886/2000... Step: 1886... Loss: 0.24794... Val Loss: 0.23188\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1887/2000... Step: 1887... Loss: 0.24794... Val Loss: 0.23188\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1888/2000... Step: 1888... Loss: 0.24793... Val Loss: 0.23187\n",
      "Epoch: 1889/2000... Step: 1889... Loss: 0.24793... Val Loss: 0.23187\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1890/2000... Step: 1890... Loss: 0.24793... Val Loss: 0.23186\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1891/2000... Step: 1891... Loss: 0.24793... Val Loss: 0.23186\n",
      "Epoch: 1892/2000... Step: 1892... Loss: 0.24793... Val Loss: 0.23185\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1893/2000... Step: 1893... Loss: 0.24793... Val Loss: 0.23185\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1894/2000... Step: 1894... Loss: 0.24793... Val Loss: 0.23185\n",
      "Epoch: 1895/2000... Step: 1895... Loss: 0.24793... Val Loss: 0.23184\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1896/2000... Step: 1896... Loss: 0.24792... Val Loss: 0.23184\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1897/2000... Step: 1897... Loss: 0.24792... Val Loss: 0.23183\n",
      "Epoch: 1898/2000... Step: 1898... Loss: 0.24792... Val Loss: 0.23183\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1899/2000... Step: 1899... Loss: 0.24792... Val Loss: 0.23183\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1900/2000... Step: 1900... Loss: 0.24792... Val Loss: 0.23182\n",
      "Epoch: 1901/2000... Step: 1901... Loss: 0.24792... Val Loss: 0.23182\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1902/2000... Step: 1902... Loss: 0.24792... Val Loss: 0.23181\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1903/2000... Step: 1903... Loss: 0.24792... Val Loss: 0.23181\n",
      "Epoch: 1904/2000... Step: 1904... Loss: 0.24791... Val Loss: 0.23181\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1905/2000... Step: 1905... Loss: 0.24791... Val Loss: 0.23180\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1906/2000... Step: 1906... Loss: 0.24791... Val Loss: 0.23180\n",
      "Epoch: 1907/2000... Step: 1907... Loss: 0.24791... Val Loss: 0.23179\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1908/2000... Step: 1908... Loss: 0.24791... Val Loss: 0.23179\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1909/2000... Step: 1909... Loss: 0.24791... Val Loss: 0.23179\n",
      "Epoch: 1910/2000... Step: 1910... Loss: 0.24791... Val Loss: 0.23178\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1911/2000... Step: 1911... Loss: 0.24791... Val Loss: 0.23178\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1912/2000... Step: 1912... Loss: 0.24791... Val Loss: 0.23177\n",
      "Epoch: 1913/2000... Step: 1913... Loss: 0.24790... Val Loss: 0.23177\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1914/2000... Step: 1914... Loss: 0.24790... Val Loss: 0.23177\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1915/2000... Step: 1915... Loss: 0.24790... Val Loss: 0.23176\n",
      "Epoch: 1916/2000... Step: 1916... Loss: 0.24790... Val Loss: 0.23176\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1917/2000... Step: 1917... Loss: 0.24790... Val Loss: 0.23175\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1918/2000... Step: 1918... Loss: 0.24790... Val Loss: 0.23175\n",
      "Epoch: 1919/2000... Step: 1919... Loss: 0.24790... Val Loss: 0.23175\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1920/2000... Step: 1920... Loss: 0.24790... Val Loss: 0.23174\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1921/2000... Step: 1921... Loss: 0.24789... Val Loss: 0.23174\n",
      "Epoch: 1922/2000... Step: 1922... Loss: 0.24789... Val Loss: 0.23173\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1923/2000... Step: 1923... Loss: 0.24789... Val Loss: 0.23173\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1924/2000... Step: 1924... Loss: 0.24789... Val Loss: 0.23173\n",
      "Epoch: 1925/2000... Step: 1925... Loss: 0.24789... Val Loss: 0.23172\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1926/2000... Step: 1926... Loss: 0.24789... Val Loss: 0.23172\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1927/2000... Step: 1927... Loss: 0.24789... Val Loss: 0.23172\n",
      "Epoch: 1928/2000... Step: 1928... Loss: 0.24789... Val Loss: 0.23171\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1929/2000... Step: 1929... Loss: 0.24789... Val Loss: 0.23171\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1930/2000... Step: 1930... Loss: 0.24788... Val Loss: 0.23170\n",
      "Epoch: 1931/2000... Step: 1931... Loss: 0.24788... Val Loss: 0.23170\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1932/2000... Step: 1932... Loss: 0.24788... Val Loss: 0.23170\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1933/2000... Step: 1933... Loss: 0.24788... Val Loss: 0.23169\n",
      "Epoch: 1934/2000... Step: 1934... Loss: 0.24788... Val Loss: 0.23169\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1935/2000... Step: 1935... Loss: 0.24788... Val Loss: 0.23169\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1936/2000... Step: 1936... Loss: 0.24788... Val Loss: 0.23168\n",
      "Epoch: 1937/2000... Step: 1937... Loss: 0.24788... Val Loss: 0.23168\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1938/2000... Step: 1938... Loss: 0.24788... Val Loss: 0.23167\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1939/2000... Step: 1939... Loss: 0.24788... Val Loss: 0.23167\n",
      "Epoch: 1940/2000... Step: 1940... Loss: 0.24787... Val Loss: 0.23167\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1941/2000... Step: 1941... Loss: 0.24787... Val Loss: 0.23166\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1942/2000... Step: 1942... Loss: 0.24787... Val Loss: 0.23166\n",
      "Epoch: 1943/2000... Step: 1943... Loss: 0.24787... Val Loss: 0.23166\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1944/2000... Step: 1944... Loss: 0.24787... Val Loss: 0.23165\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1945/2000... Step: 1945... Loss: 0.24787... Val Loss: 0.23165\n",
      "Epoch: 1946/2000... Step: 1946... Loss: 0.24787... Val Loss: 0.23164\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1947/2000... Step: 1947... Loss: 0.24787... Val Loss: 0.23164\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1948/2000... Step: 1948... Loss: 0.24787... Val Loss: 0.23164\n",
      "Epoch: 1949/2000... Step: 1949... Loss: 0.24786... Val Loss: 0.23163\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1950/2000... Step: 1950... Loss: 0.24786... Val Loss: 0.23163\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1951/2000... Step: 1951... Loss: 0.24786... Val Loss: 0.23163\n",
      "Epoch: 1952/2000... Step: 1952... Loss: 0.24786... Val Loss: 0.23162\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1953/2000... Step: 1953... Loss: 0.24786... Val Loss: 0.23162\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1954/2000... Step: 1954... Loss: 0.24786... Val Loss: 0.23162\n",
      "Epoch: 1955/2000... Step: 1955... Loss: 0.24786... Val Loss: 0.23161\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1956/2000... Step: 1956... Loss: 0.24786... Val Loss: 0.23161\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1957/2000... Step: 1957... Loss: 0.24786... Val Loss: 0.23160\n",
      "Epoch: 1958/2000... Step: 1958... Loss: 0.24786... Val Loss: 0.23160\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1959/2000... Step: 1959... Loss: 0.24785... Val Loss: 0.23160\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1960/2000... Step: 1960... Loss: 0.24785... Val Loss: 0.23159\n",
      "Epoch: 1961/2000... Step: 1961... Loss: 0.24785... Val Loss: 0.23159\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1962/2000... Step: 1962... Loss: 0.24785... Val Loss: 0.23159\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1963/2000... Step: 1963... Loss: 0.24785... Val Loss: 0.23158\n",
      "Epoch: 1964/2000... Step: 1964... Loss: 0.24785... Val Loss: 0.23158\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1965/2000... Step: 1965... Loss: 0.24785... Val Loss: 0.23158\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1966/2000... Step: 1966... Loss: 0.24785... Val Loss: 0.23157\n",
      "Epoch: 1967/2000... Step: 1967... Loss: 0.24785... Val Loss: 0.23157\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1968/2000... Step: 1968... Loss: 0.24785... Val Loss: 0.23157\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1969/2000... Step: 1969... Loss: 0.24785... Val Loss: 0.23156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1970/2000... Step: 1970... Loss: 0.24784... Val Loss: 0.23156\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1971/2000... Step: 1971... Loss: 0.24784... Val Loss: 0.23156\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1972/2000... Step: 1972... Loss: 0.24784... Val Loss: 0.23155\n",
      "Epoch: 1973/2000... Step: 1973... Loss: 0.24784... Val Loss: 0.23155\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1974/2000... Step: 1974... Loss: 0.24784... Val Loss: 0.23155\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1975/2000... Step: 1975... Loss: 0.24784... Val Loss: 0.23154\n",
      "Epoch: 1976/2000... Step: 1976... Loss: 0.24784... Val Loss: 0.23154\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1977/2000... Step: 1977... Loss: 0.24784... Val Loss: 0.23154\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1978/2000... Step: 1978... Loss: 0.24784... Val Loss: 0.23153\n",
      "Epoch: 1979/2000... Step: 1979... Loss: 0.24784... Val Loss: 0.23153\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1980/2000... Step: 1980... Loss: 0.24784... Val Loss: 0.23153\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1981/2000... Step: 1981... Loss: 0.24783... Val Loss: 0.23152\n",
      "Epoch: 1982/2000... Step: 1982... Loss: 0.24783... Val Loss: 0.23152\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1983/2000... Step: 1983... Loss: 0.24783... Val Loss: 0.23152\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1984/2000... Step: 1984... Loss: 0.24783... Val Loss: 0.23151\n",
      "Epoch: 1985/2000... Step: 1985... Loss: 0.24783... Val Loss: 0.23151\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1986/2000... Step: 1986... Loss: 0.24783... Val Loss: 0.23151\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1987/2000... Step: 1987... Loss: 0.24783... Val Loss: 0.23150\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1988/2000... Step: 1988... Loss: 0.24783... Val Loss: 0.23150\n",
      "Epoch: 1989/2000... Step: 1989... Loss: 0.24783... Val Loss: 0.23150\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1990/2000... Step: 1990... Loss: 0.24783... Val Loss: 0.23149\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1991/2000... Step: 1991... Loss: 0.24783... Val Loss: 0.23149\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1992/2000... Step: 1992... Loss: 0.24783... Val Loss: 0.23149\n",
      "Epoch: 1993/2000... Step: 1993... Loss: 0.24782... Val Loss: 0.23148\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1994/2000... Step: 1994... Loss: 0.24782... Val Loss: 0.23148\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1995/2000... Step: 1995... Loss: 0.24782... Val Loss: 0.23148\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1996/2000... Step: 1996... Loss: 0.24782... Val Loss: 0.23147\n",
      "Epoch: 1997/2000... Step: 1997... Loss: 0.24782... Val Loss: 0.23147\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1998/2000... Step: 1998... Loss: 0.24782... Val Loss: 0.23147\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1999/2000... Step: 1999... Loss: 0.24782... Val Loss: 0.23146\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 2000/2000... Step: 2000... Loss: 0.24782... Val Loss: 0.23146\n"
     ]
    }
   ],
   "source": [
    "train_losess, val_losses, roc_auc_train, roc_auc_val = train(net, training_generator, validation_generator, verbose=True,\n",
    "                                                             opt_func=opt, criterion_func=criterion, epochs=num_epochs,\n",
    "                                                             lr=0.01, check_early_stopping=False, check_auc_roc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after training on the Test set:\n",
      "0.403\n",
      "Accuracy After training on the Test set:\n",
      "0.84\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss after training on the Test set:\")\n",
    "print(infer(net, test_generator))\n",
    "print(\"Accuracy After training on the Test set:\")\n",
    "y_pred_p = test_accuracy(net, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before train on the Test set:\n",
      "0.7898\n",
      "Accuracy before train:\n",
      "0.36\n"
     ]
    }
   ],
   "source": [
    "training_generator, validation_generator, test_generator = create_generators(x_train, y_train, x_val, y_val, x_test,y_test)\n",
    "net = MlpNetwork(num_net_channels_part_a, nn.Sigmoid(), classifier=True)\n",
    "opt = torch.optim.Adam\n",
    "criterion = nn.BCELoss\n",
    "\n",
    "print(\"Loss before train on the Test set:\")\n",
    "print(infer(net, test_generator))\n",
    "print(\"Accuracy before train:\")\n",
    "_ = test_accuracy(net, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2000... Step: 1... Loss: 0.77084... Val Loss: 0.73356\n",
      "Epoch: 2/2000... Step: 2... Loss: 0.76552... Val Loss: 0.72925\n",
      "Epoch: 3/2000... Step: 3... Loss: 0.76024... Val Loss: 0.72498\n",
      "Epoch: 4/2000... Step: 4... Loss: 0.75502... Val Loss: 0.72076\n",
      "Epoch: 5/2000... Step: 5... Loss: 0.74986... Val Loss: 0.71658\n",
      "Epoch: 6/2000... Step: 6... Loss: 0.74475... Val Loss: 0.71245\n",
      "Epoch: 7/2000... Step: 7... Loss: 0.73969... Val Loss: 0.70836\n",
      "Epoch: 8/2000... Step: 8... Loss: 0.73470... Val Loss: 0.70432\n",
      "Epoch: 9/2000... Step: 9... Loss: 0.72976... Val Loss: 0.70032\n",
      "Epoch: 10/2000... Step: 10... Loss: 0.72487... Val Loss: 0.69637\n",
      "Epoch: 11/2000... Step: 11... Loss: 0.72005... Val Loss: 0.69247\n",
      "Epoch: 12/2000... Step: 12... Loss: 0.71529... Val Loss: 0.68860\n",
      "Epoch: 13/2000... Step: 13... Loss: 0.71058... Val Loss: 0.68479\n",
      "Epoch: 14/2000... Step: 14... Loss: 0.70594... Val Loss: 0.68102\n",
      "Epoch: 15/2000... Step: 15... Loss: 0.70135... Val Loss: 0.67729\n",
      "Epoch: 16/2000... Step: 16... Loss: 0.69683... Val Loss: 0.67361\n",
      "Epoch: 17/2000... Step: 17... Loss: 0.69236... Val Loss: 0.66997\n",
      "Epoch: 18/2000... Step: 18... Loss: 0.68796... Val Loss: 0.66637\n",
      "Epoch: 19/2000... Step: 19... Loss: 0.68362... Val Loss: 0.66282\n",
      "Epoch: 20/2000... Step: 20... Loss: 0.67933... Val Loss: 0.65930\n",
      "Epoch: 21/2000... Step: 21... Loss: 0.67511... Val Loss: 0.65583\n",
      "Epoch: 22/2000... Step: 22... Loss: 0.67095... Val Loss: 0.65240\n",
      "Epoch: 23/2000... Step: 23... Loss: 0.66685... Val Loss: 0.64900\n",
      "Epoch: 24/2000... Step: 24... Loss: 0.66280... Val Loss: 0.64565\n",
      "Epoch: 25/2000... Step: 25... Loss: 0.65882... Val Loss: 0.64233\n",
      "Epoch: 26/2000... Step: 26... Loss: 0.65489... Val Loss: 0.63905\n",
      "Epoch: 27/2000... Step: 27... Loss: 0.65103... Val Loss: 0.63580\n",
      "Epoch: 28/2000... Step: 28... Loss: 0.64721... Val Loss: 0.63259\n",
      "Epoch: 29/2000... Step: 29... Loss: 0.64346... Val Loss: 0.62941\n",
      "Epoch: 30/2000... Step: 30... Loss: 0.63976... Val Loss: 0.62627\n",
      "Epoch: 31/2000... Step: 31... Loss: 0.63611... Val Loss: 0.62316\n",
      "Epoch: 32/2000... Step: 32... Loss: 0.63252... Val Loss: 0.62008\n",
      "Epoch: 33/2000... Step: 33... Loss: 0.62899... Val Loss: 0.61703\n",
      "Epoch: 34/2000... Step: 34... Loss: 0.62550... Val Loss: 0.61402\n",
      "Epoch: 35/2000... Step: 35... Loss: 0.62207... Val Loss: 0.61103\n",
      "Epoch: 36/2000... Step: 36... Loss: 0.61868... Val Loss: 0.60807\n",
      "Epoch: 37/2000... Step: 37... Loss: 0.61535... Val Loss: 0.60514\n",
      "Epoch: 38/2000... Step: 38... Loss: 0.61207... Val Loss: 0.60224\n",
      "Epoch: 39/2000... Step: 39... Loss: 0.60883... Val Loss: 0.59936\n",
      "Epoch: 40/2000... Step: 40... Loss: 0.60564... Val Loss: 0.59651\n",
      "Epoch: 41/2000... Step: 41... Loss: 0.60250... Val Loss: 0.59369\n",
      "Epoch: 42/2000... Step: 42... Loss: 0.59940... Val Loss: 0.59090\n",
      "Epoch: 43/2000... Step: 43... Loss: 0.59635... Val Loss: 0.58813\n",
      "Epoch: 44/2000... Step: 44... Loss: 0.59334... Val Loss: 0.58539\n",
      "Epoch: 45/2000... Step: 45... Loss: 0.59038... Val Loss: 0.58267\n",
      "Epoch: 46/2000... Step: 46... Loss: 0.58746... Val Loss: 0.57998\n",
      "Epoch: 47/2000... Step: 47... Loss: 0.58458... Val Loss: 0.57732\n",
      "Epoch: 48/2000... Step: 48... Loss: 0.58174... Val Loss: 0.57468\n",
      "Epoch: 49/2000... Step: 49... Loss: 0.57894... Val Loss: 0.57206\n",
      "Epoch: 50/2000... Step: 50... Loss: 0.57618... Val Loss: 0.56948\n",
      "Epoch: 51/2000... Step: 51... Loss: 0.57345... Val Loss: 0.56692\n",
      "Epoch: 52/2000... Step: 52... Loss: 0.57077... Val Loss: 0.56438\n",
      "Epoch: 53/2000... Step: 53... Loss: 0.56813... Val Loss: 0.56187\n",
      "Epoch: 54/2000... Step: 54... Loss: 0.56552... Val Loss: 0.55939\n",
      "Epoch: 55/2000... Step: 55... Loss: 0.56294... Val Loss: 0.55693\n",
      "Epoch: 56/2000... Step: 56... Loss: 0.56041... Val Loss: 0.55450\n",
      "Epoch: 57/2000... Step: 57... Loss: 0.55791... Val Loss: 0.55210\n",
      "Epoch: 58/2000... Step: 58... Loss: 0.55544... Val Loss: 0.54972\n",
      "Epoch: 59/2000... Step: 59... Loss: 0.55301... Val Loss: 0.54737\n",
      "Epoch: 60/2000... Step: 60... Loss: 0.55061... Val Loss: 0.54504\n",
      "Epoch: 61/2000... Step: 61... Loss: 0.54824... Val Loss: 0.54275\n",
      "Epoch: 62/2000... Step: 62... Loss: 0.54591... Val Loss: 0.54048\n",
      "Epoch: 63/2000... Step: 63... Loss: 0.54361... Val Loss: 0.53823\n",
      "Epoch: 64/2000... Step: 64... Loss: 0.54133... Val Loss: 0.53602\n",
      "Epoch: 65/2000... Step: 65... Loss: 0.53910... Val Loss: 0.53383\n",
      "Epoch: 66/2000... Step: 66... Loss: 0.53689... Val Loss: 0.53167\n",
      "Epoch: 67/2000... Step: 67... Loss: 0.53471... Val Loss: 0.52953\n",
      "Epoch: 68/2000... Step: 68... Loss: 0.53256... Val Loss: 0.52742\n",
      "Epoch: 69/2000... Step: 69... Loss: 0.53044... Val Loss: 0.52534\n",
      "Epoch: 70/2000... Step: 70... Loss: 0.52834... Val Loss: 0.52329\n",
      "Epoch: 71/2000... Step: 71... Loss: 0.52628... Val Loss: 0.52126\n",
      "Epoch: 72/2000... Step: 72... Loss: 0.52424... Val Loss: 0.51926\n",
      "Epoch: 73/2000... Step: 73... Loss: 0.52223... Val Loss: 0.51728\n",
      "Epoch: 74/2000... Step: 74... Loss: 0.52025... Val Loss: 0.51533\n",
      "Epoch: 75/2000... Step: 75... Loss: 0.51829... Val Loss: 0.51341\n",
      "Epoch: 76/2000... Step: 76... Loss: 0.51636... Val Loss: 0.51151\n",
      "Epoch: 77/2000... Step: 77... Loss: 0.51446... Val Loss: 0.50964\n",
      "Epoch: 78/2000... Step: 78... Loss: 0.51257... Val Loss: 0.50779\n",
      "Epoch: 79/2000... Step: 79... Loss: 0.51072... Val Loss: 0.50597\n",
      "Epoch: 80/2000... Step: 80... Loss: 0.50889... Val Loss: 0.50417\n",
      "Epoch: 81/2000... Step: 81... Loss: 0.50708... Val Loss: 0.50240\n",
      "Epoch: 82/2000... Step: 82... Loss: 0.50529... Val Loss: 0.50065\n",
      "Epoch: 83/2000... Step: 83... Loss: 0.50353... Val Loss: 0.49892\n",
      "Epoch: 84/2000... Step: 84... Loss: 0.50179... Val Loss: 0.49722\n",
      "Epoch: 85/2000... Step: 85... Loss: 0.50007... Val Loss: 0.49554\n",
      "Epoch: 86/2000... Step: 86... Loss: 0.49837... Val Loss: 0.49388\n",
      "Epoch: 87/2000... Step: 87... Loss: 0.49670... Val Loss: 0.49225\n",
      "Epoch: 88/2000... Step: 88... Loss: 0.49504... Val Loss: 0.49064\n",
      "Epoch: 89/2000... Step: 89... Loss: 0.49341... Val Loss: 0.48904\n",
      "Epoch: 90/2000... Step: 90... Loss: 0.49180... Val Loss: 0.48747\n",
      "Epoch: 91/2000... Step: 91... Loss: 0.49020... Val Loss: 0.48592\n",
      "Epoch: 92/2000... Step: 92... Loss: 0.48863... Val Loss: 0.48439\n",
      "Epoch: 93/2000... Step: 93... Loss: 0.48707... Val Loss: 0.48288\n",
      "Epoch: 94/2000... Step: 94... Loss: 0.48554... Val Loss: 0.48139\n",
      "Epoch: 95/2000... Step: 95... Loss: 0.48402... Val Loss: 0.47992\n",
      "Epoch: 96/2000... Step: 96... Loss: 0.48252... Val Loss: 0.47847\n",
      "Epoch: 97/2000... Step: 97... Loss: 0.48104... Val Loss: 0.47703\n",
      "Epoch: 98/2000... Step: 98... Loss: 0.47957... Val Loss: 0.47561\n",
      "Epoch: 99/2000... Step: 99... Loss: 0.47813... Val Loss: 0.47422\n",
      "Epoch: 100/2000... Step: 100... Loss: 0.47670... Val Loss: 0.47283\n",
      "Epoch: 101/2000... Step: 101... Loss: 0.47528... Val Loss: 0.47147\n",
      "Epoch: 102/2000... Step: 102... Loss: 0.47388... Val Loss: 0.47012\n",
      "Epoch: 103/2000... Step: 103... Loss: 0.47250... Val Loss: 0.46879\n",
      "Epoch: 104/2000... Step: 104... Loss: 0.47114... Val Loss: 0.46747\n",
      "Epoch: 105/2000... Step: 105... Loss: 0.46979... Val Loss: 0.46617\n",
      "Epoch: 106/2000... Step: 106... Loss: 0.46845... Val Loss: 0.46489\n",
      "Epoch: 107/2000... Step: 107... Loss: 0.46713... Val Loss: 0.46362\n",
      "Epoch: 108/2000... Step: 108... Loss: 0.46583... Val Loss: 0.46236\n",
      "Epoch: 109/2000... Step: 109... Loss: 0.46454... Val Loss: 0.46112\n",
      "Epoch: 110/2000... Step: 110... Loss: 0.46326... Val Loss: 0.45990\n",
      "Epoch: 111/2000... Step: 111... Loss: 0.46200... Val Loss: 0.45868\n",
      "Epoch: 112/2000... Step: 112... Loss: 0.46075... Val Loss: 0.45748\n",
      "Epoch: 113/2000... Step: 113... Loss: 0.45952... Val Loss: 0.45630\n",
      "Epoch: 114/2000... Step: 114... Loss: 0.45830... Val Loss: 0.45513\n",
      "Epoch: 115/2000... Step: 115... Loss: 0.45709... Val Loss: 0.45397\n",
      "Epoch: 116/2000... Step: 116... Loss: 0.45590... Val Loss: 0.45282\n",
      "Epoch: 117/2000... Step: 117... Loss: 0.45471... Val Loss: 0.45168\n",
      "Epoch: 118/2000... Step: 118... Loss: 0.45354... Val Loss: 0.45056\n",
      "Epoch: 119/2000... Step: 119... Loss: 0.45239... Val Loss: 0.44945\n",
      "Epoch: 120/2000... Step: 120... Loss: 0.45124... Val Loss: 0.44835\n",
      "Epoch: 121/2000... Step: 121... Loss: 0.45011... Val Loss: 0.44726\n",
      "Epoch: 122/2000... Step: 122... Loss: 0.44898... Val Loss: 0.44618\n",
      "Epoch: 123/2000... Step: 123... Loss: 0.44787... Val Loss: 0.44512\n",
      "Epoch: 124/2000... Step: 124... Loss: 0.44677... Val Loss: 0.44406\n",
      "Epoch: 125/2000... Step: 125... Loss: 0.44569... Val Loss: 0.44302\n",
      "Epoch: 126/2000... Step: 126... Loss: 0.44461... Val Loss: 0.44199\n",
      "Epoch: 127/2000... Step: 127... Loss: 0.44354... Val Loss: 0.44096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 128/2000... Step: 128... Loss: 0.44249... Val Loss: 0.43995\n",
      "Epoch: 129/2000... Step: 129... Loss: 0.44144... Val Loss: 0.43895\n",
      "Epoch: 130/2000... Step: 130... Loss: 0.44041... Val Loss: 0.43795\n",
      "Epoch: 131/2000... Step: 131... Loss: 0.43938... Val Loss: 0.43697\n",
      "Epoch: 132/2000... Step: 132... Loss: 0.43837... Val Loss: 0.43599\n",
      "Epoch: 133/2000... Step: 133... Loss: 0.43736... Val Loss: 0.43503\n",
      "Epoch: 134/2000... Step: 134... Loss: 0.43636... Val Loss: 0.43407\n",
      "Epoch: 135/2000... Step: 135... Loss: 0.43538... Val Loss: 0.43313\n",
      "Epoch: 136/2000... Step: 136... Loss: 0.43440... Val Loss: 0.43219\n",
      "Epoch: 137/2000... Step: 137... Loss: 0.43343... Val Loss: 0.43126\n",
      "Epoch: 138/2000... Step: 138... Loss: 0.43247... Val Loss: 0.43034\n",
      "Epoch: 139/2000... Step: 139... Loss: 0.43152... Val Loss: 0.42943\n",
      "Epoch: 140/2000... Step: 140... Loss: 0.43058... Val Loss: 0.42852\n",
      "Epoch: 141/2000... Step: 141... Loss: 0.42965... Val Loss: 0.42763\n",
      "Epoch: 142/2000... Step: 142... Loss: 0.42872... Val Loss: 0.42674\n",
      "Epoch: 143/2000... Step: 143... Loss: 0.42781... Val Loss: 0.42586\n",
      "Epoch: 144/2000... Step: 144... Loss: 0.42690... Val Loss: 0.42499\n",
      "Epoch: 145/2000... Step: 145... Loss: 0.42600... Val Loss: 0.42413\n",
      "Epoch: 146/2000... Step: 146... Loss: 0.42511... Val Loss: 0.42327\n",
      "Epoch: 147/2000... Step: 147... Loss: 0.42422... Val Loss: 0.42242\n",
      "Epoch: 148/2000... Step: 148... Loss: 0.42335... Val Loss: 0.42158\n",
      "Epoch: 149/2000... Step: 149... Loss: 0.42248... Val Loss: 0.42075\n",
      "Epoch: 150/2000... Step: 150... Loss: 0.42162... Val Loss: 0.41992\n",
      "Epoch: 151/2000... Step: 151... Loss: 0.42076... Val Loss: 0.41910\n",
      "Epoch: 152/2000... Step: 152... Loss: 0.41992... Val Loss: 0.41829\n",
      "Epoch: 153/2000... Step: 153... Loss: 0.41908... Val Loss: 0.41748\n",
      "Epoch: 154/2000... Step: 154... Loss: 0.41825... Val Loss: 0.41668\n",
      "Epoch: 155/2000... Step: 155... Loss: 0.41742... Val Loss: 0.41589\n",
      "Epoch: 156/2000... Step: 156... Loss: 0.41660... Val Loss: 0.41511\n",
      "Epoch: 157/2000... Step: 157... Loss: 0.41579... Val Loss: 0.41433\n",
      "Epoch: 158/2000... Step: 158... Loss: 0.41499... Val Loss: 0.41355\n",
      "Epoch: 159/2000... Step: 159... Loss: 0.41419... Val Loss: 0.41279\n",
      "Epoch: 160/2000... Step: 160... Loss: 0.41339... Val Loss: 0.41203\n",
      "Epoch: 161/2000... Step: 161... Loss: 0.41261... Val Loss: 0.41127\n",
      "Epoch: 162/2000... Step: 162... Loss: 0.41183... Val Loss: 0.41053\n",
      "Epoch: 163/2000... Step: 163... Loss: 0.41106... Val Loss: 0.40978\n",
      "Epoch: 164/2000... Step: 164... Loss: 0.41029... Val Loss: 0.40905\n",
      "Epoch: 165/2000... Step: 165... Loss: 0.40953... Val Loss: 0.40832\n",
      "Epoch: 166/2000... Step: 166... Loss: 0.40877... Val Loss: 0.40759\n",
      "Epoch: 167/2000... Step: 167... Loss: 0.40802... Val Loss: 0.40687\n",
      "Epoch: 168/2000... Step: 168... Loss: 0.40728... Val Loss: 0.40616\n",
      "Epoch: 169/2000... Step: 169... Loss: 0.40654... Val Loss: 0.40545\n",
      "Epoch: 170/2000... Step: 170... Loss: 0.40581... Val Loss: 0.40475\n",
      "Epoch: 171/2000... Step: 171... Loss: 0.40508... Val Loss: 0.40405\n",
      "Epoch: 172/2000... Step: 172... Loss: 0.40436... Val Loss: 0.40335\n",
      "Epoch: 173/2000... Step: 173... Loss: 0.40364... Val Loss: 0.40267\n",
      "Epoch: 174/2000... Step: 174... Loss: 0.40293... Val Loss: 0.40198\n",
      "Epoch: 175/2000... Step: 175... Loss: 0.40223... Val Loss: 0.40131\n",
      "Epoch: 176/2000... Step: 176... Loss: 0.40153... Val Loss: 0.40063\n",
      "Epoch: 177/2000... Step: 177... Loss: 0.40083... Val Loss: 0.39997\n",
      "Epoch: 178/2000... Step: 178... Loss: 0.40014... Val Loss: 0.39930\n",
      "Epoch: 179/2000... Step: 179... Loss: 0.39946... Val Loss: 0.39864\n",
      "Epoch: 180/2000... Step: 180... Loss: 0.39878... Val Loss: 0.39799\n",
      "Epoch: 181/2000... Step: 181... Loss: 0.39810... Val Loss: 0.39734\n",
      "Epoch: 182/2000... Step: 182... Loss: 0.39743... Val Loss: 0.39670\n",
      "Epoch: 183/2000... Step: 183... Loss: 0.39676... Val Loss: 0.39606\n",
      "Epoch: 184/2000... Step: 184... Loss: 0.39610... Val Loss: 0.39542\n",
      "Epoch: 185/2000... Step: 185... Loss: 0.39544... Val Loss: 0.39479\n",
      "Epoch: 186/2000... Step: 186... Loss: 0.39479... Val Loss: 0.39416\n",
      "Epoch: 187/2000... Step: 187... Loss: 0.39414... Val Loss: 0.39354\n",
      "Epoch: 188/2000... Step: 188... Loss: 0.39350... Val Loss: 0.39292\n",
      "Epoch: 189/2000... Step: 189... Loss: 0.39286... Val Loss: 0.39231\n",
      "Epoch: 190/2000... Step: 190... Loss: 0.39223... Val Loss: 0.39170\n",
      "Epoch: 191/2000... Step: 191... Loss: 0.39160... Val Loss: 0.39109\n",
      "Epoch: 192/2000... Step: 192... Loss: 0.39097... Val Loss: 0.39049\n",
      "Epoch: 193/2000... Step: 193... Loss: 0.39035... Val Loss: 0.38989\n",
      "Epoch: 194/2000... Step: 194... Loss: 0.38973... Val Loss: 0.38929\n",
      "Epoch: 195/2000... Step: 195... Loss: 0.38911... Val Loss: 0.38870\n",
      "Epoch: 196/2000... Step: 196... Loss: 0.38850... Val Loss: 0.38811\n",
      "Epoch: 197/2000... Step: 197... Loss: 0.38790... Val Loss: 0.38753\n",
      "Epoch: 198/2000... Step: 198... Loss: 0.38730... Val Loss: 0.38695\n",
      "Epoch: 199/2000... Step: 199... Loss: 0.38670... Val Loss: 0.38637\n",
      "Epoch: 200/2000... Step: 200... Loss: 0.38610... Val Loss: 0.38580\n",
      "Epoch: 201/2000... Step: 201... Loss: 0.38551... Val Loss: 0.38523\n",
      "Epoch: 202/2000... Step: 202... Loss: 0.38492... Val Loss: 0.38466\n",
      "Epoch: 203/2000... Step: 203... Loss: 0.38434... Val Loss: 0.38410\n",
      "Epoch: 204/2000... Step: 204... Loss: 0.38376... Val Loss: 0.38354\n",
      "Epoch: 205/2000... Step: 205... Loss: 0.38318... Val Loss: 0.38298\n",
      "Epoch: 206/2000... Step: 206... Loss: 0.38261... Val Loss: 0.38243\n",
      "Epoch: 207/2000... Step: 207... Loss: 0.38204... Val Loss: 0.38188\n",
      "Epoch: 208/2000... Step: 208... Loss: 0.38148... Val Loss: 0.38134\n",
      "Epoch: 209/2000... Step: 209... Loss: 0.38091... Val Loss: 0.38079\n",
      "Epoch: 210/2000... Step: 210... Loss: 0.38036... Val Loss: 0.38025\n",
      "Epoch: 211/2000... Step: 211... Loss: 0.37980... Val Loss: 0.37972\n",
      "Epoch: 212/2000... Step: 212... Loss: 0.37925... Val Loss: 0.37918\n",
      "Epoch: 213/2000... Step: 213... Loss: 0.37870... Val Loss: 0.37865\n",
      "Epoch: 214/2000... Step: 214... Loss: 0.37815... Val Loss: 0.37812\n",
      "Epoch: 215/2000... Step: 215... Loss: 0.37761... Val Loss: 0.37760\n",
      "Epoch: 216/2000... Step: 216... Loss: 0.37707... Val Loss: 0.37708\n",
      "Epoch: 217/2000... Step: 217... Loss: 0.37653... Val Loss: 0.37656\n",
      "Epoch: 218/2000... Step: 218... Loss: 0.37600... Val Loss: 0.37604\n",
      "Epoch: 219/2000... Step: 219... Loss: 0.37547... Val Loss: 0.37553\n",
      "Epoch: 220/2000... Step: 220... Loss: 0.37494... Val Loss: 0.37502\n",
      "Epoch: 221/2000... Step: 221... Loss: 0.37442... Val Loss: 0.37451\n",
      "Epoch: 222/2000... Step: 222... Loss: 0.37390... Val Loss: 0.37400\n",
      "Epoch: 223/2000... Step: 223... Loss: 0.37338... Val Loss: 0.37350\n",
      "Epoch: 224/2000... Step: 224... Loss: 0.37287... Val Loss: 0.37300\n",
      "Epoch: 225/2000... Step: 225... Loss: 0.37235... Val Loss: 0.37251\n",
      "Epoch: 226/2000... Step: 226... Loss: 0.37184... Val Loss: 0.37201\n",
      "Epoch: 227/2000... Step: 227... Loss: 0.37134... Val Loss: 0.37152\n",
      "Epoch: 228/2000... Step: 228... Loss: 0.37083... Val Loss: 0.37103\n",
      "Epoch: 229/2000... Step: 229... Loss: 0.37033... Val Loss: 0.37054\n",
      "Epoch: 230/2000... Step: 230... Loss: 0.36984... Val Loss: 0.37006\n",
      "Epoch: 231/2000... Step: 231... Loss: 0.36934... Val Loss: 0.36958\n",
      "Epoch: 232/2000... Step: 232... Loss: 0.36885... Val Loss: 0.36910\n",
      "Epoch: 233/2000... Step: 233... Loss: 0.36836... Val Loss: 0.36862\n",
      "Epoch: 234/2000... Step: 234... Loss: 0.36787... Val Loss: 0.36815\n",
      "Epoch: 235/2000... Step: 235... Loss: 0.36738... Val Loss: 0.36767\n",
      "Epoch: 236/2000... Step: 236... Loss: 0.36690... Val Loss: 0.36720\n",
      "Epoch: 237/2000... Step: 237... Loss: 0.36642... Val Loss: 0.36674\n",
      "Epoch: 238/2000... Step: 238... Loss: 0.36594... Val Loss: 0.36627\n",
      "Epoch: 239/2000... Step: 239... Loss: 0.36547... Val Loss: 0.36581\n",
      "Epoch: 240/2000... Step: 240... Loss: 0.36500... Val Loss: 0.36535\n",
      "Epoch: 241/2000... Step: 241... Loss: 0.36453... Val Loss: 0.36489\n",
      "Epoch: 242/2000... Step: 242... Loss: 0.36406... Val Loss: 0.36443\n",
      "Epoch: 243/2000... Step: 243... Loss: 0.36360... Val Loss: 0.36398\n",
      "Epoch: 244/2000... Step: 244... Loss: 0.36313... Val Loss: 0.36353\n",
      "Epoch: 245/2000... Step: 245... Loss: 0.36267... Val Loss: 0.36308\n",
      "Epoch: 246/2000... Step: 246... Loss: 0.36222... Val Loss: 0.36263\n",
      "Epoch: 247/2000... Step: 247... Loss: 0.36176... Val Loss: 0.36219\n",
      "Epoch: 248/2000... Step: 248... Loss: 0.36131... Val Loss: 0.36174\n",
      "Epoch: 249/2000... Step: 249... Loss: 0.36086... Val Loss: 0.36130\n",
      "Epoch: 250/2000... Step: 250... Loss: 0.36041... Val Loss: 0.36086\n",
      "Epoch: 251/2000... Step: 251... Loss: 0.35996... Val Loss: 0.36042\n",
      "Epoch: 252/2000... Step: 252... Loss: 0.35952... Val Loss: 0.35999\n",
      "Epoch: 253/2000... Step: 253... Loss: 0.35908... Val Loss: 0.35956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 254/2000... Step: 254... Loss: 0.35864... Val Loss: 0.35912\n",
      "Epoch: 255/2000... Step: 255... Loss: 0.35820... Val Loss: 0.35870\n",
      "Epoch: 256/2000... Step: 256... Loss: 0.35776... Val Loss: 0.35827\n",
      "Epoch: 257/2000... Step: 257... Loss: 0.35733... Val Loss: 0.35784\n",
      "Epoch: 258/2000... Step: 258... Loss: 0.35690... Val Loss: 0.35742\n",
      "Epoch: 259/2000... Step: 259... Loss: 0.35647... Val Loss: 0.35700\n",
      "Epoch: 260/2000... Step: 260... Loss: 0.35604... Val Loss: 0.35658\n",
      "Epoch: 261/2000... Step: 261... Loss: 0.35562... Val Loss: 0.35616\n",
      "Epoch: 262/2000... Step: 262... Loss: 0.35520... Val Loss: 0.35574\n",
      "Epoch: 263/2000... Step: 263... Loss: 0.35478... Val Loss: 0.35533\n",
      "Epoch: 264/2000... Step: 264... Loss: 0.35436... Val Loss: 0.35492\n",
      "Epoch: 265/2000... Step: 265... Loss: 0.35394... Val Loss: 0.35451\n",
      "Epoch: 266/2000... Step: 266... Loss: 0.35353... Val Loss: 0.35410\n",
      "Epoch: 267/2000... Step: 267... Loss: 0.35311... Val Loss: 0.35369\n",
      "Epoch: 268/2000... Step: 268... Loss: 0.35270... Val Loss: 0.35329\n",
      "Epoch: 269/2000... Step: 269... Loss: 0.35229... Val Loss: 0.35288\n",
      "Epoch: 270/2000... Step: 270... Loss: 0.35189... Val Loss: 0.35248\n",
      "Epoch: 271/2000... Step: 271... Loss: 0.35148... Val Loss: 0.35208\n",
      "Epoch: 272/2000... Step: 272... Loss: 0.35108... Val Loss: 0.35168\n",
      "Epoch: 273/2000... Step: 273... Loss: 0.35068... Val Loss: 0.35128\n",
      "Epoch: 274/2000... Step: 274... Loss: 0.35028... Val Loss: 0.35089\n",
      "Epoch: 275/2000... Step: 275... Loss: 0.34988... Val Loss: 0.35050\n",
      "Epoch: 276/2000... Step: 276... Loss: 0.34949... Val Loss: 0.35010\n",
      "Epoch: 277/2000... Step: 277... Loss: 0.34909... Val Loss: 0.34971\n",
      "Epoch: 278/2000... Step: 278... Loss: 0.34870... Val Loss: 0.34933\n",
      "Epoch: 279/2000... Step: 279... Loss: 0.34831... Val Loss: 0.34894\n",
      "Epoch: 280/2000... Step: 280... Loss: 0.34792... Val Loss: 0.34855\n",
      "Epoch: 281/2000... Step: 281... Loss: 0.34754... Val Loss: 0.34817\n",
      "Epoch: 282/2000... Step: 282... Loss: 0.34715... Val Loss: 0.34779\n",
      "Epoch: 283/2000... Step: 283... Loss: 0.34677... Val Loss: 0.34741\n",
      "Epoch: 284/2000... Step: 284... Loss: 0.34639... Val Loss: 0.34703\n",
      "Epoch: 285/2000... Step: 285... Loss: 0.34601... Val Loss: 0.34665\n",
      "Epoch: 286/2000... Step: 286... Loss: 0.34563... Val Loss: 0.34627\n",
      "Epoch: 287/2000... Step: 287... Loss: 0.34526... Val Loss: 0.34590\n",
      "Epoch: 288/2000... Step: 288... Loss: 0.34488... Val Loss: 0.34553\n",
      "Epoch: 289/2000... Step: 289... Loss: 0.34451... Val Loss: 0.34516\n",
      "Epoch: 290/2000... Step: 290... Loss: 0.34414... Val Loss: 0.34479\n",
      "Epoch: 291/2000... Step: 291... Loss: 0.34377... Val Loss: 0.34442\n",
      "Epoch: 292/2000... Step: 292... Loss: 0.34340... Val Loss: 0.34405\n",
      "Epoch: 293/2000... Step: 293... Loss: 0.34304... Val Loss: 0.34368\n",
      "Epoch: 294/2000... Step: 294... Loss: 0.34267... Val Loss: 0.34332\n",
      "Epoch: 295/2000... Step: 295... Loss: 0.34231... Val Loss: 0.34296\n",
      "Epoch: 296/2000... Step: 296... Loss: 0.34195... Val Loss: 0.34260\n",
      "Epoch: 297/2000... Step: 297... Loss: 0.34159... Val Loss: 0.34224\n",
      "Epoch: 298/2000... Step: 298... Loss: 0.34123... Val Loss: 0.34188\n",
      "Epoch: 299/2000... Step: 299... Loss: 0.34088... Val Loss: 0.34152\n",
      "Epoch: 300/2000... Step: 300... Loss: 0.34052... Val Loss: 0.34116\n",
      "Epoch: 301/2000... Step: 301... Loss: 0.34017... Val Loss: 0.34081\n",
      "Epoch: 302/2000... Step: 302... Loss: 0.33982... Val Loss: 0.34046\n",
      "Epoch: 303/2000... Step: 303... Loss: 0.33947... Val Loss: 0.34011\n",
      "Epoch: 304/2000... Step: 304... Loss: 0.33912... Val Loss: 0.33975\n",
      "Epoch: 305/2000... Step: 305... Loss: 0.33877... Val Loss: 0.33941\n",
      "Epoch: 306/2000... Step: 306... Loss: 0.33843... Val Loss: 0.33906\n",
      "Epoch: 307/2000... Step: 307... Loss: 0.33808... Val Loss: 0.33871\n",
      "Epoch: 308/2000... Step: 308... Loss: 0.33774... Val Loss: 0.33837\n",
      "Epoch: 309/2000... Step: 309... Loss: 0.33740... Val Loss: 0.33802\n",
      "Epoch: 310/2000... Step: 310... Loss: 0.33706... Val Loss: 0.33768\n",
      "Epoch: 311/2000... Step: 311... Loss: 0.33672... Val Loss: 0.33734\n",
      "Epoch: 312/2000... Step: 312... Loss: 0.33638... Val Loss: 0.33700\n",
      "Epoch: 313/2000... Step: 313... Loss: 0.33605... Val Loss: 0.33666\n",
      "Epoch: 314/2000... Step: 314... Loss: 0.33572... Val Loss: 0.33632\n",
      "Epoch: 315/2000... Step: 315... Loss: 0.33538... Val Loss: 0.33599\n",
      "Epoch: 316/2000... Step: 316... Loss: 0.33505... Val Loss: 0.33565\n",
      "Epoch: 317/2000... Step: 317... Loss: 0.33472... Val Loss: 0.33532\n",
      "Epoch: 318/2000... Step: 318... Loss: 0.33439... Val Loss: 0.33499\n",
      "Epoch: 319/2000... Step: 319... Loss: 0.33407... Val Loss: 0.33466\n",
      "Epoch: 320/2000... Step: 320... Loss: 0.33374... Val Loss: 0.33433\n",
      "Epoch: 321/2000... Step: 321... Loss: 0.33342... Val Loss: 0.33400\n",
      "Epoch: 322/2000... Step: 322... Loss: 0.33310... Val Loss: 0.33367\n",
      "Epoch: 323/2000... Step: 323... Loss: 0.33278... Val Loss: 0.33334\n",
      "Epoch: 324/2000... Step: 324... Loss: 0.33246... Val Loss: 0.33302\n",
      "Epoch: 325/2000... Step: 325... Loss: 0.33214... Val Loss: 0.33270\n",
      "Epoch: 326/2000... Step: 326... Loss: 0.33182... Val Loss: 0.33237\n",
      "Epoch: 327/2000... Step: 327... Loss: 0.33151... Val Loss: 0.33205\n",
      "Epoch: 328/2000... Step: 328... Loss: 0.33119... Val Loss: 0.33173\n",
      "Epoch: 329/2000... Step: 329... Loss: 0.33088... Val Loss: 0.33141\n",
      "Epoch: 330/2000... Step: 330... Loss: 0.33057... Val Loss: 0.33109\n",
      "Epoch: 331/2000... Step: 331... Loss: 0.33026... Val Loss: 0.33078\n",
      "Epoch: 332/2000... Step: 332... Loss: 0.32995... Val Loss: 0.33046\n",
      "Epoch: 333/2000... Step: 333... Loss: 0.32964... Val Loss: 0.33015\n",
      "Epoch: 334/2000... Step: 334... Loss: 0.32933... Val Loss: 0.32983\n",
      "Epoch: 335/2000... Step: 335... Loss: 0.32903... Val Loss: 0.32952\n",
      "Epoch: 336/2000... Step: 336... Loss: 0.32872... Val Loss: 0.32921\n",
      "Epoch: 337/2000... Step: 337... Loss: 0.32842... Val Loss: 0.32890\n",
      "Epoch: 338/2000... Step: 338... Loss: 0.32812... Val Loss: 0.32859\n",
      "Epoch: 339/2000... Step: 339... Loss: 0.32782... Val Loss: 0.32829\n",
      "Epoch: 340/2000... Step: 340... Loss: 0.32752... Val Loss: 0.32798\n",
      "Epoch: 341/2000... Step: 341... Loss: 0.32722... Val Loss: 0.32767\n",
      "Epoch: 342/2000... Step: 342... Loss: 0.32693... Val Loss: 0.32737\n",
      "Epoch: 343/2000... Step: 343... Loss: 0.32663... Val Loss: 0.32707\n",
      "Epoch: 344/2000... Step: 344... Loss: 0.32634... Val Loss: 0.32676\n",
      "Epoch: 345/2000... Step: 345... Loss: 0.32604... Val Loss: 0.32646\n",
      "Epoch: 346/2000... Step: 346... Loss: 0.32575... Val Loss: 0.32616\n",
      "Epoch: 347/2000... Step: 347... Loss: 0.32546... Val Loss: 0.32586\n",
      "Epoch: 348/2000... Step: 348... Loss: 0.32517... Val Loss: 0.32556\n",
      "Epoch: 349/2000... Step: 349... Loss: 0.32489... Val Loss: 0.32527\n",
      "Epoch: 350/2000... Step: 350... Loss: 0.32460... Val Loss: 0.32497\n",
      "Epoch: 351/2000... Step: 351... Loss: 0.32431... Val Loss: 0.32468\n",
      "Epoch: 352/2000... Step: 352... Loss: 0.32403... Val Loss: 0.32438\n",
      "Epoch: 353/2000... Step: 353... Loss: 0.32375... Val Loss: 0.32409\n",
      "Epoch: 354/2000... Step: 354... Loss: 0.32346... Val Loss: 0.32380\n",
      "Epoch: 355/2000... Step: 355... Loss: 0.32318... Val Loss: 0.32351\n",
      "Epoch: 356/2000... Step: 356... Loss: 0.32290... Val Loss: 0.32322\n",
      "Epoch: 357/2000... Step: 357... Loss: 0.32262... Val Loss: 0.32293\n",
      "Epoch: 358/2000... Step: 358... Loss: 0.32235... Val Loss: 0.32264\n",
      "Epoch: 359/2000... Step: 359... Loss: 0.32207... Val Loss: 0.32236\n",
      "Epoch: 360/2000... Step: 360... Loss: 0.32180... Val Loss: 0.32207\n",
      "Epoch: 361/2000... Step: 361... Loss: 0.32152... Val Loss: 0.32178\n",
      "Epoch: 362/2000... Step: 362... Loss: 0.32125... Val Loss: 0.32150\n",
      "Epoch: 363/2000... Step: 363... Loss: 0.32098... Val Loss: 0.32122\n",
      "Epoch: 364/2000... Step: 364... Loss: 0.32071... Val Loss: 0.32094\n",
      "Epoch: 365/2000... Step: 365... Loss: 0.32044... Val Loss: 0.32066\n",
      "Epoch: 366/2000... Step: 366... Loss: 0.32017... Val Loss: 0.32038\n",
      "Epoch: 367/2000... Step: 367... Loss: 0.31990... Val Loss: 0.32010\n",
      "Epoch: 368/2000... Step: 368... Loss: 0.31963... Val Loss: 0.31982\n",
      "Epoch: 369/2000... Step: 369... Loss: 0.31937... Val Loss: 0.31954\n",
      "Epoch: 370/2000... Step: 370... Loss: 0.31910... Val Loss: 0.31927\n",
      "Epoch: 371/2000... Step: 371... Loss: 0.31884... Val Loss: 0.31899\n",
      "Epoch: 372/2000... Step: 372... Loss: 0.31858... Val Loss: 0.31872\n",
      "Epoch: 373/2000... Step: 373... Loss: 0.31832... Val Loss: 0.31845\n",
      "Epoch: 374/2000... Step: 374... Loss: 0.31806... Val Loss: 0.31817\n",
      "Epoch: 375/2000... Step: 375... Loss: 0.31780... Val Loss: 0.31790\n",
      "Epoch: 376/2000... Step: 376... Loss: 0.31754... Val Loss: 0.31763\n",
      "Epoch: 377/2000... Step: 377... Loss: 0.31728... Val Loss: 0.31736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 378/2000... Step: 378... Loss: 0.31703... Val Loss: 0.31709\n",
      "Epoch: 379/2000... Step: 379... Loss: 0.31677... Val Loss: 0.31683\n",
      "Epoch: 380/2000... Step: 380... Loss: 0.31652... Val Loss: 0.31656\n",
      "Epoch: 381/2000... Step: 381... Loss: 0.31627... Val Loss: 0.31629\n",
      "Epoch: 382/2000... Step: 382... Loss: 0.31602... Val Loss: 0.31603\n",
      "Epoch: 383/2000... Step: 383... Loss: 0.31577... Val Loss: 0.31577\n",
      "Epoch: 384/2000... Step: 384... Loss: 0.31552... Val Loss: 0.31550\n",
      "Epoch: 385/2000... Step: 385... Loss: 0.31527... Val Loss: 0.31524\n",
      "Epoch: 386/2000... Step: 386... Loss: 0.31502... Val Loss: 0.31498\n",
      "Epoch: 387/2000... Step: 387... Loss: 0.31477... Val Loss: 0.31472\n",
      "Epoch: 388/2000... Step: 388... Loss: 0.31453... Val Loss: 0.31446\n",
      "Epoch: 389/2000... Step: 389... Loss: 0.31428... Val Loss: 0.31420\n",
      "Epoch: 390/2000... Step: 390... Loss: 0.31404... Val Loss: 0.31395\n",
      "Epoch: 391/2000... Step: 391... Loss: 0.31380... Val Loss: 0.31369\n",
      "Epoch: 392/2000... Step: 392... Loss: 0.31355... Val Loss: 0.31343\n",
      "Epoch: 393/2000... Step: 393... Loss: 0.31331... Val Loss: 0.31318\n",
      "Epoch: 394/2000... Step: 394... Loss: 0.31307... Val Loss: 0.31292\n",
      "Epoch: 395/2000... Step: 395... Loss: 0.31284... Val Loss: 0.31267\n",
      "Epoch: 396/2000... Step: 396... Loss: 0.31260... Val Loss: 0.31242\n",
      "Epoch: 397/2000... Step: 397... Loss: 0.31236... Val Loss: 0.31217\n",
      "Epoch: 398/2000... Step: 398... Loss: 0.31212... Val Loss: 0.31192\n",
      "Epoch: 399/2000... Step: 399... Loss: 0.31189... Val Loss: 0.31167\n",
      "Epoch: 400/2000... Step: 400... Loss: 0.31166... Val Loss: 0.31142\n",
      "Epoch: 401/2000... Step: 401... Loss: 0.31142... Val Loss: 0.31117\n",
      "Epoch: 402/2000... Step: 402... Loss: 0.31119... Val Loss: 0.31092\n",
      "Epoch: 403/2000... Step: 403... Loss: 0.31096... Val Loss: 0.31068\n",
      "Epoch: 404/2000... Step: 404... Loss: 0.31073... Val Loss: 0.31043\n",
      "Epoch: 405/2000... Step: 405... Loss: 0.31050... Val Loss: 0.31019\n",
      "Epoch: 406/2000... Step: 406... Loss: 0.31027... Val Loss: 0.30994\n",
      "Epoch: 407/2000... Step: 407... Loss: 0.31004... Val Loss: 0.30970\n",
      "Epoch: 408/2000... Step: 408... Loss: 0.30982... Val Loss: 0.30946\n",
      "Epoch: 409/2000... Step: 409... Loss: 0.30959... Val Loss: 0.30922\n",
      "Epoch: 410/2000... Step: 410... Loss: 0.30936... Val Loss: 0.30898\n",
      "Epoch: 411/2000... Step: 411... Loss: 0.30914... Val Loss: 0.30874\n",
      "Epoch: 412/2000... Step: 412... Loss: 0.30892... Val Loss: 0.30850\n",
      "Epoch: 413/2000... Step: 413... Loss: 0.30869... Val Loss: 0.30826\n",
      "Epoch: 414/2000... Step: 414... Loss: 0.30847... Val Loss: 0.30802\n",
      "Epoch: 415/2000... Step: 415... Loss: 0.30825... Val Loss: 0.30778\n",
      "Epoch: 416/2000... Step: 416... Loss: 0.30803... Val Loss: 0.30755\n",
      "Epoch: 417/2000... Step: 417... Loss: 0.30781... Val Loss: 0.30731\n",
      "Epoch: 418/2000... Step: 418... Loss: 0.30760... Val Loss: 0.30708\n",
      "Epoch: 419/2000... Step: 419... Loss: 0.30738... Val Loss: 0.30685\n",
      "Epoch: 420/2000... Step: 420... Loss: 0.30716... Val Loss: 0.30661\n",
      "Epoch: 421/2000... Step: 421... Loss: 0.30695... Val Loss: 0.30638\n",
      "Epoch: 422/2000... Step: 422... Loss: 0.30673... Val Loss: 0.30615\n",
      "Epoch: 423/2000... Step: 423... Loss: 0.30652... Val Loss: 0.30592\n",
      "Epoch: 424/2000... Step: 424... Loss: 0.30631... Val Loss: 0.30569\n",
      "Epoch: 425/2000... Step: 425... Loss: 0.30609... Val Loss: 0.30546\n",
      "Epoch: 426/2000... Step: 426... Loss: 0.30588... Val Loss: 0.30524\n",
      "Epoch: 427/2000... Step: 427... Loss: 0.30567... Val Loss: 0.30501\n",
      "Epoch: 428/2000... Step: 428... Loss: 0.30546... Val Loss: 0.30478\n",
      "Epoch: 429/2000... Step: 429... Loss: 0.30526... Val Loss: 0.30456\n",
      "Epoch: 430/2000... Step: 430... Loss: 0.30505... Val Loss: 0.30433\n",
      "Epoch: 431/2000... Step: 431... Loss: 0.30484... Val Loss: 0.30411\n",
      "Epoch: 432/2000... Step: 432... Loss: 0.30463... Val Loss: 0.30388\n",
      "Epoch: 433/2000... Step: 433... Loss: 0.30443... Val Loss: 0.30366\n",
      "Epoch: 434/2000... Step: 434... Loss: 0.30422... Val Loss: 0.30344\n",
      "Epoch: 435/2000... Step: 435... Loss: 0.30402... Val Loss: 0.30322\n",
      "Epoch: 436/2000... Step: 436... Loss: 0.30382... Val Loss: 0.30300\n",
      "Epoch: 437/2000... Step: 437... Loss: 0.30362... Val Loss: 0.30278\n",
      "Epoch: 438/2000... Step: 438... Loss: 0.30341... Val Loss: 0.30256\n",
      "Epoch: 439/2000... Step: 439... Loss: 0.30321... Val Loss: 0.30234\n",
      "Epoch: 440/2000... Step: 440... Loss: 0.30301... Val Loss: 0.30212\n",
      "Epoch: 441/2000... Step: 441... Loss: 0.30281... Val Loss: 0.30191\n",
      "Epoch: 442/2000... Step: 442... Loss: 0.30262... Val Loss: 0.30169\n",
      "Epoch: 443/2000... Step: 443... Loss: 0.30242... Val Loss: 0.30148\n",
      "Epoch: 444/2000... Step: 444... Loss: 0.30222... Val Loss: 0.30126\n",
      "Epoch: 445/2000... Step: 445... Loss: 0.30203... Val Loss: 0.30105\n",
      "Epoch: 446/2000... Step: 446... Loss: 0.30183... Val Loss: 0.30083\n",
      "Epoch: 447/2000... Step: 447... Loss: 0.30164... Val Loss: 0.30062\n",
      "Epoch: 448/2000... Step: 448... Loss: 0.30144... Val Loss: 0.30041\n",
      "Epoch: 449/2000... Step: 449... Loss: 0.30125... Val Loss: 0.30020\n",
      "Epoch: 450/2000... Step: 450... Loss: 0.30106... Val Loss: 0.29999\n",
      "Epoch: 451/2000... Step: 451... Loss: 0.30087... Val Loss: 0.29978\n",
      "Epoch: 452/2000... Step: 452... Loss: 0.30068... Val Loss: 0.29957\n",
      "Epoch: 453/2000... Step: 453... Loss: 0.30049... Val Loss: 0.29936\n",
      "Epoch: 454/2000... Step: 454... Loss: 0.30030... Val Loss: 0.29916\n",
      "Epoch: 455/2000... Step: 455... Loss: 0.30011... Val Loss: 0.29895\n",
      "Epoch: 456/2000... Step: 456... Loss: 0.29992... Val Loss: 0.29874\n",
      "Epoch: 457/2000... Step: 457... Loss: 0.29973... Val Loss: 0.29854\n",
      "Epoch: 458/2000... Step: 458... Loss: 0.29955... Val Loss: 0.29833\n",
      "Epoch: 459/2000... Step: 459... Loss: 0.29936... Val Loss: 0.29813\n",
      "Epoch: 460/2000... Step: 460... Loss: 0.29918... Val Loss: 0.29793\n",
      "Epoch: 461/2000... Step: 461... Loss: 0.29899... Val Loss: 0.29772\n",
      "Epoch: 462/2000... Step: 462... Loss: 0.29881... Val Loss: 0.29752\n",
      "Epoch: 463/2000... Step: 463... Loss: 0.29863... Val Loss: 0.29732\n",
      "Epoch: 464/2000... Step: 464... Loss: 0.29845... Val Loss: 0.29712\n",
      "Epoch: 465/2000... Step: 465... Loss: 0.29826... Val Loss: 0.29692\n",
      "Epoch: 466/2000... Step: 466... Loss: 0.29808... Val Loss: 0.29672\n",
      "Epoch: 467/2000... Step: 467... Loss: 0.29790... Val Loss: 0.29652\n",
      "Epoch: 468/2000... Step: 468... Loss: 0.29772... Val Loss: 0.29632\n",
      "Epoch: 469/2000... Step: 469... Loss: 0.29755... Val Loss: 0.29613\n",
      "Epoch: 470/2000... Step: 470... Loss: 0.29737... Val Loss: 0.29593\n",
      "Epoch: 471/2000... Step: 471... Loss: 0.29719... Val Loss: 0.29573\n",
      "Epoch: 472/2000... Step: 472... Loss: 0.29701... Val Loss: 0.29554\n",
      "Epoch: 473/2000... Step: 473... Loss: 0.29684... Val Loss: 0.29534\n",
      "Epoch: 474/2000... Step: 474... Loss: 0.29666... Val Loss: 0.29515\n",
      "Epoch: 475/2000... Step: 475... Loss: 0.29649... Val Loss: 0.29496\n",
      "Epoch: 476/2000... Step: 476... Loss: 0.29632... Val Loss: 0.29476\n",
      "Epoch: 477/2000... Step: 477... Loss: 0.29614... Val Loss: 0.29457\n",
      "Epoch: 478/2000... Step: 478... Loss: 0.29597... Val Loss: 0.29438\n",
      "Epoch: 479/2000... Step: 479... Loss: 0.29580... Val Loss: 0.29419\n",
      "Epoch: 480/2000... Step: 480... Loss: 0.29563... Val Loss: 0.29400\n",
      "Epoch: 481/2000... Step: 481... Loss: 0.29546... Val Loss: 0.29381\n",
      "Epoch: 482/2000... Step: 482... Loss: 0.29529... Val Loss: 0.29362\n",
      "Epoch: 483/2000... Step: 483... Loss: 0.29512... Val Loss: 0.29343\n",
      "Epoch: 484/2000... Step: 484... Loss: 0.29495... Val Loss: 0.29324\n",
      "Epoch: 485/2000... Step: 485... Loss: 0.29478... Val Loss: 0.29306\n",
      "Epoch: 486/2000... Step: 486... Loss: 0.29462... Val Loss: 0.29287\n",
      "Epoch: 487/2000... Step: 487... Loss: 0.29445... Val Loss: 0.29268\n",
      "Epoch: 488/2000... Step: 488... Loss: 0.29428... Val Loss: 0.29250\n",
      "Epoch: 489/2000... Step: 489... Loss: 0.29412... Val Loss: 0.29231\n",
      "Epoch: 490/2000... Step: 490... Loss: 0.29395... Val Loss: 0.29213\n",
      "Epoch: 491/2000... Step: 491... Loss: 0.29379... Val Loss: 0.29195\n",
      "Epoch: 492/2000... Step: 492... Loss: 0.29363... Val Loss: 0.29176\n",
      "Epoch: 493/2000... Step: 493... Loss: 0.29347... Val Loss: 0.29158\n",
      "Epoch: 494/2000... Step: 494... Loss: 0.29330... Val Loss: 0.29140\n",
      "Epoch: 495/2000... Step: 495... Loss: 0.29314... Val Loss: 0.29122\n",
      "Epoch: 496/2000... Step: 496... Loss: 0.29298... Val Loss: 0.29104\n",
      "Epoch: 497/2000... Step: 497... Loss: 0.29282... Val Loss: 0.29086\n",
      "Epoch: 498/2000... Step: 498... Loss: 0.29266... Val Loss: 0.29068\n",
      "Epoch: 499/2000... Step: 499... Loss: 0.29250... Val Loss: 0.29050\n",
      "Epoch: 500/2000... Step: 500... Loss: 0.29234... Val Loss: 0.29032\n",
      "Epoch: 501/2000... Step: 501... Loss: 0.29219... Val Loss: 0.29015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 502/2000... Step: 502... Loss: 0.29203... Val Loss: 0.28997\n",
      "Epoch: 503/2000... Step: 503... Loss: 0.29187... Val Loss: 0.28979\n",
      "Epoch: 504/2000... Step: 504... Loss: 0.29172... Val Loss: 0.28962\n",
      "Epoch: 505/2000... Step: 505... Loss: 0.29156... Val Loss: 0.28944\n",
      "Epoch: 506/2000... Step: 506... Loss: 0.29141... Val Loss: 0.28927\n",
      "Epoch: 507/2000... Step: 507... Loss: 0.29125... Val Loss: 0.28909\n",
      "Epoch: 508/2000... Step: 508... Loss: 0.29110... Val Loss: 0.28892\n",
      "Epoch: 509/2000... Step: 509... Loss: 0.29095... Val Loss: 0.28875\n",
      "Epoch: 510/2000... Step: 510... Loss: 0.29079... Val Loss: 0.28858\n",
      "Epoch: 511/2000... Step: 511... Loss: 0.29064... Val Loss: 0.28840\n",
      "Epoch: 512/2000... Step: 512... Loss: 0.29049... Val Loss: 0.28823\n",
      "Epoch: 513/2000... Step: 513... Loss: 0.29034... Val Loss: 0.28806\n",
      "Epoch: 514/2000... Step: 514... Loss: 0.29019... Val Loss: 0.28789\n",
      "Epoch: 515/2000... Step: 515... Loss: 0.29004... Val Loss: 0.28772\n",
      "Epoch: 516/2000... Step: 516... Loss: 0.28989... Val Loss: 0.28756\n",
      "Epoch: 517/2000... Step: 517... Loss: 0.28974... Val Loss: 0.28739\n",
      "Epoch: 518/2000... Step: 518... Loss: 0.28960... Val Loss: 0.28722\n",
      "Epoch: 519/2000... Step: 519... Loss: 0.28945... Val Loss: 0.28705\n",
      "Epoch: 520/2000... Step: 520... Loss: 0.28930... Val Loss: 0.28689\n",
      "Epoch: 521/2000... Step: 521... Loss: 0.28916... Val Loss: 0.28672\n",
      "Epoch: 522/2000... Step: 522... Loss: 0.28901... Val Loss: 0.28655\n",
      "Epoch: 523/2000... Step: 523... Loss: 0.28887... Val Loss: 0.28639\n",
      "Epoch: 524/2000... Step: 524... Loss: 0.28872... Val Loss: 0.28622\n",
      "Epoch: 525/2000... Step: 525... Loss: 0.28858... Val Loss: 0.28606\n",
      "Epoch: 526/2000... Step: 526... Loss: 0.28843... Val Loss: 0.28590\n",
      "Epoch: 527/2000... Step: 527... Loss: 0.28829... Val Loss: 0.28573\n",
      "Epoch: 528/2000... Step: 528... Loss: 0.28815... Val Loss: 0.28557\n",
      "Epoch: 529/2000... Step: 529... Loss: 0.28801... Val Loss: 0.28541\n",
      "Epoch: 530/2000... Step: 530... Loss: 0.28787... Val Loss: 0.28525\n",
      "Epoch: 531/2000... Step: 531... Loss: 0.28773... Val Loss: 0.28509\n",
      "Epoch: 532/2000... Step: 532... Loss: 0.28759... Val Loss: 0.28493\n",
      "Epoch: 533/2000... Step: 533... Loss: 0.28745... Val Loss: 0.28477\n",
      "Epoch: 534/2000... Step: 534... Loss: 0.28731... Val Loss: 0.28461\n",
      "Epoch: 535/2000... Step: 535... Loss: 0.28717... Val Loss: 0.28445\n",
      "Epoch: 536/2000... Step: 536... Loss: 0.28703... Val Loss: 0.28429\n",
      "Epoch: 537/2000... Step: 537... Loss: 0.28689... Val Loss: 0.28414\n",
      "Epoch: 538/2000... Step: 538... Loss: 0.28676... Val Loss: 0.28398\n",
      "Epoch: 539/2000... Step: 539... Loss: 0.28662... Val Loss: 0.28382\n",
      "Epoch: 540/2000... Step: 540... Loss: 0.28648... Val Loss: 0.28367\n",
      "Epoch: 541/2000... Step: 541... Loss: 0.28635... Val Loss: 0.28351\n",
      "Epoch: 542/2000... Step: 542... Loss: 0.28621... Val Loss: 0.28336\n",
      "Epoch: 543/2000... Step: 543... Loss: 0.28608... Val Loss: 0.28320\n",
      "Epoch: 544/2000... Step: 544... Loss: 0.28595... Val Loss: 0.28305\n",
      "Epoch: 545/2000... Step: 545... Loss: 0.28581... Val Loss: 0.28289\n",
      "Epoch: 546/2000... Step: 546... Loss: 0.28568... Val Loss: 0.28274\n",
      "Epoch: 547/2000... Step: 547... Loss: 0.28555... Val Loss: 0.28259\n",
      "Epoch: 548/2000... Step: 548... Loss: 0.28541... Val Loss: 0.28244\n",
      "Epoch: 549/2000... Step: 549... Loss: 0.28528... Val Loss: 0.28229\n",
      "Epoch: 550/2000... Step: 550... Loss: 0.28515... Val Loss: 0.28213\n",
      "Epoch: 551/2000... Step: 551... Loss: 0.28502... Val Loss: 0.28198\n",
      "Epoch: 552/2000... Step: 552... Loss: 0.28489... Val Loss: 0.28183\n",
      "Epoch: 553/2000... Step: 553... Loss: 0.28476... Val Loss: 0.28168\n",
      "Epoch: 554/2000... Step: 554... Loss: 0.28463... Val Loss: 0.28154\n",
      "Epoch: 555/2000... Step: 555... Loss: 0.28451... Val Loss: 0.28139\n",
      "Epoch: 556/2000... Step: 556... Loss: 0.28438... Val Loss: 0.28124\n",
      "Epoch: 557/2000... Step: 557... Loss: 0.28425... Val Loss: 0.28109\n",
      "Epoch: 558/2000... Step: 558... Loss: 0.28412... Val Loss: 0.28094\n",
      "Epoch: 559/2000... Step: 559... Loss: 0.28400... Val Loss: 0.28080\n",
      "Epoch: 560/2000... Step: 560... Loss: 0.28387... Val Loss: 0.28065\n",
      "Epoch: 561/2000... Step: 561... Loss: 0.28374... Val Loss: 0.28051\n",
      "Epoch: 562/2000... Step: 562... Loss: 0.28362... Val Loss: 0.28036\n",
      "Epoch: 563/2000... Step: 563... Loss: 0.28349... Val Loss: 0.28022\n",
      "Epoch: 564/2000... Step: 564... Loss: 0.28337... Val Loss: 0.28007\n",
      "Epoch: 565/2000... Step: 565... Loss: 0.28325... Val Loss: 0.27993\n",
      "Epoch: 566/2000... Step: 566... Loss: 0.28312... Val Loss: 0.27979\n",
      "Epoch: 567/2000... Step: 567... Loss: 0.28300... Val Loss: 0.27964\n",
      "Epoch: 568/2000... Step: 568... Loss: 0.28288... Val Loss: 0.27950\n",
      "Epoch: 569/2000... Step: 569... Loss: 0.28276... Val Loss: 0.27936\n",
      "Epoch: 570/2000... Step: 570... Loss: 0.28264... Val Loss: 0.27922\n",
      "Epoch: 571/2000... Step: 571... Loss: 0.28251... Val Loss: 0.27908\n",
      "Epoch: 572/2000... Step: 572... Loss: 0.28239... Val Loss: 0.27894\n",
      "Epoch: 573/2000... Step: 573... Loss: 0.28227... Val Loss: 0.27880\n",
      "Epoch: 574/2000... Step: 574... Loss: 0.28215... Val Loss: 0.27866\n",
      "Epoch: 575/2000... Step: 575... Loss: 0.28203... Val Loss: 0.27852\n",
      "Epoch: 576/2000... Step: 576... Loss: 0.28192... Val Loss: 0.27838\n",
      "Epoch: 577/2000... Step: 577... Loss: 0.28180... Val Loss: 0.27824\n",
      "Epoch: 578/2000... Step: 578... Loss: 0.28168... Val Loss: 0.27810\n",
      "Epoch: 579/2000... Step: 579... Loss: 0.28156... Val Loss: 0.27797\n",
      "Epoch: 580/2000... Step: 580... Loss: 0.28145... Val Loss: 0.27783\n",
      "Epoch: 581/2000... Step: 581... Loss: 0.28133... Val Loss: 0.27769\n",
      "Epoch: 582/2000... Step: 582... Loss: 0.28121... Val Loss: 0.27756\n",
      "Epoch: 583/2000... Step: 583... Loss: 0.28110... Val Loss: 0.27742\n",
      "Epoch: 584/2000... Step: 584... Loss: 0.28098... Val Loss: 0.27729\n",
      "Epoch: 585/2000... Step: 585... Loss: 0.28087... Val Loss: 0.27715\n",
      "Epoch: 586/2000... Step: 586... Loss: 0.28075... Val Loss: 0.27702\n",
      "Epoch: 587/2000... Step: 587... Loss: 0.28064... Val Loss: 0.27688\n",
      "Epoch: 588/2000... Step: 588... Loss: 0.28052... Val Loss: 0.27675\n",
      "Epoch: 589/2000... Step: 589... Loss: 0.28041... Val Loss: 0.27662\n",
      "Epoch: 590/2000... Step: 590... Loss: 0.28030... Val Loss: 0.27648\n",
      "Epoch: 591/2000... Step: 591... Loss: 0.28019... Val Loss: 0.27635\n",
      "Epoch: 592/2000... Step: 592... Loss: 0.28007... Val Loss: 0.27622\n",
      "Epoch: 593/2000... Step: 593... Loss: 0.27996... Val Loss: 0.27609\n",
      "Epoch: 594/2000... Step: 594... Loss: 0.27985... Val Loss: 0.27596\n",
      "Epoch: 595/2000... Step: 595... Loss: 0.27974... Val Loss: 0.27583\n",
      "Epoch: 596/2000... Step: 596... Loss: 0.27963... Val Loss: 0.27570\n",
      "Epoch: 597/2000... Step: 597... Loss: 0.27952... Val Loss: 0.27557\n",
      "Epoch: 598/2000... Step: 598... Loss: 0.27941... Val Loss: 0.27544\n",
      "Epoch: 599/2000... Step: 599... Loss: 0.27930... Val Loss: 0.27531\n",
      "Epoch: 600/2000... Step: 600... Loss: 0.27919... Val Loss: 0.27518\n",
      "Epoch: 601/2000... Step: 601... Loss: 0.27909... Val Loss: 0.27505\n",
      "Epoch: 602/2000... Step: 602... Loss: 0.27898... Val Loss: 0.27493\n",
      "Epoch: 603/2000... Step: 603... Loss: 0.27887... Val Loss: 0.27480\n",
      "Epoch: 604/2000... Step: 604... Loss: 0.27876... Val Loss: 0.27467\n",
      "Epoch: 605/2000... Step: 605... Loss: 0.27866... Val Loss: 0.27455\n",
      "Epoch: 606/2000... Step: 606... Loss: 0.27855... Val Loss: 0.27442\n",
      "Epoch: 607/2000... Step: 607... Loss: 0.27844... Val Loss: 0.27430\n",
      "Epoch: 608/2000... Step: 608... Loss: 0.27834... Val Loss: 0.27417\n",
      "Epoch: 609/2000... Step: 609... Loss: 0.27823... Val Loss: 0.27405\n",
      "Epoch: 610/2000... Step: 610... Loss: 0.27813... Val Loss: 0.27392\n",
      "Epoch: 611/2000... Step: 611... Loss: 0.27802... Val Loss: 0.27380\n",
      "Epoch: 612/2000... Step: 612... Loss: 0.27792... Val Loss: 0.27367\n",
      "Epoch: 613/2000... Step: 613... Loss: 0.27782... Val Loss: 0.27355\n",
      "Epoch: 614/2000... Step: 614... Loss: 0.27771... Val Loss: 0.27343\n",
      "Epoch: 615/2000... Step: 615... Loss: 0.27761... Val Loss: 0.27331\n",
      "Epoch: 616/2000... Step: 616... Loss: 0.27751... Val Loss: 0.27318\n",
      "Epoch: 617/2000... Step: 617... Loss: 0.27741... Val Loss: 0.27306\n",
      "Epoch: 618/2000... Step: 618... Loss: 0.27730... Val Loss: 0.27294\n",
      "Epoch: 619/2000... Step: 619... Loss: 0.27720... Val Loss: 0.27282\n",
      "Epoch: 620/2000... Step: 620... Loss: 0.27710... Val Loss: 0.27270\n",
      "Epoch: 621/2000... Step: 621... Loss: 0.27700... Val Loss: 0.27258\n",
      "Epoch: 622/2000... Step: 622... Loss: 0.27690... Val Loss: 0.27246\n",
      "Epoch: 623/2000... Step: 623... Loss: 0.27680... Val Loss: 0.27234\n",
      "Epoch: 624/2000... Step: 624... Loss: 0.27670... Val Loss: 0.27222\n",
      "Epoch: 625/2000... Step: 625... Loss: 0.27660... Val Loss: 0.27210\n",
      "Epoch: 626/2000... Step: 626... Loss: 0.27650... Val Loss: 0.27199\n",
      "Epoch: 627/2000... Step: 627... Loss: 0.27640... Val Loss: 0.27187\n",
      "Epoch: 628/2000... Step: 628... Loss: 0.27631... Val Loss: 0.27175\n",
      "Epoch: 629/2000... Step: 629... Loss: 0.27621... Val Loss: 0.27163\n",
      "Epoch: 630/2000... Step: 630... Loss: 0.27611... Val Loss: 0.27152\n",
      "Epoch: 631/2000... Step: 631... Loss: 0.27601... Val Loss: 0.27140\n",
      "Epoch: 632/2000... Step: 632... Loss: 0.27592... Val Loss: 0.27129\n",
      "Epoch: 633/2000... Step: 633... Loss: 0.27582... Val Loss: 0.27117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 634/2000... Step: 634... Loss: 0.27573... Val Loss: 0.27106\n",
      "Epoch: 635/2000... Step: 635... Loss: 0.27563... Val Loss: 0.27094\n",
      "Epoch: 636/2000... Step: 636... Loss: 0.27553... Val Loss: 0.27083\n",
      "Epoch: 637/2000... Step: 637... Loss: 0.27544... Val Loss: 0.27071\n",
      "Epoch: 638/2000... Step: 638... Loss: 0.27534... Val Loss: 0.27060\n",
      "Epoch: 639/2000... Step: 639... Loss: 0.27525... Val Loss: 0.27049\n",
      "Epoch: 640/2000... Step: 640... Loss: 0.27516... Val Loss: 0.27037\n",
      "Epoch: 641/2000... Step: 641... Loss: 0.27506... Val Loss: 0.27026\n",
      "Epoch: 642/2000... Step: 642... Loss: 0.27497... Val Loss: 0.27015\n",
      "Epoch: 643/2000... Step: 643... Loss: 0.27488... Val Loss: 0.27004\n",
      "Epoch: 644/2000... Step: 644... Loss: 0.27478... Val Loss: 0.26993\n",
      "Epoch: 645/2000... Step: 645... Loss: 0.27469... Val Loss: 0.26981\n",
      "Epoch: 646/2000... Step: 646... Loss: 0.27460... Val Loss: 0.26970\n",
      "Epoch: 647/2000... Step: 647... Loss: 0.27451... Val Loss: 0.26959\n",
      "Epoch: 648/2000... Step: 648... Loss: 0.27442... Val Loss: 0.26948\n",
      "Epoch: 649/2000... Step: 649... Loss: 0.27433... Val Loss: 0.26937\n",
      "Epoch: 650/2000... Step: 650... Loss: 0.27424... Val Loss: 0.26926\n",
      "Epoch: 651/2000... Step: 651... Loss: 0.27415... Val Loss: 0.26916\n",
      "Epoch: 652/2000... Step: 652... Loss: 0.27406... Val Loss: 0.26905\n",
      "Epoch: 653/2000... Step: 653... Loss: 0.27397... Val Loss: 0.26894\n",
      "Epoch: 654/2000... Step: 654... Loss: 0.27388... Val Loss: 0.26883\n",
      "Epoch: 655/2000... Step: 655... Loss: 0.27379... Val Loss: 0.26872\n",
      "Epoch: 656/2000... Step: 656... Loss: 0.27370... Val Loss: 0.26862\n",
      "Epoch: 657/2000... Step: 657... Loss: 0.27361... Val Loss: 0.26851\n",
      "Epoch: 658/2000... Step: 658... Loss: 0.27352... Val Loss: 0.26840\n",
      "Epoch: 659/2000... Step: 659... Loss: 0.27343... Val Loss: 0.26830\n",
      "Epoch: 660/2000... Step: 660... Loss: 0.27335... Val Loss: 0.26819\n",
      "Epoch: 661/2000... Step: 661... Loss: 0.27326... Val Loss: 0.26808\n",
      "Epoch: 662/2000... Step: 662... Loss: 0.27317... Val Loss: 0.26798\n",
      "Epoch: 663/2000... Step: 663... Loss: 0.27309... Val Loss: 0.26787\n",
      "Epoch: 664/2000... Step: 664... Loss: 0.27300... Val Loss: 0.26777\n",
      "Epoch: 665/2000... Step: 665... Loss: 0.27291... Val Loss: 0.26766\n",
      "Epoch: 666/2000... Step: 666... Loss: 0.27283... Val Loss: 0.26756\n",
      "Epoch: 667/2000... Step: 667... Loss: 0.27274... Val Loss: 0.26746\n",
      "Epoch: 668/2000... Step: 668... Loss: 0.27266... Val Loss: 0.26735\n",
      "Epoch: 669/2000... Step: 669... Loss: 0.27257... Val Loss: 0.26725\n",
      "Epoch: 670/2000... Step: 670... Loss: 0.27249... Val Loss: 0.26715\n",
      "Epoch: 671/2000... Step: 671... Loss: 0.27241... Val Loss: 0.26705\n",
      "Epoch: 672/2000... Step: 672... Loss: 0.27232... Val Loss: 0.26694\n",
      "Epoch: 673/2000... Step: 673... Loss: 0.27224... Val Loss: 0.26684\n",
      "Epoch: 674/2000... Step: 674... Loss: 0.27216... Val Loss: 0.26674\n",
      "Epoch: 675/2000... Step: 675... Loss: 0.27207... Val Loss: 0.26664\n",
      "Epoch: 676/2000... Step: 676... Loss: 0.27199... Val Loss: 0.26654\n",
      "Epoch: 677/2000... Step: 677... Loss: 0.27191... Val Loss: 0.26644\n",
      "Epoch: 678/2000... Step: 678... Loss: 0.27183... Val Loss: 0.26634\n",
      "Epoch: 679/2000... Step: 679... Loss: 0.27174... Val Loss: 0.26624\n",
      "Epoch: 680/2000... Step: 680... Loss: 0.27166... Val Loss: 0.26614\n",
      "Epoch: 681/2000... Step: 681... Loss: 0.27158... Val Loss: 0.26604\n",
      "Epoch: 682/2000... Step: 682... Loss: 0.27150... Val Loss: 0.26594\n",
      "Epoch: 683/2000... Step: 683... Loss: 0.27142... Val Loss: 0.26584\n",
      "Epoch: 684/2000... Step: 684... Loss: 0.27134... Val Loss: 0.26574\n",
      "Epoch: 685/2000... Step: 685... Loss: 0.27126... Val Loss: 0.26565\n",
      "Epoch: 686/2000... Step: 686... Loss: 0.27118... Val Loss: 0.26555\n",
      "Epoch: 687/2000... Step: 687... Loss: 0.27110... Val Loss: 0.26545\n",
      "Epoch: 688/2000... Step: 688... Loss: 0.27102... Val Loss: 0.26535\n",
      "Epoch: 689/2000... Step: 689... Loss: 0.27094... Val Loss: 0.26526\n",
      "Epoch: 690/2000... Step: 690... Loss: 0.27086... Val Loss: 0.26516\n",
      "Epoch: 691/2000... Step: 691... Loss: 0.27079... Val Loss: 0.26506\n",
      "Epoch: 692/2000... Step: 692... Loss: 0.27071... Val Loss: 0.26497\n",
      "Epoch: 693/2000... Step: 693... Loss: 0.27063... Val Loss: 0.26487\n",
      "Epoch: 694/2000... Step: 694... Loss: 0.27055... Val Loss: 0.26478\n",
      "Epoch: 695/2000... Step: 695... Loss: 0.27048... Val Loss: 0.26468\n",
      "Epoch: 696/2000... Step: 696... Loss: 0.27040... Val Loss: 0.26459\n",
      "Epoch: 697/2000... Step: 697... Loss: 0.27032... Val Loss: 0.26449\n",
      "Epoch: 698/2000... Step: 698... Loss: 0.27025... Val Loss: 0.26440\n",
      "Epoch: 699/2000... Step: 699... Loss: 0.27017... Val Loss: 0.26431\n",
      "Epoch: 700/2000... Step: 700... Loss: 0.27009... Val Loss: 0.26421\n",
      "Epoch: 701/2000... Step: 701... Loss: 0.27002... Val Loss: 0.26412\n",
      "Epoch: 702/2000... Step: 702... Loss: 0.26994... Val Loss: 0.26403\n",
      "Epoch: 703/2000... Step: 703... Loss: 0.26987... Val Loss: 0.26393\n",
      "Epoch: 704/2000... Step: 704... Loss: 0.26979... Val Loss: 0.26384\n",
      "Epoch: 705/2000... Step: 705... Loss: 0.26972... Val Loss: 0.26375\n",
      "Epoch: 706/2000... Step: 706... Loss: 0.26965... Val Loss: 0.26366\n",
      "Epoch: 707/2000... Step: 707... Loss: 0.26957... Val Loss: 0.26357\n",
      "Epoch: 708/2000... Step: 708... Loss: 0.26950... Val Loss: 0.26347\n",
      "Epoch: 709/2000... Step: 709... Loss: 0.26942... Val Loss: 0.26338\n",
      "Epoch: 710/2000... Step: 710... Loss: 0.26935... Val Loss: 0.26329\n",
      "Epoch: 711/2000... Step: 711... Loss: 0.26928... Val Loss: 0.26320\n",
      "Epoch: 712/2000... Step: 712... Loss: 0.26921... Val Loss: 0.26311\n",
      "Epoch: 713/2000... Step: 713... Loss: 0.26913... Val Loss: 0.26302\n",
      "Epoch: 714/2000... Step: 714... Loss: 0.26906... Val Loss: 0.26293\n",
      "Epoch: 715/2000... Step: 715... Loss: 0.26899... Val Loss: 0.26284\n",
      "Epoch: 716/2000... Step: 716... Loss: 0.26892... Val Loss: 0.26276\n",
      "Epoch: 717/2000... Step: 717... Loss: 0.26885... Val Loss: 0.26267\n",
      "Epoch: 718/2000... Step: 718... Loss: 0.26877... Val Loss: 0.26258\n",
      "Epoch: 719/2000... Step: 719... Loss: 0.26870... Val Loss: 0.26249\n",
      "Epoch: 720/2000... Step: 720... Loss: 0.26863... Val Loss: 0.26240\n",
      "Epoch: 721/2000... Step: 721... Loss: 0.26856... Val Loss: 0.26231\n",
      "Epoch: 722/2000... Step: 722... Loss: 0.26849... Val Loss: 0.26223\n",
      "Epoch: 723/2000... Step: 723... Loss: 0.26842... Val Loss: 0.26214\n",
      "Epoch: 724/2000... Step: 724... Loss: 0.26835... Val Loss: 0.26205\n",
      "Epoch: 725/2000... Step: 725... Loss: 0.26828... Val Loss: 0.26197\n",
      "Epoch: 726/2000... Step: 726... Loss: 0.26821... Val Loss: 0.26188\n",
      "Epoch: 727/2000... Step: 727... Loss: 0.26814... Val Loss: 0.26179\n",
      "Epoch: 728/2000... Step: 728... Loss: 0.26808... Val Loss: 0.26171\n",
      "Epoch: 729/2000... Step: 729... Loss: 0.26801... Val Loss: 0.26162\n",
      "Epoch: 730/2000... Step: 730... Loss: 0.26794... Val Loss: 0.26154\n",
      "Epoch: 731/2000... Step: 731... Loss: 0.26787... Val Loss: 0.26145\n",
      "Epoch: 732/2000... Step: 732... Loss: 0.26780... Val Loss: 0.26137\n",
      "Epoch: 733/2000... Step: 733... Loss: 0.26774... Val Loss: 0.26128\n",
      "Epoch: 734/2000... Step: 734... Loss: 0.26767... Val Loss: 0.26120\n",
      "Epoch: 735/2000... Step: 735... Loss: 0.26760... Val Loss: 0.26111\n",
      "Epoch: 736/2000... Step: 736... Loss: 0.26753... Val Loss: 0.26103\n",
      "Epoch: 737/2000... Step: 737... Loss: 0.26747... Val Loss: 0.26095\n",
      "Epoch: 738/2000... Step: 738... Loss: 0.26740... Val Loss: 0.26086\n",
      "Epoch: 739/2000... Step: 739... Loss: 0.26734... Val Loss: 0.26078\n",
      "Epoch: 740/2000... Step: 740... Loss: 0.26727... Val Loss: 0.26070\n",
      "Epoch: 741/2000... Step: 741... Loss: 0.26720... Val Loss: 0.26062\n",
      "Epoch: 742/2000... Step: 742... Loss: 0.26714... Val Loss: 0.26053\n",
      "Epoch: 743/2000... Step: 743... Loss: 0.26707... Val Loss: 0.26045\n",
      "Epoch: 744/2000... Step: 744... Loss: 0.26701... Val Loss: 0.26037\n",
      "Epoch: 745/2000... Step: 745... Loss: 0.26694... Val Loss: 0.26029\n",
      "Epoch: 746/2000... Step: 746... Loss: 0.26688... Val Loss: 0.26021\n",
      "Epoch: 747/2000... Step: 747... Loss: 0.26681... Val Loss: 0.26013\n",
      "Epoch: 748/2000... Step: 748... Loss: 0.26675... Val Loss: 0.26005\n",
      "Epoch: 749/2000... Step: 749... Loss: 0.26669... Val Loss: 0.25997\n",
      "Epoch: 750/2000... Step: 750... Loss: 0.26662... Val Loss: 0.25989\n",
      "Epoch: 751/2000... Step: 751... Loss: 0.26656... Val Loss: 0.25981\n",
      "Epoch: 752/2000... Step: 752... Loss: 0.26650... Val Loss: 0.25973\n",
      "Epoch: 753/2000... Step: 753... Loss: 0.26643... Val Loss: 0.25965\n",
      "Epoch: 754/2000... Step: 754... Loss: 0.26637... Val Loss: 0.25957\n",
      "Epoch: 755/2000... Step: 755... Loss: 0.26631... Val Loss: 0.25949\n",
      "Epoch: 756/2000... Step: 756... Loss: 0.26625... Val Loss: 0.25941\n",
      "Epoch: 757/2000... Step: 757... Loss: 0.26618... Val Loss: 0.25933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 758/2000... Step: 758... Loss: 0.26612... Val Loss: 0.25925\n",
      "Epoch: 759/2000... Step: 759... Loss: 0.26606... Val Loss: 0.25917\n",
      "Epoch: 760/2000... Step: 760... Loss: 0.26600... Val Loss: 0.25910\n",
      "Epoch: 761/2000... Step: 761... Loss: 0.26594... Val Loss: 0.25902\n",
      "Epoch: 762/2000... Step: 762... Loss: 0.26588... Val Loss: 0.25894\n",
      "Epoch: 763/2000... Step: 763... Loss: 0.26582... Val Loss: 0.25886\n",
      "Epoch: 764/2000... Step: 764... Loss: 0.26575... Val Loss: 0.25879\n",
      "Epoch: 765/2000... Step: 765... Loss: 0.26569... Val Loss: 0.25871\n",
      "Epoch: 766/2000... Step: 766... Loss: 0.26563... Val Loss: 0.25863\n",
      "Epoch: 767/2000... Step: 767... Loss: 0.26557... Val Loss: 0.25856\n",
      "Epoch: 768/2000... Step: 768... Loss: 0.26551... Val Loss: 0.25848\n",
      "Epoch: 769/2000... Step: 769... Loss: 0.26545... Val Loss: 0.25841\n",
      "Epoch: 770/2000... Step: 770... Loss: 0.26539... Val Loss: 0.25833\n",
      "Epoch: 771/2000... Step: 771... Loss: 0.26534... Val Loss: 0.25826\n",
      "Epoch: 772/2000... Step: 772... Loss: 0.26528... Val Loss: 0.25818\n",
      "Epoch: 773/2000... Step: 773... Loss: 0.26522... Val Loss: 0.25811\n",
      "Epoch: 774/2000... Step: 774... Loss: 0.26516... Val Loss: 0.25803\n",
      "Epoch: 775/2000... Step: 775... Loss: 0.26510... Val Loss: 0.25796\n",
      "Epoch: 776/2000... Step: 776... Loss: 0.26504... Val Loss: 0.25788\n",
      "Epoch: 777/2000... Step: 777... Loss: 0.26498... Val Loss: 0.25781\n",
      "Epoch: 778/2000... Step: 778... Loss: 0.26493... Val Loss: 0.25773\n",
      "Epoch: 779/2000... Step: 779... Loss: 0.26487... Val Loss: 0.25766\n",
      "Epoch: 780/2000... Step: 780... Loss: 0.26481... Val Loss: 0.25759\n",
      "Epoch: 781/2000... Step: 781... Loss: 0.26475... Val Loss: 0.25751\n",
      "Epoch: 782/2000... Step: 782... Loss: 0.26470... Val Loss: 0.25744\n",
      "Epoch: 783/2000... Step: 783... Loss: 0.26464... Val Loss: 0.25737\n",
      "Epoch: 784/2000... Step: 784... Loss: 0.26458... Val Loss: 0.25730\n",
      "Epoch: 785/2000... Step: 785... Loss: 0.26453... Val Loss: 0.25722\n",
      "Epoch: 786/2000... Step: 786... Loss: 0.26447... Val Loss: 0.25715\n",
      "Epoch: 787/2000... Step: 787... Loss: 0.26442... Val Loss: 0.25708\n",
      "Epoch: 788/2000... Step: 788... Loss: 0.26436... Val Loss: 0.25701\n",
      "Epoch: 789/2000... Step: 789... Loss: 0.26430... Val Loss: 0.25694\n",
      "Epoch: 790/2000... Step: 790... Loss: 0.26425... Val Loss: 0.25687\n",
      "Epoch: 791/2000... Step: 791... Loss: 0.26419... Val Loss: 0.25680\n",
      "Epoch: 792/2000... Step: 792... Loss: 0.26414... Val Loss: 0.25673\n",
      "Epoch: 793/2000... Step: 793... Loss: 0.26408... Val Loss: 0.25665\n",
      "Epoch: 794/2000... Step: 794... Loss: 0.26403... Val Loss: 0.25658\n",
      "Epoch: 795/2000... Step: 795... Loss: 0.26397... Val Loss: 0.25651\n",
      "Epoch: 796/2000... Step: 796... Loss: 0.26392... Val Loss: 0.25644\n",
      "Epoch: 797/2000... Step: 797... Loss: 0.26386... Val Loss: 0.25637\n",
      "Epoch: 798/2000... Step: 798... Loss: 0.26381... Val Loss: 0.25631\n",
      "Epoch: 799/2000... Step: 799... Loss: 0.26376... Val Loss: 0.25624\n",
      "Epoch: 800/2000... Step: 800... Loss: 0.26370... Val Loss: 0.25617\n",
      "Epoch: 801/2000... Step: 801... Loss: 0.26365... Val Loss: 0.25610\n",
      "Epoch: 802/2000... Step: 802... Loss: 0.26360... Val Loss: 0.25603\n",
      "Epoch: 803/2000... Step: 803... Loss: 0.26354... Val Loss: 0.25596\n",
      "Epoch: 804/2000... Step: 804... Loss: 0.26349... Val Loss: 0.25589\n",
      "Epoch: 805/2000... Step: 805... Loss: 0.26344... Val Loss: 0.25582\n",
      "Epoch: 806/2000... Step: 806... Loss: 0.26339... Val Loss: 0.25576\n",
      "Epoch: 807/2000... Step: 807... Loss: 0.26333... Val Loss: 0.25569\n",
      "Epoch: 808/2000... Step: 808... Loss: 0.26328... Val Loss: 0.25562\n",
      "Epoch: 809/2000... Step: 809... Loss: 0.26323... Val Loss: 0.25555\n",
      "Epoch: 810/2000... Step: 810... Loss: 0.26318... Val Loss: 0.25549\n",
      "Epoch: 811/2000... Step: 811... Loss: 0.26313... Val Loss: 0.25542\n",
      "Epoch: 812/2000... Step: 812... Loss: 0.26307... Val Loss: 0.25535\n",
      "Epoch: 813/2000... Step: 813... Loss: 0.26302... Val Loss: 0.25529\n",
      "Epoch: 814/2000... Step: 814... Loss: 0.26297... Val Loss: 0.25522\n",
      "Epoch: 815/2000... Step: 815... Loss: 0.26292... Val Loss: 0.25515\n",
      "Epoch: 816/2000... Step: 816... Loss: 0.26287... Val Loss: 0.25509\n",
      "Epoch: 817/2000... Step: 817... Loss: 0.26282... Val Loss: 0.25502\n",
      "Epoch: 818/2000... Step: 818... Loss: 0.26277... Val Loss: 0.25496\n",
      "Epoch: 819/2000... Step: 819... Loss: 0.26272... Val Loss: 0.25489\n",
      "Epoch: 820/2000... Step: 820... Loss: 0.26267... Val Loss: 0.25483\n",
      "Epoch: 821/2000... Step: 821... Loss: 0.26262... Val Loss: 0.25476\n",
      "Epoch: 822/2000... Step: 822... Loss: 0.26257... Val Loss: 0.25470\n",
      "Epoch: 823/2000... Step: 823... Loss: 0.26252... Val Loss: 0.25463\n",
      "Epoch: 824/2000... Step: 824... Loss: 0.26247... Val Loss: 0.25457\n",
      "Epoch: 825/2000... Step: 825... Loss: 0.26242... Val Loss: 0.25450\n",
      "Epoch: 826/2000... Step: 826... Loss: 0.26237... Val Loss: 0.25444\n",
      "Epoch: 827/2000... Step: 827... Loss: 0.26232... Val Loss: 0.25438\n",
      "Epoch: 828/2000... Step: 828... Loss: 0.26227... Val Loss: 0.25431\n",
      "Epoch: 829/2000... Step: 829... Loss: 0.26222... Val Loss: 0.25425\n",
      "Epoch: 830/2000... Step: 830... Loss: 0.26218... Val Loss: 0.25419\n",
      "Epoch: 831/2000... Step: 831... Loss: 0.26213... Val Loss: 0.25412\n",
      "Epoch: 832/2000... Step: 832... Loss: 0.26208... Val Loss: 0.25406\n",
      "Epoch: 833/2000... Step: 833... Loss: 0.26203... Val Loss: 0.25400\n",
      "Epoch: 834/2000... Step: 834... Loss: 0.26198... Val Loss: 0.25394\n",
      "Epoch: 835/2000... Step: 835... Loss: 0.26194... Val Loss: 0.25387\n",
      "Epoch: 836/2000... Step: 836... Loss: 0.26189... Val Loss: 0.25381\n",
      "Epoch: 837/2000... Step: 837... Loss: 0.26184... Val Loss: 0.25375\n",
      "Epoch: 838/2000... Step: 838... Loss: 0.26179... Val Loss: 0.25369\n",
      "Epoch: 839/2000... Step: 839... Loss: 0.26175... Val Loss: 0.25363\n",
      "Epoch: 840/2000... Step: 840... Loss: 0.26170... Val Loss: 0.25356\n",
      "Epoch: 841/2000... Step: 841... Loss: 0.26165... Val Loss: 0.25350\n",
      "Epoch: 842/2000... Step: 842... Loss: 0.26161... Val Loss: 0.25344\n",
      "Epoch: 843/2000... Step: 843... Loss: 0.26156... Val Loss: 0.25338\n",
      "Epoch: 844/2000... Step: 844... Loss: 0.26152... Val Loss: 0.25332\n",
      "Epoch: 845/2000... Step: 845... Loss: 0.26147... Val Loss: 0.25326\n",
      "Epoch: 846/2000... Step: 846... Loss: 0.26142... Val Loss: 0.25320\n",
      "Epoch: 847/2000... Step: 847... Loss: 0.26138... Val Loss: 0.25314\n",
      "Epoch: 848/2000... Step: 848... Loss: 0.26133... Val Loss: 0.25308\n",
      "Epoch: 849/2000... Step: 849... Loss: 0.26129... Val Loss: 0.25302\n",
      "Epoch: 850/2000... Step: 850... Loss: 0.26124... Val Loss: 0.25296\n",
      "Epoch: 851/2000... Step: 851... Loss: 0.26120... Val Loss: 0.25290\n",
      "Epoch: 852/2000... Step: 852... Loss: 0.26115... Val Loss: 0.25284\n",
      "Epoch: 853/2000... Step: 853... Loss: 0.26111... Val Loss: 0.25278\n",
      "Epoch: 854/2000... Step: 854... Loss: 0.26106... Val Loss: 0.25272\n",
      "Epoch: 855/2000... Step: 855... Loss: 0.26102... Val Loss: 0.25266\n",
      "Epoch: 856/2000... Step: 856... Loss: 0.26097... Val Loss: 0.25261\n",
      "Epoch: 857/2000... Step: 857... Loss: 0.26093... Val Loss: 0.25255\n",
      "Epoch: 858/2000... Step: 858... Loss: 0.26088... Val Loss: 0.25249\n",
      "Epoch: 859/2000... Step: 859... Loss: 0.26084... Val Loss: 0.25243\n",
      "Epoch: 860/2000... Step: 860... Loss: 0.26080... Val Loss: 0.25237\n",
      "Epoch: 861/2000... Step: 861... Loss: 0.26075... Val Loss: 0.25231\n",
      "Epoch: 862/2000... Step: 862... Loss: 0.26071... Val Loss: 0.25226\n",
      "Epoch: 863/2000... Step: 863... Loss: 0.26067... Val Loss: 0.25220\n",
      "Epoch: 864/2000... Step: 864... Loss: 0.26062... Val Loss: 0.25214\n",
      "Epoch: 865/2000... Step: 865... Loss: 0.26058... Val Loss: 0.25208\n",
      "Epoch: 866/2000... Step: 866... Loss: 0.26054... Val Loss: 0.25203\n",
      "Epoch: 867/2000... Step: 867... Loss: 0.26049... Val Loss: 0.25197\n",
      "Epoch: 868/2000... Step: 868... Loss: 0.26045... Val Loss: 0.25191\n",
      "Epoch: 869/2000... Step: 869... Loss: 0.26041... Val Loss: 0.25186\n",
      "Epoch: 870/2000... Step: 870... Loss: 0.26037... Val Loss: 0.25180\n",
      "Epoch: 871/2000... Step: 871... Loss: 0.26032... Val Loss: 0.25175\n",
      "Epoch: 872/2000... Step: 872... Loss: 0.26028... Val Loss: 0.25169\n",
      "Epoch: 873/2000... Step: 873... Loss: 0.26024... Val Loss: 0.25163\n",
      "Epoch: 874/2000... Step: 874... Loss: 0.26020... Val Loss: 0.25158\n",
      "Epoch: 875/2000... Step: 875... Loss: 0.26016... Val Loss: 0.25152\n",
      "Epoch: 876/2000... Step: 876... Loss: 0.26012... Val Loss: 0.25147\n",
      "Epoch: 877/2000... Step: 877... Loss: 0.26007... Val Loss: 0.25141\n",
      "Epoch: 878/2000... Step: 878... Loss: 0.26003... Val Loss: 0.25136\n",
      "Epoch: 879/2000... Step: 879... Loss: 0.25999... Val Loss: 0.25130\n",
      "Epoch: 880/2000... Step: 880... Loss: 0.25995... Val Loss: 0.25125\n",
      "Epoch: 881/2000... Step: 881... Loss: 0.25991... Val Loss: 0.25119\n",
      "Epoch: 882/2000... Step: 882... Loss: 0.25987... Val Loss: 0.25114\n",
      "Epoch: 883/2000... Step: 883... Loss: 0.25983... Val Loss: 0.25108\n",
      "Epoch: 884/2000... Step: 884... Loss: 0.25979... Val Loss: 0.25103\n",
      "Epoch: 885/2000... Step: 885... Loss: 0.25975... Val Loss: 0.25098\n",
      "Epoch: 886/2000... Step: 886... Loss: 0.25971... Val Loss: 0.25092\n",
      "Epoch: 887/2000... Step: 887... Loss: 0.25967... Val Loss: 0.25087\n",
      "Epoch: 888/2000... Step: 888... Loss: 0.25963... Val Loss: 0.25081\n",
      "Epoch: 889/2000... Step: 889... Loss: 0.25959... Val Loss: 0.25076\n",
      "Epoch: 890/2000... Step: 890... Loss: 0.25955... Val Loss: 0.25071\n",
      "Epoch: 891/2000... Step: 891... Loss: 0.25951... Val Loss: 0.25066\n",
      "Epoch: 892/2000... Step: 892... Loss: 0.25947... Val Loss: 0.25060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 893/2000... Step: 893... Loss: 0.25943... Val Loss: 0.25055\n",
      "Epoch: 894/2000... Step: 894... Loss: 0.25939... Val Loss: 0.25050\n",
      "Epoch: 895/2000... Step: 895... Loss: 0.25935... Val Loss: 0.25044\n",
      "Epoch: 896/2000... Step: 896... Loss: 0.25931... Val Loss: 0.25039\n",
      "Epoch: 897/2000... Step: 897... Loss: 0.25927... Val Loss: 0.25034\n",
      "Epoch: 898/2000... Step: 898... Loss: 0.25924... Val Loss: 0.25029\n",
      "Epoch: 899/2000... Step: 899... Loss: 0.25920... Val Loss: 0.25024\n",
      "Epoch: 900/2000... Step: 900... Loss: 0.25916... Val Loss: 0.25018\n",
      "Epoch: 901/2000... Step: 901... Loss: 0.25912... Val Loss: 0.25013\n",
      "Epoch: 902/2000... Step: 902... Loss: 0.25908... Val Loss: 0.25008\n",
      "Epoch: 903/2000... Step: 903... Loss: 0.25905... Val Loss: 0.25003\n",
      "Epoch: 904/2000... Step: 904... Loss: 0.25901... Val Loss: 0.24998\n",
      "Epoch: 905/2000... Step: 905... Loss: 0.25897... Val Loss: 0.24993\n",
      "Epoch: 906/2000... Step: 906... Loss: 0.25893... Val Loss: 0.24988\n",
      "Epoch: 907/2000... Step: 907... Loss: 0.25889... Val Loss: 0.24983\n",
      "Epoch: 908/2000... Step: 908... Loss: 0.25886... Val Loss: 0.24978\n",
      "Epoch: 909/2000... Step: 909... Loss: 0.25882... Val Loss: 0.24973\n",
      "Epoch: 910/2000... Step: 910... Loss: 0.25878... Val Loss: 0.24968\n",
      "Epoch: 911/2000... Step: 911... Loss: 0.25875... Val Loss: 0.24963\n",
      "Epoch: 912/2000... Step: 912... Loss: 0.25871... Val Loss: 0.24958\n",
      "Epoch: 913/2000... Step: 913... Loss: 0.25867... Val Loss: 0.24953\n",
      "Epoch: 914/2000... Step: 914... Loss: 0.25864... Val Loss: 0.24948\n",
      "Epoch: 915/2000... Step: 915... Loss: 0.25860... Val Loss: 0.24943\n",
      "Epoch: 916/2000... Step: 916... Loss: 0.25856... Val Loss: 0.24938\n",
      "Epoch: 917/2000... Step: 917... Loss: 0.25853... Val Loss: 0.24933\n",
      "Epoch: 918/2000... Step: 918... Loss: 0.25849... Val Loss: 0.24928\n",
      "Epoch: 919/2000... Step: 919... Loss: 0.25846... Val Loss: 0.24923\n",
      "Epoch: 920/2000... Step: 920... Loss: 0.25842... Val Loss: 0.24918\n",
      "Epoch: 921/2000... Step: 921... Loss: 0.25838... Val Loss: 0.24913\n",
      "Epoch: 922/2000... Step: 922... Loss: 0.25835... Val Loss: 0.24908\n",
      "Epoch: 923/2000... Step: 923... Loss: 0.25831... Val Loss: 0.24903\n",
      "Epoch: 924/2000... Step: 924... Loss: 0.25828... Val Loss: 0.24899\n",
      "Epoch: 925/2000... Step: 925... Loss: 0.25824... Val Loss: 0.24894\n",
      "Epoch: 926/2000... Step: 926... Loss: 0.25821... Val Loss: 0.24889\n",
      "Epoch: 927/2000... Step: 927... Loss: 0.25817... Val Loss: 0.24884\n",
      "Epoch: 928/2000... Step: 928... Loss: 0.25814... Val Loss: 0.24879\n",
      "Epoch: 929/2000... Step: 929... Loss: 0.25810... Val Loss: 0.24875\n",
      "Epoch: 930/2000... Step: 930... Loss: 0.25807... Val Loss: 0.24870\n",
      "Epoch: 931/2000... Step: 931... Loss: 0.25803... Val Loss: 0.24865\n",
      "Epoch: 932/2000... Step: 932... Loss: 0.25800... Val Loss: 0.24860\n",
      "Epoch: 933/2000... Step: 933... Loss: 0.25796... Val Loss: 0.24856\n",
      "Epoch: 934/2000... Step: 934... Loss: 0.25793... Val Loss: 0.24851\n",
      "Epoch: 935/2000... Step: 935... Loss: 0.25789... Val Loss: 0.24846\n",
      "Epoch: 936/2000... Step: 936... Loss: 0.25786... Val Loss: 0.24842\n",
      "Epoch: 937/2000... Step: 937... Loss: 0.25783... Val Loss: 0.24837\n",
      "Epoch: 938/2000... Step: 938... Loss: 0.25779... Val Loss: 0.24832\n",
      "Epoch: 939/2000... Step: 939... Loss: 0.25776... Val Loss: 0.24828\n",
      "Epoch: 940/2000... Step: 940... Loss: 0.25773... Val Loss: 0.24823\n",
      "Epoch: 941/2000... Step: 941... Loss: 0.25769... Val Loss: 0.24819\n",
      "Epoch: 942/2000... Step: 942... Loss: 0.25766... Val Loss: 0.24814\n",
      "Epoch: 943/2000... Step: 943... Loss: 0.25763... Val Loss: 0.24809\n",
      "Epoch: 944/2000... Step: 944... Loss: 0.25759... Val Loss: 0.24805\n",
      "Epoch: 945/2000... Step: 945... Loss: 0.25756... Val Loss: 0.24800\n",
      "Epoch: 946/2000... Step: 946... Loss: 0.25753... Val Loss: 0.24796\n",
      "Epoch: 947/2000... Step: 947... Loss: 0.25749... Val Loss: 0.24791\n",
      "Epoch: 948/2000... Step: 948... Loss: 0.25746... Val Loss: 0.24787\n",
      "Epoch: 949/2000... Step: 949... Loss: 0.25743... Val Loss: 0.24782\n",
      "Epoch: 950/2000... Step: 950... Loss: 0.25740... Val Loss: 0.24778\n",
      "Epoch: 951/2000... Step: 951... Loss: 0.25736... Val Loss: 0.24773\n",
      "Epoch: 952/2000... Step: 952... Loss: 0.25733... Val Loss: 0.24769\n",
      "Epoch: 953/2000... Step: 953... Loss: 0.25730... Val Loss: 0.24764\n",
      "Epoch: 954/2000... Step: 954... Loss: 0.25727... Val Loss: 0.24760\n",
      "Epoch: 955/2000... Step: 955... Loss: 0.25724... Val Loss: 0.24755\n",
      "Epoch: 956/2000... Step: 956... Loss: 0.25720... Val Loss: 0.24751\n",
      "Epoch: 957/2000... Step: 957... Loss: 0.25717... Val Loss: 0.24746\n",
      "Epoch: 958/2000... Step: 958... Loss: 0.25714... Val Loss: 0.24742\n",
      "Epoch: 959/2000... Step: 959... Loss: 0.25711... Val Loss: 0.24738\n",
      "Epoch: 960/2000... Step: 960... Loss: 0.25708... Val Loss: 0.24733\n",
      "Epoch: 961/2000... Step: 961... Loss: 0.25705... Val Loss: 0.24729\n",
      "Epoch: 962/2000... Step: 962... Loss: 0.25701... Val Loss: 0.24725\n",
      "Epoch: 963/2000... Step: 963... Loss: 0.25698... Val Loss: 0.24720\n",
      "Epoch: 964/2000... Step: 964... Loss: 0.25695... Val Loss: 0.24716\n",
      "Epoch: 965/2000... Step: 965... Loss: 0.25692... Val Loss: 0.24712\n",
      "Epoch: 966/2000... Step: 966... Loss: 0.25689... Val Loss: 0.24707\n",
      "Epoch: 967/2000... Step: 967... Loss: 0.25686... Val Loss: 0.24703\n",
      "Epoch: 968/2000... Step: 968... Loss: 0.25683... Val Loss: 0.24699\n",
      "Epoch: 969/2000... Step: 969... Loss: 0.25680... Val Loss: 0.24695\n",
      "Epoch: 970/2000... Step: 970... Loss: 0.25677... Val Loss: 0.24690\n",
      "Epoch: 971/2000... Step: 971... Loss: 0.25674... Val Loss: 0.24686\n",
      "Epoch: 972/2000... Step: 972... Loss: 0.25671... Val Loss: 0.24682\n",
      "Epoch: 973/2000... Step: 973... Loss: 0.25668... Val Loss: 0.24678\n",
      "Epoch: 974/2000... Step: 974... Loss: 0.25665... Val Loss: 0.24673\n",
      "Epoch: 975/2000... Step: 975... Loss: 0.25662... Val Loss: 0.24669\n",
      "Epoch: 976/2000... Step: 976... Loss: 0.25659... Val Loss: 0.24665\n",
      "Epoch: 977/2000... Step: 977... Loss: 0.25656... Val Loss: 0.24661\n",
      "Epoch: 978/2000... Step: 978... Loss: 0.25653... Val Loss: 0.24657\n",
      "Epoch: 979/2000... Step: 979... Loss: 0.25650... Val Loss: 0.24653\n",
      "Epoch: 980/2000... Step: 980... Loss: 0.25647... Val Loss: 0.24648\n",
      "Epoch: 981/2000... Step: 981... Loss: 0.25644... Val Loss: 0.24644\n",
      "Epoch: 982/2000... Step: 982... Loss: 0.25641... Val Loss: 0.24640\n",
      "Epoch: 983/2000... Step: 983... Loss: 0.25638... Val Loss: 0.24636\n",
      "Epoch: 984/2000... Step: 984... Loss: 0.25635... Val Loss: 0.24632\n",
      "Epoch: 985/2000... Step: 985... Loss: 0.25632... Val Loss: 0.24628\n",
      "Epoch: 986/2000... Step: 986... Loss: 0.25629... Val Loss: 0.24624\n",
      "Epoch: 987/2000... Step: 987... Loss: 0.25627... Val Loss: 0.24620\n",
      "Epoch: 988/2000... Step: 988... Loss: 0.25624... Val Loss: 0.24616\n",
      "Epoch: 989/2000... Step: 989... Loss: 0.25621... Val Loss: 0.24612\n",
      "Epoch: 990/2000... Step: 990... Loss: 0.25618... Val Loss: 0.24608\n",
      "Epoch: 991/2000... Step: 991... Loss: 0.25615... Val Loss: 0.24604\n",
      "Epoch: 992/2000... Step: 992... Loss: 0.25612... Val Loss: 0.24600\n",
      "Epoch: 993/2000... Step: 993... Loss: 0.25610... Val Loss: 0.24596\n",
      "Epoch: 994/2000... Step: 994... Loss: 0.25607... Val Loss: 0.24592\n",
      "Epoch: 995/2000... Step: 995... Loss: 0.25604... Val Loss: 0.24588\n",
      "Epoch: 996/2000... Step: 996... Loss: 0.25601... Val Loss: 0.24584\n",
      "Epoch: 997/2000... Step: 997... Loss: 0.25598... Val Loss: 0.24580\n",
      "Epoch: 998/2000... Step: 998... Loss: 0.25596... Val Loss: 0.24576\n",
      "Epoch: 999/2000... Step: 999... Loss: 0.25593... Val Loss: 0.24572\n",
      "Epoch: 1000/2000... Step: 1000... Loss: 0.25590... Val Loss: 0.24568\n",
      "Epoch: 1001/2000... Step: 1001... Loss: 0.25587... Val Loss: 0.24564\n",
      "Epoch: 1002/2000... Step: 1002... Loss: 0.25585... Val Loss: 0.24560\n",
      "Epoch: 1003/2000... Step: 1003... Loss: 0.25582... Val Loss: 0.24556\n",
      "Epoch: 1004/2000... Step: 1004... Loss: 0.25579... Val Loss: 0.24552\n",
      "Epoch: 1005/2000... Step: 1005... Loss: 0.25576... Val Loss: 0.24549\n",
      "Epoch: 1006/2000... Step: 1006... Loss: 0.25574... Val Loss: 0.24545\n",
      "Epoch: 1007/2000... Step: 1007... Loss: 0.25571... Val Loss: 0.24541\n",
      "Epoch: 1008/2000... Step: 1008... Loss: 0.25568... Val Loss: 0.24537\n",
      "Epoch: 1009/2000... Step: 1009... Loss: 0.25566... Val Loss: 0.24533\n",
      "Epoch: 1010/2000... Step: 1010... Loss: 0.25563... Val Loss: 0.24529\n",
      "Epoch: 1011/2000... Step: 1011... Loss: 0.25560... Val Loss: 0.24526\n",
      "Epoch: 1012/2000... Step: 1012... Loss: 0.25558... Val Loss: 0.24522\n",
      "Epoch: 1013/2000... Step: 1013... Loss: 0.25555... Val Loss: 0.24518\n",
      "Epoch: 1014/2000... Step: 1014... Loss: 0.25552... Val Loss: 0.24514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1015/2000... Step: 1015... Loss: 0.25550... Val Loss: 0.24510\n",
      "Epoch: 1016/2000... Step: 1016... Loss: 0.25547... Val Loss: 0.24507\n",
      "Epoch: 1017/2000... Step: 1017... Loss: 0.25544... Val Loss: 0.24503\n",
      "Epoch: 1018/2000... Step: 1018... Loss: 0.25542... Val Loss: 0.24499\n",
      "Epoch: 1019/2000... Step: 1019... Loss: 0.25539... Val Loss: 0.24496\n",
      "Epoch: 1020/2000... Step: 1020... Loss: 0.25537... Val Loss: 0.24492\n",
      "Epoch: 1021/2000... Step: 1021... Loss: 0.25534... Val Loss: 0.24488\n",
      "Epoch: 1022/2000... Step: 1022... Loss: 0.25532... Val Loss: 0.24484\n",
      "Epoch: 1023/2000... Step: 1023... Loss: 0.25529... Val Loss: 0.24481\n",
      "Epoch: 1024/2000... Step: 1024... Loss: 0.25526... Val Loss: 0.24477\n",
      "Epoch: 1025/2000... Step: 1025... Loss: 0.25524... Val Loss: 0.24473\n",
      "Epoch: 1026/2000... Step: 1026... Loss: 0.25521... Val Loss: 0.24470\n",
      "Epoch: 1027/2000... Step: 1027... Loss: 0.25519... Val Loss: 0.24466\n",
      "Epoch: 1028/2000... Step: 1028... Loss: 0.25516... Val Loss: 0.24463\n",
      "Epoch: 1029/2000... Step: 1029... Loss: 0.25514... Val Loss: 0.24459\n",
      "Epoch: 1030/2000... Step: 1030... Loss: 0.25511... Val Loss: 0.24455\n",
      "Epoch: 1031/2000... Step: 1031... Loss: 0.25509... Val Loss: 0.24452\n",
      "Epoch: 1032/2000... Step: 1032... Loss: 0.25506... Val Loss: 0.24448\n",
      "Epoch: 1033/2000... Step: 1033... Loss: 0.25504... Val Loss: 0.24445\n",
      "Epoch: 1034/2000... Step: 1034... Loss: 0.25501... Val Loss: 0.24441\n",
      "Epoch: 1035/2000... Step: 1035... Loss: 0.25499... Val Loss: 0.24437\n",
      "Epoch: 1036/2000... Step: 1036... Loss: 0.25496... Val Loss: 0.24434\n",
      "Epoch: 1037/2000... Step: 1037... Loss: 0.25494... Val Loss: 0.24430\n",
      "Epoch: 1038/2000... Step: 1038... Loss: 0.25492... Val Loss: 0.24427\n",
      "Epoch: 1039/2000... Step: 1039... Loss: 0.25489... Val Loss: 0.24423\n",
      "Epoch: 1040/2000... Step: 1040... Loss: 0.25487... Val Loss: 0.24420\n",
      "Epoch: 1041/2000... Step: 1041... Loss: 0.25484... Val Loss: 0.24416\n",
      "Epoch: 1042/2000... Step: 1042... Loss: 0.25482... Val Loss: 0.24413\n",
      "Epoch: 1043/2000... Step: 1043... Loss: 0.25480... Val Loss: 0.24409\n",
      "Epoch: 1044/2000... Step: 1044... Loss: 0.25477... Val Loss: 0.24406\n",
      "Epoch: 1045/2000... Step: 1045... Loss: 0.25475... Val Loss: 0.24402\n",
      "Epoch: 1046/2000... Step: 1046... Loss: 0.25472... Val Loss: 0.24399\n",
      "Epoch: 1047/2000... Step: 1047... Loss: 0.25470... Val Loss: 0.24395\n",
      "Epoch: 1048/2000... Step: 1048... Loss: 0.25468... Val Loss: 0.24392\n",
      "Epoch: 1049/2000... Step: 1049... Loss: 0.25465... Val Loss: 0.24388\n",
      "Epoch: 1050/2000... Step: 1050... Loss: 0.25463... Val Loss: 0.24385\n",
      "Epoch: 1051/2000... Step: 1051... Loss: 0.25461... Val Loss: 0.24382\n",
      "Epoch: 1052/2000... Step: 1052... Loss: 0.25458... Val Loss: 0.24378\n",
      "Epoch: 1053/2000... Step: 1053... Loss: 0.25456... Val Loss: 0.24375\n",
      "Epoch: 1054/2000... Step: 1054... Loss: 0.25454... Val Loss: 0.24371\n",
      "Epoch: 1055/2000... Step: 1055... Loss: 0.25451... Val Loss: 0.24368\n",
      "Epoch: 1056/2000... Step: 1056... Loss: 0.25449... Val Loss: 0.24365\n",
      "Epoch: 1057/2000... Step: 1057... Loss: 0.25447... Val Loss: 0.24361\n",
      "Epoch: 1058/2000... Step: 1058... Loss: 0.25444... Val Loss: 0.24358\n",
      "Epoch: 1059/2000... Step: 1059... Loss: 0.25442... Val Loss: 0.24355\n",
      "Epoch: 1060/2000... Step: 1060... Loss: 0.25440... Val Loss: 0.24351\n",
      "Epoch: 1061/2000... Step: 1061... Loss: 0.25438... Val Loss: 0.24348\n",
      "Epoch: 1062/2000... Step: 1062... Loss: 0.25435... Val Loss: 0.24345\n",
      "Epoch: 1063/2000... Step: 1063... Loss: 0.25433... Val Loss: 0.24341\n",
      "Epoch: 1064/2000... Step: 1064... Loss: 0.25431... Val Loss: 0.24338\n",
      "Epoch: 1065/2000... Step: 1065... Loss: 0.25429... Val Loss: 0.24335\n",
      "Epoch: 1066/2000... Step: 1066... Loss: 0.25426... Val Loss: 0.24332\n",
      "Epoch: 1067/2000... Step: 1067... Loss: 0.25424... Val Loss: 0.24328\n",
      "Epoch: 1068/2000... Step: 1068... Loss: 0.25422... Val Loss: 0.24325\n",
      "Epoch: 1069/2000... Step: 1069... Loss: 0.25420... Val Loss: 0.24322\n",
      "Epoch: 1070/2000... Step: 1070... Loss: 0.25418... Val Loss: 0.24318\n",
      "Epoch: 1071/2000... Step: 1071... Loss: 0.25415... Val Loss: 0.24315\n",
      "Epoch: 1072/2000... Step: 1072... Loss: 0.25413... Val Loss: 0.24312\n",
      "Epoch: 1073/2000... Step: 1073... Loss: 0.25411... Val Loss: 0.24309\n",
      "Epoch: 1074/2000... Step: 1074... Loss: 0.25409... Val Loss: 0.24306\n",
      "Epoch: 1075/2000... Step: 1075... Loss: 0.25407... Val Loss: 0.24302\n",
      "Epoch: 1076/2000... Step: 1076... Loss: 0.25405... Val Loss: 0.24299\n",
      "Epoch: 1077/2000... Step: 1077... Loss: 0.25402... Val Loss: 0.24296\n",
      "Epoch: 1078/2000... Step: 1078... Loss: 0.25400... Val Loss: 0.24293\n",
      "Epoch: 1079/2000... Step: 1079... Loss: 0.25398... Val Loss: 0.24290\n",
      "Epoch: 1080/2000... Step: 1080... Loss: 0.25396... Val Loss: 0.24287\n",
      "Epoch: 1081/2000... Step: 1081... Loss: 0.25394... Val Loss: 0.24283\n",
      "Epoch: 1082/2000... Step: 1082... Loss: 0.25392... Val Loss: 0.24280\n",
      "Epoch: 1083/2000... Step: 1083... Loss: 0.25390... Val Loss: 0.24277\n",
      "Epoch: 1084/2000... Step: 1084... Loss: 0.25388... Val Loss: 0.24274\n",
      "Epoch: 1085/2000... Step: 1085... Loss: 0.25385... Val Loss: 0.24271\n",
      "Epoch: 1086/2000... Step: 1086... Loss: 0.25383... Val Loss: 0.24268\n",
      "Epoch: 1087/2000... Step: 1087... Loss: 0.25381... Val Loss: 0.24265\n",
      "Epoch: 1088/2000... Step: 1088... Loss: 0.25379... Val Loss: 0.24262\n",
      "Epoch: 1089/2000... Step: 1089... Loss: 0.25377... Val Loss: 0.24258\n",
      "Epoch: 1090/2000... Step: 1090... Loss: 0.25375... Val Loss: 0.24255\n",
      "Epoch: 1091/2000... Step: 1091... Loss: 0.25373... Val Loss: 0.24252\n",
      "Epoch: 1092/2000... Step: 1092... Loss: 0.25371... Val Loss: 0.24249\n",
      "Epoch: 1093/2000... Step: 1093... Loss: 0.25369... Val Loss: 0.24246\n",
      "Epoch: 1094/2000... Step: 1094... Loss: 0.25367... Val Loss: 0.24243\n",
      "Epoch: 1095/2000... Step: 1095... Loss: 0.25365... Val Loss: 0.24240\n",
      "Epoch: 1096/2000... Step: 1096... Loss: 0.25363... Val Loss: 0.24237\n",
      "Epoch: 1097/2000... Step: 1097... Loss: 0.25361... Val Loss: 0.24234\n",
      "Epoch: 1098/2000... Step: 1098... Loss: 0.25359... Val Loss: 0.24231\n",
      "Epoch: 1099/2000... Step: 1099... Loss: 0.25357... Val Loss: 0.24228\n",
      "Epoch: 1100/2000... Step: 1100... Loss: 0.25355... Val Loss: 0.24225\n",
      "Epoch: 1101/2000... Step: 1101... Loss: 0.25353... Val Loss: 0.24222\n",
      "Epoch: 1102/2000... Step: 1102... Loss: 0.25351... Val Loss: 0.24219\n",
      "Epoch: 1103/2000... Step: 1103... Loss: 0.25349... Val Loss: 0.24216\n",
      "Epoch: 1104/2000... Step: 1104... Loss: 0.25347... Val Loss: 0.24213\n",
      "Epoch: 1105/2000... Step: 1105... Loss: 0.25345... Val Loss: 0.24210\n",
      "Epoch: 1106/2000... Step: 1106... Loss: 0.25343... Val Loss: 0.24207\n",
      "Epoch: 1107/2000... Step: 1107... Loss: 0.25341... Val Loss: 0.24204\n",
      "Epoch: 1108/2000... Step: 1108... Loss: 0.25339... Val Loss: 0.24201\n",
      "Epoch: 1109/2000... Step: 1109... Loss: 0.25337... Val Loss: 0.24198\n",
      "Epoch: 1110/2000... Step: 1110... Loss: 0.25335... Val Loss: 0.24196\n",
      "Epoch: 1111/2000... Step: 1111... Loss: 0.25333... Val Loss: 0.24193\n",
      "Epoch: 1112/2000... Step: 1112... Loss: 0.25331... Val Loss: 0.24190\n",
      "Epoch: 1113/2000... Step: 1113... Loss: 0.25330... Val Loss: 0.24187\n",
      "Epoch: 1114/2000... Step: 1114... Loss: 0.25328... Val Loss: 0.24184\n",
      "Epoch: 1115/2000... Step: 1115... Loss: 0.25326... Val Loss: 0.24181\n",
      "Epoch: 1116/2000... Step: 1116... Loss: 0.25324... Val Loss: 0.24178\n",
      "Epoch: 1117/2000... Step: 1117... Loss: 0.25322... Val Loss: 0.24175\n",
      "Epoch: 1118/2000... Step: 1118... Loss: 0.25320... Val Loss: 0.24172\n",
      "Epoch: 1119/2000... Step: 1119... Loss: 0.25318... Val Loss: 0.24170\n",
      "Epoch: 1120/2000... Step: 1120... Loss: 0.25316... Val Loss: 0.24167\n",
      "Epoch: 1121/2000... Step: 1121... Loss: 0.25314... Val Loss: 0.24164\n",
      "Epoch: 1122/2000... Step: 1122... Loss: 0.25313... Val Loss: 0.24161\n",
      "Epoch: 1123/2000... Step: 1123... Loss: 0.25311... Val Loss: 0.24158\n",
      "Epoch: 1124/2000... Step: 1124... Loss: 0.25309... Val Loss: 0.24155\n",
      "Epoch: 1125/2000... Step: 1125... Loss: 0.25307... Val Loss: 0.24153\n",
      "Epoch: 1126/2000... Step: 1126... Loss: 0.25305... Val Loss: 0.24150\n",
      "Epoch: 1127/2000... Step: 1127... Loss: 0.25303... Val Loss: 0.24147\n",
      "Epoch: 1128/2000... Step: 1128... Loss: 0.25302... Val Loss: 0.24144\n",
      "Epoch: 1129/2000... Step: 1129... Loss: 0.25300... Val Loss: 0.24141\n",
      "Epoch: 1130/2000... Step: 1130... Loss: 0.25298... Val Loss: 0.24139\n",
      "Epoch: 1131/2000... Step: 1131... Loss: 0.25296... Val Loss: 0.24136\n",
      "Epoch: 1132/2000... Step: 1132... Loss: 0.25294... Val Loss: 0.24133\n",
      "Epoch: 1133/2000... Step: 1133... Loss: 0.25293... Val Loss: 0.24130\n",
      "Epoch: 1134/2000... Step: 1134... Loss: 0.25291... Val Loss: 0.24128\n",
      "Epoch: 1135/2000... Step: 1135... Loss: 0.25289... Val Loss: 0.24125\n",
      "Epoch: 1136/2000... Step: 1136... Loss: 0.25287... Val Loss: 0.24122\n",
      "Epoch: 1137/2000... Step: 1137... Loss: 0.25285... Val Loss: 0.24120\n",
      "Epoch: 1138/2000... Step: 1138... Loss: 0.25284... Val Loss: 0.24117\n",
      "Epoch: 1139/2000... Step: 1139... Loss: 0.25282... Val Loss: 0.24114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1140/2000... Step: 1140... Loss: 0.25280... Val Loss: 0.24111\n",
      "Epoch: 1141/2000... Step: 1141... Loss: 0.25278... Val Loss: 0.24109\n",
      "Epoch: 1142/2000... Step: 1142... Loss: 0.25277... Val Loss: 0.24106\n",
      "Epoch: 1143/2000... Step: 1143... Loss: 0.25275... Val Loss: 0.24103\n",
      "Epoch: 1144/2000... Step: 1144... Loss: 0.25273... Val Loss: 0.24101\n",
      "Epoch: 1145/2000... Step: 1145... Loss: 0.25272... Val Loss: 0.24098\n",
      "Epoch: 1146/2000... Step: 1146... Loss: 0.25270... Val Loss: 0.24095\n",
      "Epoch: 1147/2000... Step: 1147... Loss: 0.25268... Val Loss: 0.24093\n",
      "Epoch: 1148/2000... Step: 1148... Loss: 0.25266... Val Loss: 0.24090\n",
      "Epoch: 1149/2000... Step: 1149... Loss: 0.25265... Val Loss: 0.24087\n",
      "Epoch: 1150/2000... Step: 1150... Loss: 0.25263... Val Loss: 0.24085\n",
      "Epoch: 1151/2000... Step: 1151... Loss: 0.25261... Val Loss: 0.24082\n",
      "Epoch: 1152/2000... Step: 1152... Loss: 0.25260... Val Loss: 0.24080\n",
      "Epoch: 1153/2000... Step: 1153... Loss: 0.25258... Val Loss: 0.24077\n",
      "Epoch: 1154/2000... Step: 1154... Loss: 0.25256... Val Loss: 0.24074\n",
      "Epoch: 1155/2000... Step: 1155... Loss: 0.25255... Val Loss: 0.24072\n",
      "Epoch: 1156/2000... Step: 1156... Loss: 0.25253... Val Loss: 0.24069\n",
      "Epoch: 1157/2000... Step: 1157... Loss: 0.25251... Val Loss: 0.24067\n",
      "Epoch: 1158/2000... Step: 1158... Loss: 0.25250... Val Loss: 0.24064\n",
      "Epoch: 1159/2000... Step: 1159... Loss: 0.25248... Val Loss: 0.24061\n",
      "Epoch: 1160/2000... Step: 1160... Loss: 0.25246... Val Loss: 0.24059\n",
      "Epoch: 1161/2000... Step: 1161... Loss: 0.25245... Val Loss: 0.24056\n",
      "Epoch: 1162/2000... Step: 1162... Loss: 0.25243... Val Loss: 0.24054\n",
      "Epoch: 1163/2000... Step: 1163... Loss: 0.25241... Val Loss: 0.24051\n",
      "Epoch: 1164/2000... Step: 1164... Loss: 0.25240... Val Loss: 0.24049\n",
      "Epoch: 1165/2000... Step: 1165... Loss: 0.25238... Val Loss: 0.24046\n",
      "Epoch: 1166/2000... Step: 1166... Loss: 0.25237... Val Loss: 0.24044\n",
      "Epoch: 1167/2000... Step: 1167... Loss: 0.25235... Val Loss: 0.24041\n",
      "Epoch: 1168/2000... Step: 1168... Loss: 0.25233... Val Loss: 0.24039\n",
      "Epoch: 1169/2000... Step: 1169... Loss: 0.25232... Val Loss: 0.24036\n",
      "Epoch: 1170/2000... Step: 1170... Loss: 0.25230... Val Loss: 0.24034\n",
      "Epoch: 1171/2000... Step: 1171... Loss: 0.25229... Val Loss: 0.24031\n",
      "Epoch: 1172/2000... Step: 1172... Loss: 0.25227... Val Loss: 0.24029\n",
      "Epoch: 1173/2000... Step: 1173... Loss: 0.25225... Val Loss: 0.24026\n",
      "Epoch: 1174/2000... Step: 1174... Loss: 0.25224... Val Loss: 0.24024\n",
      "Epoch: 1175/2000... Step: 1175... Loss: 0.25222... Val Loss: 0.24021\n",
      "Epoch: 1176/2000... Step: 1176... Loss: 0.25221... Val Loss: 0.24019\n",
      "Epoch: 1177/2000... Step: 1177... Loss: 0.25219... Val Loss: 0.24016\n",
      "Epoch: 1178/2000... Step: 1178... Loss: 0.25218... Val Loss: 0.24014\n",
      "Epoch: 1179/2000... Step: 1179... Loss: 0.25216... Val Loss: 0.24011\n",
      "Epoch: 1180/2000... Step: 1180... Loss: 0.25215... Val Loss: 0.24009\n",
      "Epoch: 1181/2000... Step: 1181... Loss: 0.25213... Val Loss: 0.24007\n",
      "Epoch: 1182/2000... Step: 1182... Loss: 0.25211... Val Loss: 0.24004\n",
      "Epoch: 1183/2000... Step: 1183... Loss: 0.25210... Val Loss: 0.24002\n",
      "Epoch: 1184/2000... Step: 1184... Loss: 0.25208... Val Loss: 0.23999\n",
      "Epoch: 1185/2000... Step: 1185... Loss: 0.25207... Val Loss: 0.23997\n",
      "Epoch: 1186/2000... Step: 1186... Loss: 0.25205... Val Loss: 0.23995\n",
      "Epoch: 1187/2000... Step: 1187... Loss: 0.25204... Val Loss: 0.23992\n",
      "Epoch: 1188/2000... Step: 1188... Loss: 0.25202... Val Loss: 0.23990\n",
      "Epoch: 1189/2000... Step: 1189... Loss: 0.25201... Val Loss: 0.23987\n",
      "Epoch: 1190/2000... Step: 1190... Loss: 0.25199... Val Loss: 0.23985\n",
      "Epoch: 1191/2000... Step: 1191... Loss: 0.25198... Val Loss: 0.23983\n",
      "Epoch: 1192/2000... Step: 1192... Loss: 0.25196... Val Loss: 0.23980\n",
      "Epoch: 1193/2000... Step: 1193... Loss: 0.25195... Val Loss: 0.23978\n",
      "Epoch: 1194/2000... Step: 1194... Loss: 0.25194... Val Loss: 0.23976\n",
      "Epoch: 1195/2000... Step: 1195... Loss: 0.25192... Val Loss: 0.23973\n",
      "Epoch: 1196/2000... Step: 1196... Loss: 0.25191... Val Loss: 0.23971\n",
      "Epoch: 1197/2000... Step: 1197... Loss: 0.25189... Val Loss: 0.23969\n",
      "Epoch: 1198/2000... Step: 1198... Loss: 0.25188... Val Loss: 0.23966\n",
      "Epoch: 1199/2000... Step: 1199... Loss: 0.25186... Val Loss: 0.23964\n",
      "Epoch: 1200/2000... Step: 1200... Loss: 0.25185... Val Loss: 0.23962\n",
      "Epoch: 1201/2000... Step: 1201... Loss: 0.25183... Val Loss: 0.23959\n",
      "Epoch: 1202/2000... Step: 1202... Loss: 0.25182... Val Loss: 0.23957\n",
      "Epoch: 1203/2000... Step: 1203... Loss: 0.25180... Val Loss: 0.23955\n",
      "Epoch: 1204/2000... Step: 1204... Loss: 0.25179... Val Loss: 0.23952\n",
      "Epoch: 1205/2000... Step: 1205... Loss: 0.25178... Val Loss: 0.23950\n",
      "Epoch: 1206/2000... Step: 1206... Loss: 0.25176... Val Loss: 0.23948\n",
      "Epoch: 1207/2000... Step: 1207... Loss: 0.25175... Val Loss: 0.23946\n",
      "Epoch: 1208/2000... Step: 1208... Loss: 0.25173... Val Loss: 0.23943\n",
      "Epoch: 1209/2000... Step: 1209... Loss: 0.25172... Val Loss: 0.23941\n",
      "Epoch: 1210/2000... Step: 1210... Loss: 0.25171... Val Loss: 0.23939\n",
      "Epoch: 1211/2000... Step: 1211... Loss: 0.25169... Val Loss: 0.23937\n",
      "Epoch: 1212/2000... Step: 1212... Loss: 0.25168... Val Loss: 0.23934\n",
      "Epoch: 1213/2000... Step: 1213... Loss: 0.25166... Val Loss: 0.23932\n",
      "Epoch: 1214/2000... Step: 1214... Loss: 0.25165... Val Loss: 0.23930\n",
      "Epoch: 1215/2000... Step: 1215... Loss: 0.25164... Val Loss: 0.23928\n",
      "Epoch: 1216/2000... Step: 1216... Loss: 0.25162... Val Loss: 0.23925\n",
      "Epoch: 1217/2000... Step: 1217... Loss: 0.25161... Val Loss: 0.23923\n",
      "Epoch: 1218/2000... Step: 1218... Loss: 0.25160... Val Loss: 0.23921\n",
      "Epoch: 1219/2000... Step: 1219... Loss: 0.25158... Val Loss: 0.23919\n",
      "Epoch: 1220/2000... Step: 1220... Loss: 0.25157... Val Loss: 0.23917\n",
      "Epoch: 1221/2000... Step: 1221... Loss: 0.25156... Val Loss: 0.23914\n",
      "Epoch: 1222/2000... Step: 1222... Loss: 0.25154... Val Loss: 0.23912\n",
      "Epoch: 1223/2000... Step: 1223... Loss: 0.25153... Val Loss: 0.23910\n",
      "Epoch: 1224/2000... Step: 1224... Loss: 0.25152... Val Loss: 0.23908\n",
      "Epoch: 1225/2000... Step: 1225... Loss: 0.25150... Val Loss: 0.23906\n",
      "Epoch: 1226/2000... Step: 1226... Loss: 0.25149... Val Loss: 0.23904\n",
      "Epoch: 1227/2000... Step: 1227... Loss: 0.25148... Val Loss: 0.23901\n",
      "Epoch: 1228/2000... Step: 1228... Loss: 0.25146... Val Loss: 0.23899\n",
      "Epoch: 1229/2000... Step: 1229... Loss: 0.25145... Val Loss: 0.23897\n",
      "Epoch: 1230/2000... Step: 1230... Loss: 0.25144... Val Loss: 0.23895\n",
      "Epoch: 1231/2000... Step: 1231... Loss: 0.25142... Val Loss: 0.23893\n",
      "Epoch: 1232/2000... Step: 1232... Loss: 0.25141... Val Loss: 0.23891\n",
      "Epoch: 1233/2000... Step: 1233... Loss: 0.25140... Val Loss: 0.23889\n",
      "Epoch: 1234/2000... Step: 1234... Loss: 0.25138... Val Loss: 0.23886\n",
      "Epoch: 1235/2000... Step: 1235... Loss: 0.25137... Val Loss: 0.23884\n",
      "Epoch: 1236/2000... Step: 1236... Loss: 0.25136... Val Loss: 0.23882\n",
      "Epoch: 1237/2000... Step: 1237... Loss: 0.25135... Val Loss: 0.23880\n",
      "Epoch: 1238/2000... Step: 1238... Loss: 0.25133... Val Loss: 0.23878\n",
      "Epoch: 1239/2000... Step: 1239... Loss: 0.25132... Val Loss: 0.23876\n",
      "Epoch: 1240/2000... Step: 1240... Loss: 0.25131... Val Loss: 0.23874\n",
      "Epoch: 1241/2000... Step: 1241... Loss: 0.25129... Val Loss: 0.23872\n",
      "Epoch: 1242/2000... Step: 1242... Loss: 0.25128... Val Loss: 0.23870\n",
      "Epoch: 1243/2000... Step: 1243... Loss: 0.25127... Val Loss: 0.23868\n",
      "Epoch: 1244/2000... Step: 1244... Loss: 0.25126... Val Loss: 0.23865\n",
      "Epoch: 1245/2000... Step: 1245... Loss: 0.25124... Val Loss: 0.23863\n",
      "Epoch: 1246/2000... Step: 1246... Loss: 0.25123... Val Loss: 0.23861\n",
      "Epoch: 1247/2000... Step: 1247... Loss: 0.25122... Val Loss: 0.23859\n",
      "Epoch: 1248/2000... Step: 1248... Loss: 0.25121... Val Loss: 0.23857\n",
      "Epoch: 1249/2000... Step: 1249... Loss: 0.25120... Val Loss: 0.23855\n",
      "Epoch: 1250/2000... Step: 1250... Loss: 0.25118... Val Loss: 0.23853\n",
      "Epoch: 1251/2000... Step: 1251... Loss: 0.25117... Val Loss: 0.23851\n",
      "Epoch: 1252/2000... Step: 1252... Loss: 0.25116... Val Loss: 0.23849\n",
      "Epoch: 1253/2000... Step: 1253... Loss: 0.25115... Val Loss: 0.23847\n",
      "Epoch: 1254/2000... Step: 1254... Loss: 0.25113... Val Loss: 0.23845\n",
      "Epoch: 1255/2000... Step: 1255... Loss: 0.25112... Val Loss: 0.23843\n",
      "Epoch: 1256/2000... Step: 1256... Loss: 0.25111... Val Loss: 0.23841\n",
      "Epoch: 1257/2000... Step: 1257... Loss: 0.25110... Val Loss: 0.23839\n",
      "Epoch: 1258/2000... Step: 1258... Loss: 0.25109... Val Loss: 0.23837\n",
      "Epoch: 1259/2000... Step: 1259... Loss: 0.25107... Val Loss: 0.23835\n",
      "Epoch: 1260/2000... Step: 1260... Loss: 0.25106... Val Loss: 0.23833\n",
      "Epoch: 1261/2000... Step: 1261... Loss: 0.25105... Val Loss: 0.23831\n",
      "Epoch: 1262/2000... Step: 1262... Loss: 0.25104... Val Loss: 0.23829\n",
      "Epoch: 1263/2000... Step: 1263... Loss: 0.25103... Val Loss: 0.23827\n",
      "Epoch: 1264/2000... Step: 1264... Loss: 0.25102... Val Loss: 0.23825\n",
      "Epoch: 1265/2000... Step: 1265... Loss: 0.25100... Val Loss: 0.23823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1266/2000... Step: 1266... Loss: 0.25099... Val Loss: 0.23821\n",
      "Epoch: 1267/2000... Step: 1267... Loss: 0.25098... Val Loss: 0.23819\n",
      "Epoch: 1268/2000... Step: 1268... Loss: 0.25097... Val Loss: 0.23817\n",
      "Epoch: 1269/2000... Step: 1269... Loss: 0.25096... Val Loss: 0.23815\n",
      "Epoch: 1270/2000... Step: 1270... Loss: 0.25095... Val Loss: 0.23814\n",
      "Epoch: 1271/2000... Step: 1271... Loss: 0.25093... Val Loss: 0.23812\n",
      "Epoch: 1272/2000... Step: 1272... Loss: 0.25092... Val Loss: 0.23810\n",
      "Epoch: 1273/2000... Step: 1273... Loss: 0.25091... Val Loss: 0.23808\n",
      "Epoch: 1274/2000... Step: 1274... Loss: 0.25090... Val Loss: 0.23806\n",
      "Epoch: 1275/2000... Step: 1275... Loss: 0.25089... Val Loss: 0.23804\n",
      "Epoch: 1276/2000... Step: 1276... Loss: 0.25088... Val Loss: 0.23802\n",
      "Epoch: 1277/2000... Step: 1277... Loss: 0.25087... Val Loss: 0.23800\n",
      "Epoch: 1278/2000... Step: 1278... Loss: 0.25086... Val Loss: 0.23798\n",
      "Epoch: 1279/2000... Step: 1279... Loss: 0.25084... Val Loss: 0.23796\n",
      "Epoch: 1280/2000... Step: 1280... Loss: 0.25083... Val Loss: 0.23794\n",
      "Epoch: 1281/2000... Step: 1281... Loss: 0.25082... Val Loss: 0.23793\n",
      "Epoch: 1282/2000... Step: 1282... Loss: 0.25081... Val Loss: 0.23791\n",
      "Epoch: 1283/2000... Step: 1283... Loss: 0.25080... Val Loss: 0.23789\n",
      "Epoch: 1284/2000... Step: 1284... Loss: 0.25079... Val Loss: 0.23787\n",
      "Epoch: 1285/2000... Step: 1285... Loss: 0.25078... Val Loss: 0.23785\n",
      "Epoch: 1286/2000... Step: 1286... Loss: 0.25077... Val Loss: 0.23783\n",
      "Epoch: 1287/2000... Step: 1287... Loss: 0.25076... Val Loss: 0.23781\n",
      "Epoch: 1288/2000... Step: 1288... Loss: 0.25074... Val Loss: 0.23780\n",
      "Epoch: 1289/2000... Step: 1289... Loss: 0.25073... Val Loss: 0.23778\n",
      "Epoch: 1290/2000... Step: 1290... Loss: 0.25072... Val Loss: 0.23776\n",
      "Epoch: 1291/2000... Step: 1291... Loss: 0.25071... Val Loss: 0.23774\n",
      "Epoch: 1292/2000... Step: 1292... Loss: 0.25070... Val Loss: 0.23772\n",
      "Epoch: 1293/2000... Step: 1293... Loss: 0.25069... Val Loss: 0.23770\n",
      "Epoch: 1294/2000... Step: 1294... Loss: 0.25068... Val Loss: 0.23769\n",
      "Epoch: 1295/2000... Step: 1295... Loss: 0.25067... Val Loss: 0.23767\n",
      "Epoch: 1296/2000... Step: 1296... Loss: 0.25066... Val Loss: 0.23765\n",
      "Epoch: 1297/2000... Step: 1297... Loss: 0.25065... Val Loss: 0.23763\n",
      "Epoch: 1298/2000... Step: 1298... Loss: 0.25064... Val Loss: 0.23761\n",
      "Epoch: 1299/2000... Step: 1299... Loss: 0.25063... Val Loss: 0.23759\n",
      "Epoch: 1300/2000... Step: 1300... Loss: 0.25062... Val Loss: 0.23758\n",
      "Epoch: 1301/2000... Step: 1301... Loss: 0.25061... Val Loss: 0.23756\n",
      "Epoch: 1302/2000... Step: 1302... Loss: 0.25060... Val Loss: 0.23754\n",
      "Epoch: 1303/2000... Step: 1303... Loss: 0.25059... Val Loss: 0.23752\n",
      "Epoch: 1304/2000... Step: 1304... Loss: 0.25058... Val Loss: 0.23751\n",
      "Epoch: 1305/2000... Step: 1305... Loss: 0.25057... Val Loss: 0.23749\n",
      "Epoch: 1306/2000... Step: 1306... Loss: 0.25056... Val Loss: 0.23747\n",
      "Epoch: 1307/2000... Step: 1307... Loss: 0.25055... Val Loss: 0.23745\n",
      "Epoch: 1308/2000... Step: 1308... Loss: 0.25054... Val Loss: 0.23743\n",
      "Epoch: 1309/2000... Step: 1309... Loss: 0.25053... Val Loss: 0.23742\n",
      "Epoch: 1310/2000... Step: 1310... Loss: 0.25052... Val Loss: 0.23740\n",
      "Epoch: 1311/2000... Step: 1311... Loss: 0.25050... Val Loss: 0.23738\n",
      "Epoch: 1312/2000... Step: 1312... Loss: 0.25050... Val Loss: 0.23736\n",
      "Epoch: 1313/2000... Step: 1313... Loss: 0.25049... Val Loss: 0.23735\n",
      "Epoch: 1314/2000... Step: 1314... Loss: 0.25048... Val Loss: 0.23733\n",
      "Epoch: 1315/2000... Step: 1315... Loss: 0.25047... Val Loss: 0.23731\n",
      "Epoch: 1316/2000... Step: 1316... Loss: 0.25046... Val Loss: 0.23730\n",
      "Epoch: 1317/2000... Step: 1317... Loss: 0.25045... Val Loss: 0.23728\n",
      "Epoch: 1318/2000... Step: 1318... Loss: 0.25044... Val Loss: 0.23726\n",
      "Epoch: 1319/2000... Step: 1319... Loss: 0.25043... Val Loss: 0.23724\n",
      "Epoch: 1320/2000... Step: 1320... Loss: 0.25042... Val Loss: 0.23723\n",
      "Epoch: 1321/2000... Step: 1321... Loss: 0.25041... Val Loss: 0.23721\n",
      "Epoch: 1322/2000... Step: 1322... Loss: 0.25040... Val Loss: 0.23719\n",
      "Epoch: 1323/2000... Step: 1323... Loss: 0.25039... Val Loss: 0.23718\n",
      "Epoch: 1324/2000... Step: 1324... Loss: 0.25038... Val Loss: 0.23716\n",
      "Epoch: 1325/2000... Step: 1325... Loss: 0.25037... Val Loss: 0.23714\n",
      "Epoch: 1326/2000... Step: 1326... Loss: 0.25036... Val Loss: 0.23713\n",
      "Epoch: 1327/2000... Step: 1327... Loss: 0.25035... Val Loss: 0.23711\n",
      "Epoch: 1328/2000... Step: 1328... Loss: 0.25034... Val Loss: 0.23709\n",
      "Epoch: 1329/2000... Step: 1329... Loss: 0.25033... Val Loss: 0.23708\n",
      "Epoch: 1330/2000... Step: 1330... Loss: 0.25032... Val Loss: 0.23706\n",
      "Epoch: 1331/2000... Step: 1331... Loss: 0.25031... Val Loss: 0.23704\n",
      "Epoch: 1332/2000... Step: 1332... Loss: 0.25030... Val Loss: 0.23703\n",
      "Epoch: 1333/2000... Step: 1333... Loss: 0.25029... Val Loss: 0.23701\n",
      "Epoch: 1334/2000... Step: 1334... Loss: 0.25028... Val Loss: 0.23699\n",
      "Epoch: 1335/2000... Step: 1335... Loss: 0.25027... Val Loss: 0.23698\n",
      "Epoch: 1336/2000... Step: 1336... Loss: 0.25026... Val Loss: 0.23696\n",
      "Epoch: 1337/2000... Step: 1337... Loss: 0.25025... Val Loss: 0.23694\n",
      "Epoch: 1338/2000... Step: 1338... Loss: 0.25025... Val Loss: 0.23693\n",
      "Epoch: 1339/2000... Step: 1339... Loss: 0.25024... Val Loss: 0.23691\n",
      "Epoch: 1340/2000... Step: 1340... Loss: 0.25023... Val Loss: 0.23689\n",
      "Epoch: 1341/2000... Step: 1341... Loss: 0.25022... Val Loss: 0.23688\n",
      "Epoch: 1342/2000... Step: 1342... Loss: 0.25021... Val Loss: 0.23686\n",
      "Epoch: 1343/2000... Step: 1343... Loss: 0.25020... Val Loss: 0.23685\n",
      "Epoch: 1344/2000... Step: 1344... Loss: 0.25019... Val Loss: 0.23683\n",
      "Epoch: 1345/2000... Step: 1345... Loss: 0.25018... Val Loss: 0.23681\n",
      "Epoch: 1346/2000... Step: 1346... Loss: 0.25017... Val Loss: 0.23680\n",
      "Epoch: 1347/2000... Step: 1347... Loss: 0.25016... Val Loss: 0.23678\n",
      "Epoch: 1348/2000... Step: 1348... Loss: 0.25016... Val Loss: 0.23677\n",
      "Epoch: 1349/2000... Step: 1349... Loss: 0.25015... Val Loss: 0.23675\n",
      "Epoch: 1350/2000... Step: 1350... Loss: 0.25014... Val Loss: 0.23673\n",
      "Epoch: 1351/2000... Step: 1351... Loss: 0.25013... Val Loss: 0.23672\n",
      "Epoch: 1352/2000... Step: 1352... Loss: 0.25012... Val Loss: 0.23670\n",
      "Epoch: 1353/2000... Step: 1353... Loss: 0.25011... Val Loss: 0.23669\n",
      "Epoch: 1354/2000... Step: 1354... Loss: 0.25010... Val Loss: 0.23667\n",
      "Epoch: 1355/2000... Step: 1355... Loss: 0.25009... Val Loss: 0.23666\n",
      "Epoch: 1356/2000... Step: 1356... Loss: 0.25008... Val Loss: 0.23664\n",
      "Epoch: 1357/2000... Step: 1357... Loss: 0.25008... Val Loss: 0.23662\n",
      "Epoch: 1358/2000... Step: 1358... Loss: 0.25007... Val Loss: 0.23661\n",
      "Epoch: 1359/2000... Step: 1359... Loss: 0.25006... Val Loss: 0.23659\n",
      "Epoch: 1360/2000... Step: 1360... Loss: 0.25005... Val Loss: 0.23658\n",
      "Epoch: 1361/2000... Step: 1361... Loss: 0.25004... Val Loss: 0.23656\n",
      "Epoch: 1362/2000... Step: 1362... Loss: 0.25003... Val Loss: 0.23655\n",
      "Epoch: 1363/2000... Step: 1363... Loss: 0.25002... Val Loss: 0.23653\n",
      "Epoch: 1364/2000... Step: 1364... Loss: 0.25002... Val Loss: 0.23652\n",
      "Epoch: 1365/2000... Step: 1365... Loss: 0.25001... Val Loss: 0.23650\n",
      "Epoch: 1366/2000... Step: 1366... Loss: 0.25000... Val Loss: 0.23649\n",
      "Epoch: 1367/2000... Step: 1367... Loss: 0.24999... Val Loss: 0.23647\n",
      "Epoch: 1368/2000... Step: 1368... Loss: 0.24998... Val Loss: 0.23646\n",
      "Epoch: 1369/2000... Step: 1369... Loss: 0.24997... Val Loss: 0.23644\n",
      "Epoch: 1370/2000... Step: 1370... Loss: 0.24997... Val Loss: 0.23642\n",
      "Epoch: 1371/2000... Step: 1371... Loss: 0.24996... Val Loss: 0.23641\n",
      "Epoch: 1372/2000... Step: 1372... Loss: 0.24995... Val Loss: 0.23639\n",
      "Epoch: 1373/2000... Step: 1373... Loss: 0.24994... Val Loss: 0.23638\n",
      "Epoch: 1374/2000... Step: 1374... Loss: 0.24993... Val Loss: 0.23636\n",
      "Epoch: 1375/2000... Step: 1375... Loss: 0.24993... Val Loss: 0.23635\n",
      "Epoch: 1376/2000... Step: 1376... Loss: 0.24992... Val Loss: 0.23634\n",
      "Epoch: 1377/2000... Step: 1377... Loss: 0.24991... Val Loss: 0.23632\n",
      "Epoch: 1378/2000... Step: 1378... Loss: 0.24990... Val Loss: 0.23631\n",
      "Epoch: 1379/2000... Step: 1379... Loss: 0.24989... Val Loss: 0.23629\n",
      "Epoch: 1380/2000... Step: 1380... Loss: 0.24988... Val Loss: 0.23628\n",
      "Epoch: 1381/2000... Step: 1381... Loss: 0.24988... Val Loss: 0.23626\n",
      "Epoch: 1382/2000... Step: 1382... Loss: 0.24987... Val Loss: 0.23625\n",
      "Epoch: 1383/2000... Step: 1383... Loss: 0.24986... Val Loss: 0.23623\n",
      "Epoch: 1384/2000... Step: 1384... Loss: 0.24985... Val Loss: 0.23622\n",
      "Epoch: 1385/2000... Step: 1385... Loss: 0.24984... Val Loss: 0.23620\n",
      "Epoch: 1386/2000... Step: 1386... Loss: 0.24984... Val Loss: 0.23619\n",
      "Epoch: 1387/2000... Step: 1387... Loss: 0.24983... Val Loss: 0.23617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1388/2000... Step: 1388... Loss: 0.24982... Val Loss: 0.23616\n",
      "Epoch: 1389/2000... Step: 1389... Loss: 0.24981... Val Loss: 0.23615\n",
      "Epoch: 1390/2000... Step: 1390... Loss: 0.24981... Val Loss: 0.23613\n",
      "Epoch: 1391/2000... Step: 1391... Loss: 0.24980... Val Loss: 0.23612\n",
      "Epoch: 1392/2000... Step: 1392... Loss: 0.24979... Val Loss: 0.23610\n",
      "Epoch: 1393/2000... Step: 1393... Loss: 0.24978... Val Loss: 0.23609\n",
      "Epoch: 1394/2000... Step: 1394... Loss: 0.24977... Val Loss: 0.23607\n",
      "Epoch: 1395/2000... Step: 1395... Loss: 0.24977... Val Loss: 0.23606\n",
      "Epoch: 1396/2000... Step: 1396... Loss: 0.24976... Val Loss: 0.23605\n",
      "Epoch: 1397/2000... Step: 1397... Loss: 0.24975... Val Loss: 0.23603\n",
      "Epoch: 1398/2000... Step: 1398... Loss: 0.24974... Val Loss: 0.23602\n",
      "Epoch: 1399/2000... Step: 1399... Loss: 0.24974... Val Loss: 0.23600\n",
      "Epoch: 1400/2000... Step: 1400... Loss: 0.24973... Val Loss: 0.23599\n",
      "Epoch: 1401/2000... Step: 1401... Loss: 0.24972... Val Loss: 0.23597\n",
      "Epoch: 1402/2000... Step: 1402... Loss: 0.24971... Val Loss: 0.23596\n",
      "Epoch: 1403/2000... Step: 1403... Loss: 0.24971... Val Loss: 0.23595\n",
      "Epoch: 1404/2000... Step: 1404... Loss: 0.24970... Val Loss: 0.23593\n",
      "Epoch: 1405/2000... Step: 1405... Loss: 0.24969... Val Loss: 0.23592\n",
      "Epoch: 1406/2000... Step: 1406... Loss: 0.24968... Val Loss: 0.23591\n",
      "Epoch: 1407/2000... Step: 1407... Loss: 0.24968... Val Loss: 0.23589\n",
      "Epoch: 1408/2000... Step: 1408... Loss: 0.24967... Val Loss: 0.23588\n",
      "Epoch: 1409/2000... Step: 1409... Loss: 0.24966... Val Loss: 0.23586\n",
      "Epoch: 1410/2000... Step: 1410... Loss: 0.24966... Val Loss: 0.23585\n",
      "Epoch: 1411/2000... Step: 1411... Loss: 0.24965... Val Loss: 0.23584\n",
      "Epoch: 1412/2000... Step: 1412... Loss: 0.24964... Val Loss: 0.23582\n",
      "Epoch: 1413/2000... Step: 1413... Loss: 0.24963... Val Loss: 0.23581\n",
      "Epoch: 1414/2000... Step: 1414... Loss: 0.24963... Val Loss: 0.23580\n",
      "Epoch: 1415/2000... Step: 1415... Loss: 0.24962... Val Loss: 0.23578\n",
      "Epoch: 1416/2000... Step: 1416... Loss: 0.24961... Val Loss: 0.23577\n",
      "Epoch: 1417/2000... Step: 1417... Loss: 0.24961... Val Loss: 0.23576\n",
      "Epoch: 1418/2000... Step: 1418... Loss: 0.24960... Val Loss: 0.23574\n",
      "Epoch: 1419/2000... Step: 1419... Loss: 0.24959... Val Loss: 0.23573\n",
      "Epoch: 1420/2000... Step: 1420... Loss: 0.24958... Val Loss: 0.23572\n",
      "Epoch: 1421/2000... Step: 1421... Loss: 0.24958... Val Loss: 0.23570\n",
      "Epoch: 1422/2000... Step: 1422... Loss: 0.24957... Val Loss: 0.23569\n",
      "Epoch: 1423/2000... Step: 1423... Loss: 0.24956... Val Loss: 0.23568\n",
      "Epoch: 1424/2000... Step: 1424... Loss: 0.24956... Val Loss: 0.23566\n",
      "Epoch: 1425/2000... Step: 1425... Loss: 0.24955... Val Loss: 0.23565\n",
      "Epoch: 1426/2000... Step: 1426... Loss: 0.24954... Val Loss: 0.23564\n",
      "Epoch: 1427/2000... Step: 1427... Loss: 0.24954... Val Loss: 0.23562\n",
      "Epoch: 1428/2000... Step: 1428... Loss: 0.24953... Val Loss: 0.23561\n",
      "Epoch: 1429/2000... Step: 1429... Loss: 0.24952... Val Loss: 0.23560\n",
      "Epoch: 1430/2000... Step: 1430... Loss: 0.24952... Val Loss: 0.23558\n",
      "Epoch: 1431/2000... Step: 1431... Loss: 0.24951... Val Loss: 0.23557\n",
      "Epoch: 1432/2000... Step: 1432... Loss: 0.24950... Val Loss: 0.23556\n",
      "Epoch: 1433/2000... Step: 1433... Loss: 0.24950... Val Loss: 0.23554\n",
      "Epoch: 1434/2000... Step: 1434... Loss: 0.24949... Val Loss: 0.23553\n",
      "Epoch: 1435/2000... Step: 1435... Loss: 0.24948... Val Loss: 0.23552\n",
      "Epoch: 1436/2000... Step: 1436... Loss: 0.24947... Val Loss: 0.23551\n",
      "Epoch: 1437/2000... Step: 1437... Loss: 0.24947... Val Loss: 0.23549\n",
      "Epoch: 1438/2000... Step: 1438... Loss: 0.24946... Val Loss: 0.23548\n",
      "Epoch: 1439/2000... Step: 1439... Loss: 0.24946... Val Loss: 0.23547\n",
      "Epoch: 1440/2000... Step: 1440... Loss: 0.24945... Val Loss: 0.23546\n",
      "Epoch: 1441/2000... Step: 1441... Loss: 0.24944... Val Loss: 0.23544\n",
      "Epoch: 1442/2000... Step: 1442... Loss: 0.24944... Val Loss: 0.23543\n",
      "Epoch: 1443/2000... Step: 1443... Loss: 0.24943... Val Loss: 0.23542\n",
      "Epoch: 1444/2000... Step: 1444... Loss: 0.24942... Val Loss: 0.23540\n",
      "Epoch: 1445/2000... Step: 1445... Loss: 0.24942... Val Loss: 0.23539\n",
      "Epoch: 1446/2000... Step: 1446... Loss: 0.24941... Val Loss: 0.23538\n",
      "Epoch: 1447/2000... Step: 1447... Loss: 0.24940... Val Loss: 0.23537\n",
      "Epoch: 1448/2000... Step: 1448... Loss: 0.24940... Val Loss: 0.23535\n",
      "Epoch: 1449/2000... Step: 1449... Loss: 0.24939... Val Loss: 0.23534\n",
      "Epoch: 1450/2000... Step: 1450... Loss: 0.24938... Val Loss: 0.23533\n",
      "Epoch: 1451/2000... Step: 1451... Loss: 0.24938... Val Loss: 0.23532\n",
      "Epoch: 1452/2000... Step: 1452... Loss: 0.24937... Val Loss: 0.23531\n",
      "Epoch: 1453/2000... Step: 1453... Loss: 0.24936... Val Loss: 0.23529\n",
      "Epoch: 1454/2000... Step: 1454... Loss: 0.24936... Val Loss: 0.23528\n",
      "Epoch: 1455/2000... Step: 1455... Loss: 0.24935... Val Loss: 0.23527\n",
      "Epoch: 1456/2000... Step: 1456... Loss: 0.24935... Val Loss: 0.23526\n",
      "Epoch: 1457/2000... Step: 1457... Loss: 0.24934... Val Loss: 0.23524\n",
      "Epoch: 1458/2000... Step: 1458... Loss: 0.24933... Val Loss: 0.23523\n",
      "Epoch: 1459/2000... Step: 1459... Loss: 0.24933... Val Loss: 0.23522\n",
      "Epoch: 1460/2000... Step: 1460... Loss: 0.24932... Val Loss: 0.23521\n",
      "Epoch: 1461/2000... Step: 1461... Loss: 0.24932... Val Loss: 0.23520\n",
      "Epoch: 1462/2000... Step: 1462... Loss: 0.24931... Val Loss: 0.23518\n",
      "Epoch: 1463/2000... Step: 1463... Loss: 0.24930... Val Loss: 0.23517\n",
      "Epoch: 1464/2000... Step: 1464... Loss: 0.24930... Val Loss: 0.23516\n",
      "Epoch: 1465/2000... Step: 1465... Loss: 0.24929... Val Loss: 0.23515\n",
      "Epoch: 1466/2000... Step: 1466... Loss: 0.24928... Val Loss: 0.23514\n",
      "Epoch: 1467/2000... Step: 1467... Loss: 0.24928... Val Loss: 0.23512\n",
      "Epoch: 1468/2000... Step: 1468... Loss: 0.24927... Val Loss: 0.23511\n",
      "Epoch: 1469/2000... Step: 1469... Loss: 0.24927... Val Loss: 0.23510\n",
      "Epoch: 1470/2000... Step: 1470... Loss: 0.24926... Val Loss: 0.23509\n",
      "Epoch: 1471/2000... Step: 1471... Loss: 0.24926... Val Loss: 0.23508\n",
      "Epoch: 1472/2000... Step: 1472... Loss: 0.24925... Val Loss: 0.23506\n",
      "Epoch: 1473/2000... Step: 1473... Loss: 0.24924... Val Loss: 0.23505\n",
      "Epoch: 1474/2000... Step: 1474... Loss: 0.24924... Val Loss: 0.23504\n",
      "Epoch: 1475/2000... Step: 1475... Loss: 0.24923... Val Loss: 0.23503\n",
      "Epoch: 1476/2000... Step: 1476... Loss: 0.24923... Val Loss: 0.23502\n",
      "Epoch: 1477/2000... Step: 1477... Loss: 0.24922... Val Loss: 0.23501\n",
      "Epoch: 1478/2000... Step: 1478... Loss: 0.24921... Val Loss: 0.23499\n",
      "Epoch: 1479/2000... Step: 1479... Loss: 0.24921... Val Loss: 0.23498\n",
      "Epoch: 1480/2000... Step: 1480... Loss: 0.24920... Val Loss: 0.23497\n",
      "Epoch: 1481/2000... Step: 1481... Loss: 0.24920... Val Loss: 0.23496\n",
      "Epoch: 1482/2000... Step: 1482... Loss: 0.24919... Val Loss: 0.23495\n",
      "Epoch: 1483/2000... Step: 1483... Loss: 0.24919... Val Loss: 0.23494\n",
      "Epoch: 1484/2000... Step: 1484... Loss: 0.24918... Val Loss: 0.23493\n",
      "Epoch: 1485/2000... Step: 1485... Loss: 0.24917... Val Loss: 0.23491\n",
      "Epoch: 1486/2000... Step: 1486... Loss: 0.24917... Val Loss: 0.23490\n",
      "Epoch: 1487/2000... Step: 1487... Loss: 0.24916... Val Loss: 0.23489\n",
      "Epoch: 1488/2000... Step: 1488... Loss: 0.24916... Val Loss: 0.23488\n",
      "Epoch: 1489/2000... Step: 1489... Loss: 0.24915... Val Loss: 0.23487\n",
      "Epoch: 1490/2000... Step: 1490... Loss: 0.24915... Val Loss: 0.23486\n",
      "Epoch: 1491/2000... Step: 1491... Loss: 0.24914... Val Loss: 0.23485\n",
      "Epoch: 1492/2000... Step: 1492... Loss: 0.24913... Val Loss: 0.23483\n",
      "Epoch: 1493/2000... Step: 1493... Loss: 0.24913... Val Loss: 0.23482\n",
      "Epoch: 1494/2000... Step: 1494... Loss: 0.24912... Val Loss: 0.23481\n",
      "Epoch: 1495/2000... Step: 1495... Loss: 0.24912... Val Loss: 0.23480\n",
      "Epoch: 1496/2000... Step: 1496... Loss: 0.24911... Val Loss: 0.23479\n",
      "Epoch: 1497/2000... Step: 1497... Loss: 0.24911... Val Loss: 0.23478\n",
      "Epoch: 1498/2000... Step: 1498... Loss: 0.24910... Val Loss: 0.23477\n",
      "Epoch: 1499/2000... Step: 1499... Loss: 0.24910... Val Loss: 0.23476\n",
      "Epoch: 1500/2000... Step: 1500... Loss: 0.24909... Val Loss: 0.23475\n",
      "Epoch: 1501/2000... Step: 1501... Loss: 0.24909... Val Loss: 0.23473\n",
      "Epoch: 1502/2000... Step: 1502... Loss: 0.24908... Val Loss: 0.23472\n",
      "Epoch: 1503/2000... Step: 1503... Loss: 0.24908... Val Loss: 0.23471\n",
      "Epoch: 1504/2000... Step: 1504... Loss: 0.24907... Val Loss: 0.23470\n",
      "Epoch: 1505/2000... Step: 1505... Loss: 0.24906... Val Loss: 0.23469\n",
      "Epoch: 1506/2000... Step: 1506... Loss: 0.24906... Val Loss: 0.23468\n",
      "Epoch: 1507/2000... Step: 1507... Loss: 0.24905... Val Loss: 0.23467\n",
      "Epoch: 1508/2000... Step: 1508... Loss: 0.24905... Val Loss: 0.23466\n",
      "Epoch: 1509/2000... Step: 1509... Loss: 0.24904... Val Loss: 0.23465\n",
      "Epoch: 1510/2000... Step: 1510... Loss: 0.24904... Val Loss: 0.23464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1511/2000... Step: 1511... Loss: 0.24903... Val Loss: 0.23463\n",
      "Epoch: 1512/2000... Step: 1512... Loss: 0.24903... Val Loss: 0.23462\n",
      "Epoch: 1513/2000... Step: 1513... Loss: 0.24902... Val Loss: 0.23461\n",
      "Epoch: 1514/2000... Step: 1514... Loss: 0.24902... Val Loss: 0.23459\n",
      "Epoch: 1515/2000... Step: 1515... Loss: 0.24901... Val Loss: 0.23458\n",
      "Epoch: 1516/2000... Step: 1516... Loss: 0.24901... Val Loss: 0.23457\n",
      "Epoch: 1517/2000... Step: 1517... Loss: 0.24900... Val Loss: 0.23456\n",
      "Epoch: 1518/2000... Step: 1518... Loss: 0.24900... Val Loss: 0.23455\n",
      "Epoch: 1519/2000... Step: 1519... Loss: 0.24899... Val Loss: 0.23454\n",
      "Epoch: 1520/2000... Step: 1520... Loss: 0.24899... Val Loss: 0.23453\n",
      "Epoch: 1521/2000... Step: 1521... Loss: 0.24898... Val Loss: 0.23452\n",
      "Epoch: 1522/2000... Step: 1522... Loss: 0.24898... Val Loss: 0.23451\n",
      "Epoch: 1523/2000... Step: 1523... Loss: 0.24897... Val Loss: 0.23450\n",
      "Epoch: 1524/2000... Step: 1524... Loss: 0.24897... Val Loss: 0.23449\n",
      "Epoch: 1525/2000... Step: 1525... Loss: 0.24896... Val Loss: 0.23448\n",
      "Epoch: 1526/2000... Step: 1526... Loss: 0.24896... Val Loss: 0.23447\n",
      "Epoch: 1527/2000... Step: 1527... Loss: 0.24895... Val Loss: 0.23446\n",
      "Epoch: 1528/2000... Step: 1528... Loss: 0.24895... Val Loss: 0.23445\n",
      "Epoch: 1529/2000... Step: 1529... Loss: 0.24894... Val Loss: 0.23444\n",
      "Epoch: 1530/2000... Step: 1530... Loss: 0.24894... Val Loss: 0.23443\n",
      "Epoch: 1531/2000... Step: 1531... Loss: 0.24893... Val Loss: 0.23442\n",
      "Epoch: 1532/2000... Step: 1532... Loss: 0.24893... Val Loss: 0.23441\n",
      "Epoch: 1533/2000... Step: 1533... Loss: 0.24892... Val Loss: 0.23440\n",
      "Epoch: 1534/2000... Step: 1534... Loss: 0.24892... Val Loss: 0.23439\n",
      "Epoch: 1535/2000... Step: 1535... Loss: 0.24891... Val Loss: 0.23438\n",
      "Epoch: 1536/2000... Step: 1536... Loss: 0.24891... Val Loss: 0.23437\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1537/2000... Step: 1537... Loss: 0.24890... Val Loss: 0.23436\n",
      "Epoch: 1538/2000... Step: 1538... Loss: 0.24890... Val Loss: 0.23435\n",
      "Epoch: 1539/2000... Step: 1539... Loss: 0.24890... Val Loss: 0.23434\n",
      "Epoch: 1540/2000... Step: 1540... Loss: 0.24889... Val Loss: 0.23433\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1541/2000... Step: 1541... Loss: 0.24889... Val Loss: 0.23432\n",
      "Epoch: 1542/2000... Step: 1542... Loss: 0.24888... Val Loss: 0.23431\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1543/2000... Step: 1543... Loss: 0.24888... Val Loss: 0.23430\n",
      "Epoch: 1544/2000... Step: 1544... Loss: 0.24887... Val Loss: 0.23429\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1545/2000... Step: 1545... Loss: 0.24887... Val Loss: 0.23428\n",
      "Epoch: 1546/2000... Step: 1546... Loss: 0.24886... Val Loss: 0.23427\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1547/2000... Step: 1547... Loss: 0.24886... Val Loss: 0.23426\n",
      "Epoch: 1548/2000... Step: 1548... Loss: 0.24885... Val Loss: 0.23425\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1549/2000... Step: 1549... Loss: 0.24885... Val Loss: 0.23424\n",
      "Epoch: 1550/2000... Step: 1550... Loss: 0.24884... Val Loss: 0.23423\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1551/2000... Step: 1551... Loss: 0.24884... Val Loss: 0.23422\n",
      "Epoch: 1552/2000... Step: 1552... Loss: 0.24884... Val Loss: 0.23421\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1553/2000... Step: 1553... Loss: 0.24883... Val Loss: 0.23420\n",
      "Epoch: 1554/2000... Step: 1554... Loss: 0.24883... Val Loss: 0.23419\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1555/2000... Step: 1555... Loss: 0.24882... Val Loss: 0.23418\n",
      "Epoch: 1556/2000... Step: 1556... Loss: 0.24882... Val Loss: 0.23417\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1557/2000... Step: 1557... Loss: 0.24881... Val Loss: 0.23416\n",
      "Epoch: 1558/2000... Step: 1558... Loss: 0.24881... Val Loss: 0.23415\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1559/2000... Step: 1559... Loss: 0.24880... Val Loss: 0.23414\n",
      "Epoch: 1560/2000... Step: 1560... Loss: 0.24880... Val Loss: 0.23413\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1561/2000... Step: 1561... Loss: 0.24880... Val Loss: 0.23412\n",
      "Epoch: 1562/2000... Step: 1562... Loss: 0.24879... Val Loss: 0.23411\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1563/2000... Step: 1563... Loss: 0.24879... Val Loss: 0.23410\n",
      "Epoch: 1564/2000... Step: 1564... Loss: 0.24878... Val Loss: 0.23409\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1565/2000... Step: 1565... Loss: 0.24878... Val Loss: 0.23408\n",
      "Epoch: 1566/2000... Step: 1566... Loss: 0.24877... Val Loss: 0.23408\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1567/2000... Step: 1567... Loss: 0.24877... Val Loss: 0.23407\n",
      "Epoch: 1568/2000... Step: 1568... Loss: 0.24877... Val Loss: 0.23406\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1569/2000... Step: 1569... Loss: 0.24876... Val Loss: 0.23405\n",
      "Epoch: 1570/2000... Step: 1570... Loss: 0.24876... Val Loss: 0.23404\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1571/2000... Step: 1571... Loss: 0.24875... Val Loss: 0.23403\n",
      "Epoch: 1572/2000... Step: 1572... Loss: 0.24875... Val Loss: 0.23402\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1573/2000... Step: 1573... Loss: 0.24874... Val Loss: 0.23401\n",
      "Epoch: 1574/2000... Step: 1574... Loss: 0.24874... Val Loss: 0.23400\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1575/2000... Step: 1575... Loss: 0.24874... Val Loss: 0.23399\n",
      "Epoch: 1576/2000... Step: 1576... Loss: 0.24873... Val Loss: 0.23398\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1577/2000... Step: 1577... Loss: 0.24873... Val Loss: 0.23397\n",
      "Epoch: 1578/2000... Step: 1578... Loss: 0.24872... Val Loss: 0.23396\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1579/2000... Step: 1579... Loss: 0.24872... Val Loss: 0.23396\n",
      "Epoch: 1580/2000... Step: 1580... Loss: 0.24872... Val Loss: 0.23395\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1581/2000... Step: 1581... Loss: 0.24871... Val Loss: 0.23394\n",
      "Epoch: 1582/2000... Step: 1582... Loss: 0.24871... Val Loss: 0.23393\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1583/2000... Step: 1583... Loss: 0.24870... Val Loss: 0.23392\n",
      "Epoch: 1584/2000... Step: 1584... Loss: 0.24870... Val Loss: 0.23391\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1585/2000... Step: 1585... Loss: 0.24869... Val Loss: 0.23390\n",
      "Epoch: 1586/2000... Step: 1586... Loss: 0.24869... Val Loss: 0.23389\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1587/2000... Step: 1587... Loss: 0.24869... Val Loss: 0.23388\n",
      "Epoch: 1588/2000... Step: 1588... Loss: 0.24868... Val Loss: 0.23387\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1589/2000... Step: 1589... Loss: 0.24868... Val Loss: 0.23387\n",
      "Epoch: 1590/2000... Step: 1590... Loss: 0.24867... Val Loss: 0.23386\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1591/2000... Step: 1591... Loss: 0.24867... Val Loss: 0.23385\n",
      "Epoch: 1592/2000... Step: 1592... Loss: 0.24867... Val Loss: 0.23384\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1593/2000... Step: 1593... Loss: 0.24866... Val Loss: 0.23383\n",
      "Epoch: 1594/2000... Step: 1594... Loss: 0.24866... Val Loss: 0.23382\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1595/2000... Step: 1595... Loss: 0.24866... Val Loss: 0.23381\n",
      "Epoch: 1596/2000... Step: 1596... Loss: 0.24865... Val Loss: 0.23380\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1597/2000... Step: 1597... Loss: 0.24865... Val Loss: 0.23380\n",
      "Epoch: 1598/2000... Step: 1598... Loss: 0.24864... Val Loss: 0.23379\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1599/2000... Step: 1599... Loss: 0.24864... Val Loss: 0.23378\n",
      "Epoch: 1600/2000... Step: 1600... Loss: 0.24864... Val Loss: 0.23377\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1601/2000... Step: 1601... Loss: 0.24863... Val Loss: 0.23376\n",
      "Epoch: 1602/2000... Step: 1602... Loss: 0.24863... Val Loss: 0.23375\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1603/2000... Step: 1603... Loss: 0.24862... Val Loss: 0.23374\n",
      "Epoch: 1604/2000... Step: 1604... Loss: 0.24862... Val Loss: 0.23373\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1605/2000... Step: 1605... Loss: 0.24862... Val Loss: 0.23373\n",
      "Epoch: 1606/2000... Step: 1606... Loss: 0.24861... Val Loss: 0.23372\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1607/2000... Step: 1607... Loss: 0.24861... Val Loss: 0.23371\n",
      "Epoch: 1608/2000... Step: 1608... Loss: 0.24861... Val Loss: 0.23370\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1609/2000... Step: 1609... Loss: 0.24860... Val Loss: 0.23369\n",
      "Epoch: 1610/2000... Step: 1610... Loss: 0.24860... Val Loss: 0.23368\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1611/2000... Step: 1611... Loss: 0.24859... Val Loss: 0.23368\n",
      "Epoch: 1612/2000... Step: 1612... Loss: 0.24859... Val Loss: 0.23367\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1613/2000... Step: 1613... Loss: 0.24859... Val Loss: 0.23366\n",
      "Epoch: 1614/2000... Step: 1614... Loss: 0.24858... Val Loss: 0.23365\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1615/2000... Step: 1615... Loss: 0.24858... Val Loss: 0.23364\n",
      "Epoch: 1616/2000... Step: 1616... Loss: 0.24858... Val Loss: 0.23363\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1617/2000... Step: 1617... Loss: 0.24857... Val Loss: 0.23362\n",
      "Epoch: 1618/2000... Step: 1618... Loss: 0.24857... Val Loss: 0.23362\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1619/2000... Step: 1619... Loss: 0.24857... Val Loss: 0.23361\n",
      "Epoch: 1620/2000... Step: 1620... Loss: 0.24856... Val Loss: 0.23360\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1621/2000... Step: 1621... Loss: 0.24856... Val Loss: 0.23359\n",
      "Epoch: 1622/2000... Step: 1622... Loss: 0.24856... Val Loss: 0.23358\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1623/2000... Step: 1623... Loss: 0.24855... Val Loss: 0.23358\n",
      "Epoch: 1624/2000... Step: 1624... Loss: 0.24855... Val Loss: 0.23357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1625/2000... Step: 1625... Loss: 0.24854... Val Loss: 0.23356\n",
      "Epoch: 1626/2000... Step: 1626... Loss: 0.24854... Val Loss: 0.23355\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1627/2000... Step: 1627... Loss: 0.24854... Val Loss: 0.23354\n",
      "Epoch: 1628/2000... Step: 1628... Loss: 0.24853... Val Loss: 0.23353\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1629/2000... Step: 1629... Loss: 0.24853... Val Loss: 0.23353\n",
      "Epoch: 1630/2000... Step: 1630... Loss: 0.24853... Val Loss: 0.23352\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1631/2000... Step: 1631... Loss: 0.24852... Val Loss: 0.23351\n",
      "Epoch: 1632/2000... Step: 1632... Loss: 0.24852... Val Loss: 0.23350\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1633/2000... Step: 1633... Loss: 0.24852... Val Loss: 0.23349\n",
      "Epoch: 1634/2000... Step: 1634... Loss: 0.24851... Val Loss: 0.23349\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1635/2000... Step: 1635... Loss: 0.24851... Val Loss: 0.23348\n",
      "Epoch: 1636/2000... Step: 1636... Loss: 0.24851... Val Loss: 0.23347\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1637/2000... Step: 1637... Loss: 0.24850... Val Loss: 0.23346\n",
      "Epoch: 1638/2000... Step: 1638... Loss: 0.24850... Val Loss: 0.23345\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1639/2000... Step: 1639... Loss: 0.24850... Val Loss: 0.23345\n",
      "Epoch: 1640/2000... Step: 1640... Loss: 0.24849... Val Loss: 0.23344\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1641/2000... Step: 1641... Loss: 0.24849... Val Loss: 0.23343\n",
      "Epoch: 1642/2000... Step: 1642... Loss: 0.24849... Val Loss: 0.23342\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1643/2000... Step: 1643... Loss: 0.24848... Val Loss: 0.23342\n",
      "Epoch: 1644/2000... Step: 1644... Loss: 0.24848... Val Loss: 0.23341\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1645/2000... Step: 1645... Loss: 0.24848... Val Loss: 0.23340\n",
      "Epoch: 1646/2000... Step: 1646... Loss: 0.24847... Val Loss: 0.23339\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1647/2000... Step: 1647... Loss: 0.24847... Val Loss: 0.23338\n",
      "Epoch: 1648/2000... Step: 1648... Loss: 0.24847... Val Loss: 0.23338\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1649/2000... Step: 1649... Loss: 0.24846... Val Loss: 0.23337\n",
      "Epoch: 1650/2000... Step: 1650... Loss: 0.24846... Val Loss: 0.23336\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1651/2000... Step: 1651... Loss: 0.24846... Val Loss: 0.23335\n",
      "Epoch: 1652/2000... Step: 1652... Loss: 0.24845... Val Loss: 0.23335\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1653/2000... Step: 1653... Loss: 0.24845... Val Loss: 0.23334\n",
      "Epoch: 1654/2000... Step: 1654... Loss: 0.24845... Val Loss: 0.23333\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1655/2000... Step: 1655... Loss: 0.24845... Val Loss: 0.23332\n",
      "Epoch: 1656/2000... Step: 1656... Loss: 0.24844... Val Loss: 0.23332\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1657/2000... Step: 1657... Loss: 0.24844... Val Loss: 0.23331\n",
      "Epoch: 1658/2000... Step: 1658... Loss: 0.24844... Val Loss: 0.23330\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1659/2000... Step: 1659... Loss: 0.24843... Val Loss: 0.23329\n",
      "Epoch: 1660/2000... Step: 1660... Loss: 0.24843... Val Loss: 0.23329\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1661/2000... Step: 1661... Loss: 0.24843... Val Loss: 0.23328\n",
      "Epoch: 1662/2000... Step: 1662... Loss: 0.24842... Val Loss: 0.23327\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1663/2000... Step: 1663... Loss: 0.24842... Val Loss: 0.23326\n",
      "Epoch: 1664/2000... Step: 1664... Loss: 0.24842... Val Loss: 0.23326\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1665/2000... Step: 1665... Loss: 0.24841... Val Loss: 0.23325\n",
      "Epoch: 1666/2000... Step: 1666... Loss: 0.24841... Val Loss: 0.23324\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1667/2000... Step: 1667... Loss: 0.24841... Val Loss: 0.23323\n",
      "Epoch: 1668/2000... Step: 1668... Loss: 0.24841... Val Loss: 0.23323\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1669/2000... Step: 1669... Loss: 0.24840... Val Loss: 0.23322\n",
      "Epoch: 1670/2000... Step: 1670... Loss: 0.24840... Val Loss: 0.23321\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1671/2000... Step: 1671... Loss: 0.24840... Val Loss: 0.23320\n",
      "Epoch: 1672/2000... Step: 1672... Loss: 0.24839... Val Loss: 0.23320\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1673/2000... Step: 1673... Loss: 0.24839... Val Loss: 0.23319\n",
      "Epoch: 1674/2000... Step: 1674... Loss: 0.24839... Val Loss: 0.23318\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1675/2000... Step: 1675... Loss: 0.24838... Val Loss: 0.23317\n",
      "Epoch: 1676/2000... Step: 1676... Loss: 0.24838... Val Loss: 0.23317\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1677/2000... Step: 1677... Loss: 0.24838... Val Loss: 0.23316\n",
      "Epoch: 1678/2000... Step: 1678... Loss: 0.24838... Val Loss: 0.23315\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1679/2000... Step: 1679... Loss: 0.24837... Val Loss: 0.23315\n",
      "Epoch: 1680/2000... Step: 1680... Loss: 0.24837... Val Loss: 0.23314\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1681/2000... Step: 1681... Loss: 0.24837... Val Loss: 0.23313\n",
      "Epoch: 1682/2000... Step: 1682... Loss: 0.24836... Val Loss: 0.23312\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1683/2000... Step: 1683... Loss: 0.24836... Val Loss: 0.23312\n",
      "Epoch: 1684/2000... Step: 1684... Loss: 0.24836... Val Loss: 0.23311\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1685/2000... Step: 1685... Loss: 0.24836... Val Loss: 0.23310\n",
      "Epoch: 1686/2000... Step: 1686... Loss: 0.24835... Val Loss: 0.23310\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1687/2000... Step: 1687... Loss: 0.24835... Val Loss: 0.23309\n",
      "Epoch: 1688/2000... Step: 1688... Loss: 0.24835... Val Loss: 0.23308\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1689/2000... Step: 1689... Loss: 0.24834... Val Loss: 0.23308\n",
      "Epoch: 1690/2000... Step: 1690... Loss: 0.24834... Val Loss: 0.23307\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1691/2000... Step: 1691... Loss: 0.24834... Val Loss: 0.23306\n",
      "Epoch: 1692/2000... Step: 1692... Loss: 0.24834... Val Loss: 0.23305\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1693/2000... Step: 1693... Loss: 0.24833... Val Loss: 0.23305\n",
      "Epoch: 1694/2000... Step: 1694... Loss: 0.24833... Val Loss: 0.23304\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1695/2000... Step: 1695... Loss: 0.24833... Val Loss: 0.23303\n",
      "Epoch: 1696/2000... Step: 1696... Loss: 0.24832... Val Loss: 0.23303\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1697/2000... Step: 1697... Loss: 0.24832... Val Loss: 0.23302\n",
      "Epoch: 1698/2000... Step: 1698... Loss: 0.24832... Val Loss: 0.23301\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1699/2000... Step: 1699... Loss: 0.24832... Val Loss: 0.23301\n",
      "Epoch: 1700/2000... Step: 1700... Loss: 0.24831... Val Loss: 0.23300\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1701/2000... Step: 1701... Loss: 0.24831... Val Loss: 0.23299\n",
      "Epoch: 1702/2000... Step: 1702... Loss: 0.24831... Val Loss: 0.23299\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1703/2000... Step: 1703... Loss: 0.24831... Val Loss: 0.23298\n",
      "Epoch: 1704/2000... Step: 1704... Loss: 0.24830... Val Loss: 0.23297\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1705/2000... Step: 1705... Loss: 0.24830... Val Loss: 0.23296\n",
      "Epoch: 1706/2000... Step: 1706... Loss: 0.24830... Val Loss: 0.23296\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1707/2000... Step: 1707... Loss: 0.24830... Val Loss: 0.23295\n",
      "Epoch: 1708/2000... Step: 1708... Loss: 0.24829... Val Loss: 0.23294\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1709/2000... Step: 1709... Loss: 0.24829... Val Loss: 0.23294\n",
      "Epoch: 1710/2000... Step: 1710... Loss: 0.24829... Val Loss: 0.23293\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1711/2000... Step: 1711... Loss: 0.24828... Val Loss: 0.23292\n",
      "Epoch: 1712/2000... Step: 1712... Loss: 0.24828... Val Loss: 0.23292\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1713/2000... Step: 1713... Loss: 0.24828... Val Loss: 0.23291\n",
      "Epoch: 1714/2000... Step: 1714... Loss: 0.24828... Val Loss: 0.23290\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1715/2000... Step: 1715... Loss: 0.24827... Val Loss: 0.23290\n",
      "Epoch: 1716/2000... Step: 1716... Loss: 0.24827... Val Loss: 0.23289\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1717/2000... Step: 1717... Loss: 0.24827... Val Loss: 0.23289\n",
      "Epoch: 1718/2000... Step: 1718... Loss: 0.24827... Val Loss: 0.23288\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1719/2000... Step: 1719... Loss: 0.24826... Val Loss: 0.23287\n",
      "Epoch: 1720/2000... Step: 1720... Loss: 0.24826... Val Loss: 0.23287\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1721/2000... Step: 1721... Loss: 0.24826... Val Loss: 0.23286\n",
      "Epoch: 1722/2000... Step: 1722... Loss: 0.24826... Val Loss: 0.23285\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1723/2000... Step: 1723... Loss: 0.24825... Val Loss: 0.23285\n",
      "Epoch: 1724/2000... Step: 1724... Loss: 0.24825... Val Loss: 0.23284\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1725/2000... Step: 1725... Loss: 0.24825... Val Loss: 0.23283\n",
      "Epoch: 1726/2000... Step: 1726... Loss: 0.24825... Val Loss: 0.23283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1727/2000... Step: 1727... Loss: 0.24824... Val Loss: 0.23282\n",
      "Epoch: 1728/2000... Step: 1728... Loss: 0.24824... Val Loss: 0.23281\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1729/2000... Step: 1729... Loss: 0.24824... Val Loss: 0.23281\n",
      "Epoch: 1730/2000... Step: 1730... Loss: 0.24824... Val Loss: 0.23280\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1731/2000... Step: 1731... Loss: 0.24824... Val Loss: 0.23279\n",
      "Epoch: 1732/2000... Step: 1732... Loss: 0.24823... Val Loss: 0.23279\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1733/2000... Step: 1733... Loss: 0.24823... Val Loss: 0.23278\n",
      "Epoch: 1734/2000... Step: 1734... Loss: 0.24823... Val Loss: 0.23278\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1735/2000... Step: 1735... Loss: 0.24823... Val Loss: 0.23277\n",
      "Epoch: 1736/2000... Step: 1736... Loss: 0.24822... Val Loss: 0.23276\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1737/2000... Step: 1737... Loss: 0.24822... Val Loss: 0.23276\n",
      "Epoch: 1738/2000... Step: 1738... Loss: 0.24822... Val Loss: 0.23275\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1739/2000... Step: 1739... Loss: 0.24822... Val Loss: 0.23274\n",
      "Epoch: 1740/2000... Step: 1740... Loss: 0.24821... Val Loss: 0.23274\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1741/2000... Step: 1741... Loss: 0.24821... Val Loss: 0.23273\n",
      "Epoch: 1742/2000... Step: 1742... Loss: 0.24821... Val Loss: 0.23273\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1743/2000... Step: 1743... Loss: 0.24821... Val Loss: 0.23272\n",
      "Epoch: 1744/2000... Step: 1744... Loss: 0.24820... Val Loss: 0.23271\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1745/2000... Step: 1745... Loss: 0.24820... Val Loss: 0.23271\n",
      "Epoch: 1746/2000... Step: 1746... Loss: 0.24820... Val Loss: 0.23270\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1747/2000... Step: 1747... Loss: 0.24820... Val Loss: 0.23270\n",
      "Epoch: 1748/2000... Step: 1748... Loss: 0.24820... Val Loss: 0.23269\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1749/2000... Step: 1749... Loss: 0.24819... Val Loss: 0.23268\n",
      "Epoch: 1750/2000... Step: 1750... Loss: 0.24819... Val Loss: 0.23268\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1751/2000... Step: 1751... Loss: 0.24819... Val Loss: 0.23267\n",
      "Epoch: 1752/2000... Step: 1752... Loss: 0.24819... Val Loss: 0.23266\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1753/2000... Step: 1753... Loss: 0.24818... Val Loss: 0.23266\n",
      "Epoch: 1754/2000... Step: 1754... Loss: 0.24818... Val Loss: 0.23265\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1755/2000... Step: 1755... Loss: 0.24818... Val Loss: 0.23265\n",
      "Epoch: 1756/2000... Step: 1756... Loss: 0.24818... Val Loss: 0.23264\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1757/2000... Step: 1757... Loss: 0.24818... Val Loss: 0.23263\n",
      "Epoch: 1758/2000... Step: 1758... Loss: 0.24817... Val Loss: 0.23263\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1759/2000... Step: 1759... Loss: 0.24817... Val Loss: 0.23262\n",
      "Epoch: 1760/2000... Step: 1760... Loss: 0.24817... Val Loss: 0.23262\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1761/2000... Step: 1761... Loss: 0.24817... Val Loss: 0.23261\n",
      "Epoch: 1762/2000... Step: 1762... Loss: 0.24816... Val Loss: 0.23260\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1763/2000... Step: 1763... Loss: 0.24816... Val Loss: 0.23260\n",
      "Epoch: 1764/2000... Step: 1764... Loss: 0.24816... Val Loss: 0.23259\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1765/2000... Step: 1765... Loss: 0.24816... Val Loss: 0.23259\n",
      "Epoch: 1766/2000... Step: 1766... Loss: 0.24816... Val Loss: 0.23258\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1767/2000... Step: 1767... Loss: 0.24815... Val Loss: 0.23258\n",
      "Epoch: 1768/2000... Step: 1768... Loss: 0.24815... Val Loss: 0.23257\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1769/2000... Step: 1769... Loss: 0.24815... Val Loss: 0.23256\n",
      "Epoch: 1770/2000... Step: 1770... Loss: 0.24815... Val Loss: 0.23256\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1771/2000... Step: 1771... Loss: 0.24815... Val Loss: 0.23255\n",
      "Epoch: 1772/2000... Step: 1772... Loss: 0.24814... Val Loss: 0.23255\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1773/2000... Step: 1773... Loss: 0.24814... Val Loss: 0.23254\n",
      "Epoch: 1774/2000... Step: 1774... Loss: 0.24814... Val Loss: 0.23254\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1775/2000... Step: 1775... Loss: 0.24814... Val Loss: 0.23253\n",
      "Epoch: 1776/2000... Step: 1776... Loss: 0.24814... Val Loss: 0.23252\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1777/2000... Step: 1777... Loss: 0.24813... Val Loss: 0.23252\n",
      "Epoch: 1778/2000... Step: 1778... Loss: 0.24813... Val Loss: 0.23251\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1779/2000... Step: 1779... Loss: 0.24813... Val Loss: 0.23251\n",
      "Epoch: 1780/2000... Step: 1780... Loss: 0.24813... Val Loss: 0.23250\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1781/2000... Step: 1781... Loss: 0.24813... Val Loss: 0.23250\n",
      "Epoch: 1782/2000... Step: 1782... Loss: 0.24812... Val Loss: 0.23249\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1783/2000... Step: 1783... Loss: 0.24812... Val Loss: 0.23248\n",
      "Epoch: 1784/2000... Step: 1784... Loss: 0.24812... Val Loss: 0.23248\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1785/2000... Step: 1785... Loss: 0.24812... Val Loss: 0.23247\n",
      "Epoch: 1786/2000... Step: 1786... Loss: 0.24812... Val Loss: 0.23247\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1787/2000... Step: 1787... Loss: 0.24811... Val Loss: 0.23246\n",
      "Epoch: 1788/2000... Step: 1788... Loss: 0.24811... Val Loss: 0.23246\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1789/2000... Step: 1789... Loss: 0.24811... Val Loss: 0.23245\n",
      "Epoch: 1790/2000... Step: 1790... Loss: 0.24811... Val Loss: 0.23244\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1791/2000... Step: 1791... Loss: 0.24811... Val Loss: 0.23244\n",
      "Epoch: 1792/2000... Step: 1792... Loss: 0.24810... Val Loss: 0.23243\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1793/2000... Step: 1793... Loss: 0.24810... Val Loss: 0.23243\n",
      "Epoch: 1794/2000... Step: 1794... Loss: 0.24810... Val Loss: 0.23242\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1795/2000... Step: 1795... Loss: 0.24810... Val Loss: 0.23242\n",
      "Epoch: 1796/2000... Step: 1796... Loss: 0.24810... Val Loss: 0.23241\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1797/2000... Step: 1797... Loss: 0.24809... Val Loss: 0.23241\n",
      "Epoch: 1798/2000... Step: 1798... Loss: 0.24809... Val Loss: 0.23240\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1799/2000... Step: 1799... Loss: 0.24809... Val Loss: 0.23240\n",
      "Epoch: 1800/2000... Step: 1800... Loss: 0.24809... Val Loss: 0.23239\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1801/2000... Step: 1801... Loss: 0.24809... Val Loss: 0.23238\n",
      "Epoch: 1802/2000... Step: 1802... Loss: 0.24808... Val Loss: 0.23238\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1803/2000... Step: 1803... Loss: 0.24808... Val Loss: 0.23237\n",
      "Epoch: 1804/2000... Step: 1804... Loss: 0.24808... Val Loss: 0.23237\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1805/2000... Step: 1805... Loss: 0.24808... Val Loss: 0.23236\n",
      "Epoch: 1806/2000... Step: 1806... Loss: 0.24808... Val Loss: 0.23236\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1807/2000... Step: 1807... Loss: 0.24808... Val Loss: 0.23235\n",
      "Epoch: 1808/2000... Step: 1808... Loss: 0.24807... Val Loss: 0.23235\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1809/2000... Step: 1809... Loss: 0.24807... Val Loss: 0.23234\n",
      "Epoch: 1810/2000... Step: 1810... Loss: 0.24807... Val Loss: 0.23234\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1811/2000... Step: 1811... Loss: 0.24807... Val Loss: 0.23233\n",
      "Epoch: 1812/2000... Step: 1812... Loss: 0.24807... Val Loss: 0.23233\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1813/2000... Step: 1813... Loss: 0.24806... Val Loss: 0.23232\n",
      "Epoch: 1814/2000... Step: 1814... Loss: 0.24806... Val Loss: 0.23232\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1815/2000... Step: 1815... Loss: 0.24806... Val Loss: 0.23231\n",
      "Epoch: 1816/2000... Step: 1816... Loss: 0.24806... Val Loss: 0.23231\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1817/2000... Step: 1817... Loss: 0.24806... Val Loss: 0.23230\n",
      "Epoch: 1818/2000... Step: 1818... Loss: 0.24806... Val Loss: 0.23230\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1819/2000... Step: 1819... Loss: 0.24805... Val Loss: 0.23229\n",
      "Epoch: 1820/2000... Step: 1820... Loss: 0.24805... Val Loss: 0.23228\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1821/2000... Step: 1821... Loss: 0.24805... Val Loss: 0.23228\n",
      "Epoch: 1822/2000... Step: 1822... Loss: 0.24805... Val Loss: 0.23227\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1823/2000... Step: 1823... Loss: 0.24805... Val Loss: 0.23227\n",
      "Epoch: 1824/2000... Step: 1824... Loss: 0.24805... Val Loss: 0.23226\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1825/2000... Step: 1825... Loss: 0.24804... Val Loss: 0.23226\n",
      "Epoch: 1826/2000... Step: 1826... Loss: 0.24804... Val Loss: 0.23225\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1827/2000... Step: 1827... Loss: 0.24804... Val Loss: 0.23225\n",
      "Epoch: 1828/2000... Step: 1828... Loss: 0.24804... Val Loss: 0.23224\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1829/2000... Step: 1829... Loss: 0.24804... Val Loss: 0.23224\n",
      "Epoch: 1830/2000... Step: 1830... Loss: 0.24804... Val Loss: 0.23223\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1831/2000... Step: 1831... Loss: 0.24803... Val Loss: 0.23223\n",
      "Epoch: 1832/2000... Step: 1832... Loss: 0.24803... Val Loss: 0.23222\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1833/2000... Step: 1833... Loss: 0.24803... Val Loss: 0.23222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1834/2000... Step: 1834... Loss: 0.24803... Val Loss: 0.23221\n",
      "Epoch: 1835/2000... Step: 1835... Loss: 0.24803... Val Loss: 0.23221\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1836/2000... Step: 1836... Loss: 0.24803... Val Loss: 0.23220\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1837/2000... Step: 1837... Loss: 0.24802... Val Loss: 0.23220\n",
      "Epoch: 1838/2000... Step: 1838... Loss: 0.24802... Val Loss: 0.23219\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1839/2000... Step: 1839... Loss: 0.24802... Val Loss: 0.23219\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1840/2000... Step: 1840... Loss: 0.24802... Val Loss: 0.23218\n",
      "Epoch: 1841/2000... Step: 1841... Loss: 0.24802... Val Loss: 0.23218\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1842/2000... Step: 1842... Loss: 0.24802... Val Loss: 0.23217\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1843/2000... Step: 1843... Loss: 0.24801... Val Loss: 0.23217\n",
      "Epoch: 1844/2000... Step: 1844... Loss: 0.24801... Val Loss: 0.23216\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1845/2000... Step: 1845... Loss: 0.24801... Val Loss: 0.23216\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1846/2000... Step: 1846... Loss: 0.24801... Val Loss: 0.23215\n",
      "Epoch: 1847/2000... Step: 1847... Loss: 0.24801... Val Loss: 0.23215\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1848/2000... Step: 1848... Loss: 0.24801... Val Loss: 0.23214\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1849/2000... Step: 1849... Loss: 0.24800... Val Loss: 0.23214\n",
      "Epoch: 1850/2000... Step: 1850... Loss: 0.24800... Val Loss: 0.23214\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1851/2000... Step: 1851... Loss: 0.24800... Val Loss: 0.23213\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1852/2000... Step: 1852... Loss: 0.24800... Val Loss: 0.23213\n",
      "Epoch: 1853/2000... Step: 1853... Loss: 0.24800... Val Loss: 0.23212\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1854/2000... Step: 1854... Loss: 0.24800... Val Loss: 0.23212\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1855/2000... Step: 1855... Loss: 0.24800... Val Loss: 0.23211\n",
      "Epoch: 1856/2000... Step: 1856... Loss: 0.24799... Val Loss: 0.23211\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1857/2000... Step: 1857... Loss: 0.24799... Val Loss: 0.23210\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1858/2000... Step: 1858... Loss: 0.24799... Val Loss: 0.23210\n",
      "Epoch: 1859/2000... Step: 1859... Loss: 0.24799... Val Loss: 0.23209\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1860/2000... Step: 1860... Loss: 0.24799... Val Loss: 0.23209\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1861/2000... Step: 1861... Loss: 0.24799... Val Loss: 0.23208\n",
      "Epoch: 1862/2000... Step: 1862... Loss: 0.24798... Val Loss: 0.23208\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1863/2000... Step: 1863... Loss: 0.24798... Val Loss: 0.23207\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1864/2000... Step: 1864... Loss: 0.24798... Val Loss: 0.23207\n",
      "Epoch: 1865/2000... Step: 1865... Loss: 0.24798... Val Loss: 0.23206\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1866/2000... Step: 1866... Loss: 0.24798... Val Loss: 0.23206\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1867/2000... Step: 1867... Loss: 0.24798... Val Loss: 0.23206\n",
      "Epoch: 1868/2000... Step: 1868... Loss: 0.24798... Val Loss: 0.23205\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1869/2000... Step: 1869... Loss: 0.24797... Val Loss: 0.23205\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1870/2000... Step: 1870... Loss: 0.24797... Val Loss: 0.23204\n",
      "Epoch: 1871/2000... Step: 1871... Loss: 0.24797... Val Loss: 0.23204\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1872/2000... Step: 1872... Loss: 0.24797... Val Loss: 0.23203\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1873/2000... Step: 1873... Loss: 0.24797... Val Loss: 0.23203\n",
      "Epoch: 1874/2000... Step: 1874... Loss: 0.24797... Val Loss: 0.23202\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1875/2000... Step: 1875... Loss: 0.24797... Val Loss: 0.23202\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1876/2000... Step: 1876... Loss: 0.24796... Val Loss: 0.23201\n",
      "Epoch: 1877/2000... Step: 1877... Loss: 0.24796... Val Loss: 0.23201\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1878/2000... Step: 1878... Loss: 0.24796... Val Loss: 0.23201\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1879/2000... Step: 1879... Loss: 0.24796... Val Loss: 0.23200\n",
      "Epoch: 1880/2000... Step: 1880... Loss: 0.24796... Val Loss: 0.23200\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1881/2000... Step: 1881... Loss: 0.24796... Val Loss: 0.23199\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1882/2000... Step: 1882... Loss: 0.24796... Val Loss: 0.23199\n",
      "Epoch: 1883/2000... Step: 1883... Loss: 0.24795... Val Loss: 0.23198\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1884/2000... Step: 1884... Loss: 0.24795... Val Loss: 0.23198\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1885/2000... Step: 1885... Loss: 0.24795... Val Loss: 0.23197\n",
      "Epoch: 1886/2000... Step: 1886... Loss: 0.24795... Val Loss: 0.23197\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1887/2000... Step: 1887... Loss: 0.24795... Val Loss: 0.23196\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1888/2000... Step: 1888... Loss: 0.24795... Val Loss: 0.23196\n",
      "Epoch: 1889/2000... Step: 1889... Loss: 0.24795... Val Loss: 0.23196\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1890/2000... Step: 1890... Loss: 0.24795... Val Loss: 0.23195\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1891/2000... Step: 1891... Loss: 0.24794... Val Loss: 0.23195\n",
      "Epoch: 1892/2000... Step: 1892... Loss: 0.24794... Val Loss: 0.23194\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1893/2000... Step: 1893... Loss: 0.24794... Val Loss: 0.23194\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1894/2000... Step: 1894... Loss: 0.24794... Val Loss: 0.23193\n",
      "Epoch: 1895/2000... Step: 1895... Loss: 0.24794... Val Loss: 0.23193\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1896/2000... Step: 1896... Loss: 0.24794... Val Loss: 0.23193\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1897/2000... Step: 1897... Loss: 0.24794... Val Loss: 0.23192\n",
      "Epoch: 1898/2000... Step: 1898... Loss: 0.24794... Val Loss: 0.23192\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1899/2000... Step: 1899... Loss: 0.24793... Val Loss: 0.23191\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1900/2000... Step: 1900... Loss: 0.24793... Val Loss: 0.23191\n",
      "Epoch: 1901/2000... Step: 1901... Loss: 0.24793... Val Loss: 0.23190\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1902/2000... Step: 1902... Loss: 0.24793... Val Loss: 0.23190\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1903/2000... Step: 1903... Loss: 0.24793... Val Loss: 0.23190\n",
      "Epoch: 1904/2000... Step: 1904... Loss: 0.24793... Val Loss: 0.23189\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1905/2000... Step: 1905... Loss: 0.24793... Val Loss: 0.23189\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1906/2000... Step: 1906... Loss: 0.24792... Val Loss: 0.23188\n",
      "Epoch: 1907/2000... Step: 1907... Loss: 0.24792... Val Loss: 0.23188\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1908/2000... Step: 1908... Loss: 0.24792... Val Loss: 0.23187\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1909/2000... Step: 1909... Loss: 0.24792... Val Loss: 0.23187\n",
      "Epoch: 1910/2000... Step: 1910... Loss: 0.24792... Val Loss: 0.23187\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1911/2000... Step: 1911... Loss: 0.24792... Val Loss: 0.23186\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1912/2000... Step: 1912... Loss: 0.24792... Val Loss: 0.23186\n",
      "Epoch: 1913/2000... Step: 1913... Loss: 0.24792... Val Loss: 0.23185\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1914/2000... Step: 1914... Loss: 0.24791... Val Loss: 0.23185\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1915/2000... Step: 1915... Loss: 0.24791... Val Loss: 0.23185\n",
      "Epoch: 1916/2000... Step: 1916... Loss: 0.24791... Val Loss: 0.23184\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1917/2000... Step: 1917... Loss: 0.24791... Val Loss: 0.23184\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1918/2000... Step: 1918... Loss: 0.24791... Val Loss: 0.23183\n",
      "Epoch: 1919/2000... Step: 1919... Loss: 0.24791... Val Loss: 0.23183\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1920/2000... Step: 1920... Loss: 0.24791... Val Loss: 0.23182\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1921/2000... Step: 1921... Loss: 0.24791... Val Loss: 0.23182\n",
      "Epoch: 1922/2000... Step: 1922... Loss: 0.24791... Val Loss: 0.23182\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1923/2000... Step: 1923... Loss: 0.24790... Val Loss: 0.23181\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1924/2000... Step: 1924... Loss: 0.24790... Val Loss: 0.23181\n",
      "Epoch: 1925/2000... Step: 1925... Loss: 0.24790... Val Loss: 0.23180\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1926/2000... Step: 1926... Loss: 0.24790... Val Loss: 0.23180\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1927/2000... Step: 1927... Loss: 0.24790... Val Loss: 0.23180\n",
      "Epoch: 1928/2000... Step: 1928... Loss: 0.24790... Val Loss: 0.23179\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1929/2000... Step: 1929... Loss: 0.24790... Val Loss: 0.23179\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1930/2000... Step: 1930... Loss: 0.24790... Val Loss: 0.23178\n",
      "Epoch: 1931/2000... Step: 1931... Loss: 0.24789... Val Loss: 0.23178\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1932/2000... Step: 1932... Loss: 0.24789... Val Loss: 0.23178\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1933/2000... Step: 1933... Loss: 0.24789... Val Loss: 0.23177\n",
      "Epoch: 1934/2000... Step: 1934... Loss: 0.24789... Val Loss: 0.23177\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1935/2000... Step: 1935... Loss: 0.24789... Val Loss: 0.23176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1936/2000... Step: 1936... Loss: 0.24789... Val Loss: 0.23176\n",
      "Epoch: 1937/2000... Step: 1937... Loss: 0.24789... Val Loss: 0.23176\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1938/2000... Step: 1938... Loss: 0.24789... Val Loss: 0.23175\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1939/2000... Step: 1939... Loss: 0.24789... Val Loss: 0.23175\n",
      "Epoch: 1940/2000... Step: 1940... Loss: 0.24788... Val Loss: 0.23175\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1941/2000... Step: 1941... Loss: 0.24788... Val Loss: 0.23174\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1942/2000... Step: 1942... Loss: 0.24788... Val Loss: 0.23174\n",
      "Epoch: 1943/2000... Step: 1943... Loss: 0.24788... Val Loss: 0.23173\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1944/2000... Step: 1944... Loss: 0.24788... Val Loss: 0.23173\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1945/2000... Step: 1945... Loss: 0.24788... Val Loss: 0.23173\n",
      "Epoch: 1946/2000... Step: 1946... Loss: 0.24788... Val Loss: 0.23172\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1947/2000... Step: 1947... Loss: 0.24788... Val Loss: 0.23172\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1948/2000... Step: 1948... Loss: 0.24788... Val Loss: 0.23171\n",
      "Epoch: 1949/2000... Step: 1949... Loss: 0.24788... Val Loss: 0.23171\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1950/2000... Step: 1950... Loss: 0.24787... Val Loss: 0.23171\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1951/2000... Step: 1951... Loss: 0.24787... Val Loss: 0.23170\n",
      "Epoch: 1952/2000... Step: 1952... Loss: 0.24787... Val Loss: 0.23170\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1953/2000... Step: 1953... Loss: 0.24787... Val Loss: 0.23170\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1954/2000... Step: 1954... Loss: 0.24787... Val Loss: 0.23169\n",
      "Epoch: 1955/2000... Step: 1955... Loss: 0.24787... Val Loss: 0.23169\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1956/2000... Step: 1956... Loss: 0.24787... Val Loss: 0.23168\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1957/2000... Step: 1957... Loss: 0.24787... Val Loss: 0.23168\n",
      "Epoch: 1958/2000... Step: 1958... Loss: 0.24787... Val Loss: 0.23168\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1959/2000... Step: 1959... Loss: 0.24786... Val Loss: 0.23167\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1960/2000... Step: 1960... Loss: 0.24786... Val Loss: 0.23167\n",
      "Epoch: 1961/2000... Step: 1961... Loss: 0.24786... Val Loss: 0.23167\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1962/2000... Step: 1962... Loss: 0.24786... Val Loss: 0.23166\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1963/2000... Step: 1963... Loss: 0.24786... Val Loss: 0.23166\n",
      "Epoch: 1964/2000... Step: 1964... Loss: 0.24786... Val Loss: 0.23165\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1965/2000... Step: 1965... Loss: 0.24786... Val Loss: 0.23165\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1966/2000... Step: 1966... Loss: 0.24786... Val Loss: 0.23165\n",
      "Epoch: 1967/2000... Step: 1967... Loss: 0.24786... Val Loss: 0.23164\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1968/2000... Step: 1968... Loss: 0.24786... Val Loss: 0.23164\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1969/2000... Step: 1969... Loss: 0.24785... Val Loss: 0.23164\n",
      "Epoch: 1970/2000... Step: 1970... Loss: 0.24785... Val Loss: 0.23163\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1971/2000... Step: 1971... Loss: 0.24785... Val Loss: 0.23163\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1972/2000... Step: 1972... Loss: 0.24785... Val Loss: 0.23162\n",
      "Epoch: 1973/2000... Step: 1973... Loss: 0.24785... Val Loss: 0.23162\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1974/2000... Step: 1974... Loss: 0.24785... Val Loss: 0.23162\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1975/2000... Step: 1975... Loss: 0.24785... Val Loss: 0.23161\n",
      "Epoch: 1976/2000... Step: 1976... Loss: 0.24785... Val Loss: 0.23161\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1977/2000... Step: 1977... Loss: 0.24785... Val Loss: 0.23161\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1978/2000... Step: 1978... Loss: 0.24785... Val Loss: 0.23160\n",
      "Epoch: 1979/2000... Step: 1979... Loss: 0.24785... Val Loss: 0.23160\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1980/2000... Step: 1980... Loss: 0.24784... Val Loss: 0.23160\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1981/2000... Step: 1981... Loss: 0.24784... Val Loss: 0.23159\n",
      "Epoch: 1982/2000... Step: 1982... Loss: 0.24784... Val Loss: 0.23159\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1983/2000... Step: 1983... Loss: 0.24784... Val Loss: 0.23159\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1984/2000... Step: 1984... Loss: 0.24784... Val Loss: 0.23158\n",
      "Epoch: 1985/2000... Step: 1985... Loss: 0.24784... Val Loss: 0.23158\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1986/2000... Step: 1986... Loss: 0.24784... Val Loss: 0.23158\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1987/2000... Step: 1987... Loss: 0.24784... Val Loss: 0.23157\n",
      "Epoch: 1988/2000... Step: 1988... Loss: 0.24784... Val Loss: 0.23157\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1989/2000... Step: 1989... Loss: 0.24784... Val Loss: 0.23156\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1990/2000... Step: 1990... Loss: 0.24783... Val Loss: 0.23156\n",
      "Epoch: 1991/2000... Step: 1991... Loss: 0.24783... Val Loss: 0.23156\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1992/2000... Step: 1992... Loss: 0.24783... Val Loss: 0.23155\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1993/2000... Step: 1993... Loss: 0.24783... Val Loss: 0.23155\n",
      "Epoch: 1994/2000... Step: 1994... Loss: 0.24783... Val Loss: 0.23155\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1995/2000... Step: 1995... Loss: 0.24783... Val Loss: 0.23154\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1996/2000... Step: 1996... Loss: 0.24783... Val Loss: 0.23154\n",
      "Epoch: 1997/2000... Step: 1997... Loss: 0.24783... Val Loss: 0.23154\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1998/2000... Step: 1998... Loss: 0.24783... Val Loss: 0.23153\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1999/2000... Step: 1999... Loss: 0.24783... Val Loss: 0.23153\n",
      "Epoch: 2000/2000... Step: 2000... Loss: 0.24783... Val Loss: 0.23153\n"
     ]
    }
   ],
   "source": [
    "train_losess, val_losses, roc_auc_train, roc_auc_val = train(net, training_generator, validation_generator, verbose=True,\n",
    "                                                             opt_func=opt, criterion_func=criterion, epochs=num_epochs,\n",
    "                                                             lr=0.01, check_early_stopping=True, check_auc_roc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after training on the Test set:\n",
      "0.403\n",
      "Accuracy After training on the Test set:\n",
      "0.84\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss after training on the Test set:\")\n",
    "print(infer(net, test_generator))\n",
    "print(\"Accuracy After training on the Test set:\")\n",
    "y_pred_p = test_accuracy(net, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the plots**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot losses vs epochs\n",
    "The loss function of the train and the validation is getting lower. <br>\n",
    "Also we can see that the validation loss is lower than the train loss. <br>\n",
    "To aviod overfiiting we add for the train a earlystopping function. (now is False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_vs_epochs(train_losess, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot auc vs epochs**\n",
    "\n",
    "**The trend is positive until plateau on the train & validation set. As we can see the validation and the train making the same results. We can assume that the data is very simple and the size of the validation is very small so maybe it is not representing data of the usecase *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_auc_vs_epochs(roc_auc_train, roc_auc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot roc cruve:**\n",
    "\n",
    "You can see that the drift is on the stairs.\n",
    "In each step we can select the leftmost point in the step by preserving the false positive we are willing to allow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test auc is: 0.91\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_cruve(y_test=y_test, y_pred_p=y_pred_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 0.84\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_decision_boundary(x, x_test, y_test, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the decision boundary linear or nonlinear in the case of a logistic regression? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type your answer here:**\n",
    "\n",
    "When applying a sigmoid function on a linear function, we get a non linear function. If we choose  a constant threshold to this outcome - the decision boundary is linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you implemented \"classical\" logistic regression, now you will be implementing a neural network with one or more hidden layers.\n",
    "You will need to choose the number of hidden layers and nodes in a feedforward neural network, activation function, the type of optimizer and its hyperparmeters which will give you the best result. Remember, we don't want to overfit the training data, we want to generalize the solution for new data not seen during training. \n",
    "\n",
    "Plot the same graphs as in the previous sections and explain the similarities and differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator, validation_generator, test_generator = create_generators(x_train, y_train, x_val, y_val, x_test, y_test)\n",
    "net = MlpNetwork(num_net_channels_part_b, nn.Sigmoid(), classifier=True)\n",
    "opt = torch.optim.Adam\n",
    "criterion = nn.BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before train on the Test set:\n",
      "0.6999\n",
      "Accuracy before train:\n",
      "0.57\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss before train on the Test set:\")\n",
    "print(infer(net, test_generator))\n",
    "print(\"Accuracy before train:\")\n",
    "_ = test_accuracy(net, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2000... Step: 1... Loss: 0.72138... Val Loss: 0.72877\n",
      "Epoch: 2/2000... Step: 2... Loss: 0.71296... Val Loss: 0.71479\n",
      "Epoch: 3/2000... Step: 3... Loss: 0.70600... Val Loss: 0.70294\n",
      "Epoch: 4/2000... Step: 4... Loss: 0.70028... Val Loss: 0.69319\n",
      "Epoch: 5/2000... Step: 5... Loss: 0.69536... Val Loss: 0.68518\n",
      "Epoch: 6/2000... Step: 6... Loss: 0.69078... Val Loss: 0.67850\n",
      "Epoch: 7/2000... Step: 7... Loss: 0.68627... Val Loss: 0.67281\n",
      "Epoch: 8/2000... Step: 8... Loss: 0.68170... Val Loss: 0.66787\n",
      "Epoch: 9/2000... Step: 9... Loss: 0.67702... Val Loss: 0.66349\n",
      "Epoch: 10/2000... Step: 10... Loss: 0.67224... Val Loss: 0.65955\n",
      "Epoch: 11/2000... Step: 11... Loss: 0.66742... Val Loss: 0.65594\n",
      "Epoch: 12/2000... Step: 12... Loss: 0.66258... Val Loss: 0.65254\n",
      "Epoch: 13/2000... Step: 13... Loss: 0.65778... Val Loss: 0.64923\n",
      "Epoch: 14/2000... Step: 14... Loss: 0.65301... Val Loss: 0.64590\n",
      "Epoch: 15/2000... Step: 15... Loss: 0.64828... Val Loss: 0.64241\n",
      "Epoch: 16/2000... Step: 16... Loss: 0.64355... Val Loss: 0.63865\n",
      "Epoch: 17/2000... Step: 17... Loss: 0.63880... Val Loss: 0.63452\n",
      "Epoch: 18/2000... Step: 18... Loss: 0.63396... Val Loss: 0.62996\n",
      "Epoch: 19/2000... Step: 19... Loss: 0.62902... Val Loss: 0.62497\n",
      "Epoch: 20/2000... Step: 20... Loss: 0.62393... Val Loss: 0.61956\n",
      "Epoch: 21/2000... Step: 21... Loss: 0.61870... Val Loss: 0.61377\n",
      "Epoch: 22/2000... Step: 22... Loss: 0.61332... Val Loss: 0.60767\n",
      "Epoch: 23/2000... Step: 23... Loss: 0.60780... Val Loss: 0.60132\n",
      "Epoch: 24/2000... Step: 24... Loss: 0.60213... Val Loss: 0.59477\n",
      "Epoch: 25/2000... Step: 25... Loss: 0.59635... Val Loss: 0.58810\n",
      "Epoch: 26/2000... Step: 26... Loss: 0.59044... Val Loss: 0.58133\n",
      "Epoch: 27/2000... Step: 27... Loss: 0.58441... Val Loss: 0.57452\n",
      "Epoch: 28/2000... Step: 28... Loss: 0.57826... Val Loss: 0.56769\n",
      "Epoch: 29/2000... Step: 29... Loss: 0.57200... Val Loss: 0.56086\n",
      "Epoch: 30/2000... Step: 30... Loss: 0.56561... Val Loss: 0.55403\n",
      "Epoch: 31/2000... Step: 31... Loss: 0.55911... Val Loss: 0.54722\n",
      "Epoch: 32/2000... Step: 32... Loss: 0.55248... Val Loss: 0.54042\n",
      "Epoch: 33/2000... Step: 33... Loss: 0.54576... Val Loss: 0.53362\n",
      "Epoch: 34/2000... Step: 34... Loss: 0.53893... Val Loss: 0.52683\n",
      "Epoch: 35/2000... Step: 35... Loss: 0.53203... Val Loss: 0.52003\n",
      "Epoch: 36/2000... Step: 36... Loss: 0.52505... Val Loss: 0.51322\n",
      "Epoch: 37/2000... Step: 37... Loss: 0.51803... Val Loss: 0.50639\n",
      "Epoch: 38/2000... Step: 38... Loss: 0.51097... Val Loss: 0.49954\n",
      "Epoch: 39/2000... Step: 39... Loss: 0.50389... Val Loss: 0.49267\n",
      "Epoch: 40/2000... Step: 40... Loss: 0.49680... Val Loss: 0.48578\n",
      "Epoch: 41/2000... Step: 41... Loss: 0.48973... Val Loss: 0.47888\n",
      "Epoch: 42/2000... Step: 42... Loss: 0.48268... Val Loss: 0.47198\n",
      "Epoch: 43/2000... Step: 43... Loss: 0.47567... Val Loss: 0.46510\n",
      "Epoch: 44/2000... Step: 44... Loss: 0.46872... Val Loss: 0.45826\n",
      "Epoch: 45/2000... Step: 45... Loss: 0.46185... Val Loss: 0.45148\n",
      "Epoch: 46/2000... Step: 46... Loss: 0.45506... Val Loss: 0.44480\n",
      "Epoch: 47/2000... Step: 47... Loss: 0.44837... Val Loss: 0.43823\n",
      "Epoch: 48/2000... Step: 48... Loss: 0.44181... Val Loss: 0.43179\n",
      "Epoch: 49/2000... Step: 49... Loss: 0.43537... Val Loss: 0.42552\n",
      "Epoch: 50/2000... Step: 50... Loss: 0.42909... Val Loss: 0.41942\n",
      "Epoch: 51/2000... Step: 51... Loss: 0.42296... Val Loss: 0.41352\n",
      "Epoch: 52/2000... Step: 52... Loss: 0.41699... Val Loss: 0.40782\n",
      "Epoch: 53/2000... Step: 53... Loss: 0.41119... Val Loss: 0.40233\n",
      "Epoch: 54/2000... Step: 54... Loss: 0.40558... Val Loss: 0.39705\n",
      "Epoch: 55/2000... Step: 55... Loss: 0.40014... Val Loss: 0.39198\n",
      "Epoch: 56/2000... Step: 56... Loss: 0.39490... Val Loss: 0.38712\n",
      "Epoch: 57/2000... Step: 57... Loss: 0.38983... Val Loss: 0.38246\n",
      "Epoch: 58/2000... Step: 58... Loss: 0.38496... Val Loss: 0.37799\n",
      "Epoch: 59/2000... Step: 59... Loss: 0.38027... Val Loss: 0.37371\n",
      "Epoch: 60/2000... Step: 60... Loss: 0.37577... Val Loss: 0.36961\n",
      "Epoch: 61/2000... Step: 61... Loss: 0.37145... Val Loss: 0.36568\n",
      "Epoch: 62/2000... Step: 62... Loss: 0.36731... Val Loss: 0.36191\n",
      "Epoch: 63/2000... Step: 63... Loss: 0.36334... Val Loss: 0.35830\n",
      "Epoch: 64/2000... Step: 64... Loss: 0.35954... Val Loss: 0.35483\n",
      "Epoch: 65/2000... Step: 65... Loss: 0.35589... Val Loss: 0.35151\n",
      "Epoch: 66/2000... Step: 66... Loss: 0.35240... Val Loss: 0.34831\n",
      "Epoch: 67/2000... Step: 67... Loss: 0.34906... Val Loss: 0.34525\n",
      "Epoch: 68/2000... Step: 68... Loss: 0.34585... Val Loss: 0.34231\n",
      "Epoch: 69/2000... Step: 69... Loss: 0.34277... Val Loss: 0.33948\n",
      "Epoch: 70/2000... Step: 70... Loss: 0.33981... Val Loss: 0.33676\n",
      "Epoch: 71/2000... Step: 71... Loss: 0.33698... Val Loss: 0.33414\n",
      "Epoch: 72/2000... Step: 72... Loss: 0.33424... Val Loss: 0.33162\n",
      "Epoch: 73/2000... Step: 73... Loss: 0.33162... Val Loss: 0.32918\n",
      "Epoch: 74/2000... Step: 74... Loss: 0.32908... Val Loss: 0.32682\n",
      "Epoch: 75/2000... Step: 75... Loss: 0.32663... Val Loss: 0.32453\n",
      "Epoch: 76/2000... Step: 76... Loss: 0.32427... Val Loss: 0.32230\n",
      "Epoch: 77/2000... Step: 77... Loss: 0.32198... Val Loss: 0.32013\n",
      "Epoch: 78/2000... Step: 78... Loss: 0.31976... Val Loss: 0.31802\n",
      "Epoch: 79/2000... Step: 79... Loss: 0.31760... Val Loss: 0.31594\n",
      "Epoch: 80/2000... Step: 80... Loss: 0.31551... Val Loss: 0.31391\n",
      "Epoch: 81/2000... Step: 81... Loss: 0.31347... Val Loss: 0.31192\n",
      "Epoch: 82/2000... Step: 82... Loss: 0.31149... Val Loss: 0.30996\n",
      "Epoch: 83/2000... Step: 83... Loss: 0.30956... Val Loss: 0.30803\n",
      "Epoch: 84/2000... Step: 84... Loss: 0.30768... Val Loss: 0.30613\n",
      "Epoch: 85/2000... Step: 85... Loss: 0.30584... Val Loss: 0.30427\n",
      "Epoch: 86/2000... Step: 86... Loss: 0.30404... Val Loss: 0.30243\n",
      "Epoch: 87/2000... Step: 87... Loss: 0.30229... Val Loss: 0.30061\n",
      "Epoch: 88/2000... Step: 88... Loss: 0.30057... Val Loss: 0.29882\n",
      "Epoch: 89/2000... Step: 89... Loss: 0.29890... Val Loss: 0.29705\n",
      "Epoch: 90/2000... Step: 90... Loss: 0.29726... Val Loss: 0.29530\n",
      "Epoch: 91/2000... Step: 91... Loss: 0.29566... Val Loss: 0.29357\n",
      "Epoch: 92/2000... Step: 92... Loss: 0.29409... Val Loss: 0.29186\n",
      "Epoch: 93/2000... Step: 93... Loss: 0.29257... Val Loss: 0.29018\n",
      "Epoch: 94/2000... Step: 94... Loss: 0.29107... Val Loss: 0.28851\n",
      "Epoch: 95/2000... Step: 95... Loss: 0.28962... Val Loss: 0.28686\n",
      "Epoch: 96/2000... Step: 96... Loss: 0.28820... Val Loss: 0.28524\n",
      "Epoch: 97/2000... Step: 97... Loss: 0.28681... Val Loss: 0.28364\n",
      "Epoch: 98/2000... Step: 98... Loss: 0.28546... Val Loss: 0.28206\n",
      "Epoch: 99/2000... Step: 99... Loss: 0.28415... Val Loss: 0.28051\n",
      "Epoch: 100/2000... Step: 100... Loss: 0.28287... Val Loss: 0.27898\n",
      "Epoch: 101/2000... Step: 101... Loss: 0.28163... Val Loss: 0.27748\n",
      "Epoch: 102/2000... Step: 102... Loss: 0.28043... Val Loss: 0.27600\n",
      "Epoch: 103/2000... Step: 103... Loss: 0.27926... Val Loss: 0.27456\n",
      "Epoch: 104/2000... Step: 104... Loss: 0.27812... Val Loss: 0.27314\n",
      "Epoch: 105/2000... Step: 105... Loss: 0.27703... Val Loss: 0.27175\n",
      "Epoch: 106/2000... Step: 106... Loss: 0.27596... Val Loss: 0.27039\n",
      "Epoch: 107/2000... Step: 107... Loss: 0.27494... Val Loss: 0.26906\n",
      "Epoch: 108/2000... Step: 108... Loss: 0.27395... Val Loss: 0.26776\n",
      "Epoch: 109/2000... Step: 109... Loss: 0.27299... Val Loss: 0.26649\n",
      "Epoch: 110/2000... Step: 110... Loss: 0.27207... Val Loss: 0.26525\n",
      "Epoch: 111/2000... Step: 111... Loss: 0.27118... Val Loss: 0.26404\n",
      "Epoch: 112/2000... Step: 112... Loss: 0.27032... Val Loss: 0.26287\n",
      "Epoch: 113/2000... Step: 113... Loss: 0.26950... Val Loss: 0.26172\n",
      "Epoch: 114/2000... Step: 114... Loss: 0.26870... Val Loss: 0.26061\n",
      "Epoch: 115/2000... Step: 115... Loss: 0.26794... Val Loss: 0.25952\n",
      "Epoch: 116/2000... Step: 116... Loss: 0.26721... Val Loss: 0.25848\n",
      "Epoch: 117/2000... Step: 117... Loss: 0.26651... Val Loss: 0.25746\n",
      "Epoch: 118/2000... Step: 118... Loss: 0.26583... Val Loss: 0.25648\n",
      "Epoch: 119/2000... Step: 119... Loss: 0.26518... Val Loss: 0.25553\n",
      "Epoch: 120/2000... Step: 120... Loss: 0.26456... Val Loss: 0.25462\n",
      "Epoch: 121/2000... Step: 121... Loss: 0.26396... Val Loss: 0.25373\n",
      "Epoch: 122/2000... Step: 122... Loss: 0.26339... Val Loss: 0.25288\n",
      "Epoch: 123/2000... Step: 123... Loss: 0.26284... Val Loss: 0.25206\n",
      "Epoch: 124/2000... Step: 124... Loss: 0.26231... Val Loss: 0.25127\n",
      "Epoch: 125/2000... Step: 125... Loss: 0.26181... Val Loss: 0.25050\n",
      "Epoch: 126/2000... Step: 126... Loss: 0.26132... Val Loss: 0.24977\n",
      "Epoch: 127/2000... Step: 127... Loss: 0.26085... Val Loss: 0.24907\n",
      "Epoch: 128/2000... Step: 128... Loss: 0.26041... Val Loss: 0.24839\n",
      "Epoch: 129/2000... Step: 129... Loss: 0.25998... Val Loss: 0.24774\n",
      "Epoch: 130/2000... Step: 130... Loss: 0.25956... Val Loss: 0.24712\n",
      "Epoch: 131/2000... Step: 131... Loss: 0.25917... Val Loss: 0.24652\n",
      "Epoch: 132/2000... Step: 132... Loss: 0.25879... Val Loss: 0.24595\n",
      "Epoch: 133/2000... Step: 133... Loss: 0.25842... Val Loss: 0.24541\n",
      "Epoch: 134/2000... Step: 134... Loss: 0.25807... Val Loss: 0.24489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 135/2000... Step: 135... Loss: 0.25774... Val Loss: 0.24440\n",
      "Epoch: 136/2000... Step: 136... Loss: 0.25741... Val Loss: 0.24392\n",
      "Epoch: 137/2000... Step: 137... Loss: 0.25710... Val Loss: 0.24347\n",
      "Epoch: 138/2000... Step: 138... Loss: 0.25681... Val Loss: 0.24304\n",
      "Epoch: 139/2000... Step: 139... Loss: 0.25652... Val Loss: 0.24263\n",
      "Epoch: 140/2000... Step: 140... Loss: 0.25625... Val Loss: 0.24223\n",
      "Epoch: 141/2000... Step: 141... Loss: 0.25599... Val Loss: 0.24185\n",
      "Epoch: 142/2000... Step: 142... Loss: 0.25573... Val Loss: 0.24149\n",
      "Epoch: 143/2000... Step: 143... Loss: 0.25549... Val Loss: 0.24115\n",
      "Epoch: 144/2000... Step: 144... Loss: 0.25526... Val Loss: 0.24082\n",
      "Epoch: 145/2000... Step: 145... Loss: 0.25504... Val Loss: 0.24050\n",
      "Epoch: 146/2000... Step: 146... Loss: 0.25483... Val Loss: 0.24020\n",
      "Epoch: 147/2000... Step: 147... Loss: 0.25463... Val Loss: 0.23991\n",
      "Epoch: 148/2000... Step: 148... Loss: 0.25444... Val Loss: 0.23964\n",
      "Epoch: 149/2000... Step: 149... Loss: 0.25425... Val Loss: 0.23937\n",
      "Epoch: 150/2000... Step: 150... Loss: 0.25407... Val Loss: 0.23911\n",
      "Epoch: 151/2000... Step: 151... Loss: 0.25390... Val Loss: 0.23887\n",
      "Epoch: 152/2000... Step: 152... Loss: 0.25374... Val Loss: 0.23863\n",
      "Epoch: 153/2000... Step: 153... Loss: 0.25359... Val Loss: 0.23840\n",
      "Epoch: 154/2000... Step: 154... Loss: 0.25344... Val Loss: 0.23818\n",
      "Epoch: 155/2000... Step: 155... Loss: 0.25330... Val Loss: 0.23796\n",
      "Epoch: 156/2000... Step: 156... Loss: 0.25316... Val Loss: 0.23776\n",
      "Epoch: 157/2000... Step: 157... Loss: 0.25303... Val Loss: 0.23755\n",
      "Epoch: 158/2000... Step: 158... Loss: 0.25291... Val Loss: 0.23736\n",
      "Epoch: 159/2000... Step: 159... Loss: 0.25279... Val Loss: 0.23717\n",
      "Epoch: 160/2000... Step: 160... Loss: 0.25268... Val Loss: 0.23698\n",
      "Epoch: 161/2000... Step: 161... Loss: 0.25257... Val Loss: 0.23680\n",
      "Epoch: 162/2000... Step: 162... Loss: 0.25246... Val Loss: 0.23663\n",
      "Epoch: 163/2000... Step: 163... Loss: 0.25236... Val Loss: 0.23646\n",
      "Epoch: 164/2000... Step: 164... Loss: 0.25227... Val Loss: 0.23629\n",
      "Epoch: 165/2000... Step: 165... Loss: 0.25218... Val Loss: 0.23613\n",
      "Epoch: 166/2000... Step: 166... Loss: 0.25209... Val Loss: 0.23597\n",
      "Epoch: 167/2000... Step: 167... Loss: 0.25201... Val Loss: 0.23581\n",
      "Epoch: 168/2000... Step: 168... Loss: 0.25193... Val Loss: 0.23566\n",
      "Epoch: 169/2000... Step: 169... Loss: 0.25185... Val Loss: 0.23551\n",
      "Epoch: 170/2000... Step: 170... Loss: 0.25178... Val Loss: 0.23537\n",
      "Epoch: 171/2000... Step: 171... Loss: 0.25171... Val Loss: 0.23523\n",
      "Epoch: 172/2000... Step: 172... Loss: 0.25164... Val Loss: 0.23509\n",
      "Epoch: 173/2000... Step: 173... Loss: 0.25157... Val Loss: 0.23496\n",
      "Epoch: 174/2000... Step: 174... Loss: 0.25151... Val Loss: 0.23483\n",
      "Epoch: 175/2000... Step: 175... Loss: 0.25145... Val Loss: 0.23470\n",
      "Epoch: 176/2000... Step: 176... Loss: 0.25139... Val Loss: 0.23458\n",
      "Epoch: 177/2000... Step: 177... Loss: 0.25134... Val Loss: 0.23446\n",
      "Epoch: 178/2000... Step: 178... Loss: 0.25128... Val Loss: 0.23435\n",
      "Epoch: 179/2000... Step: 179... Loss: 0.25123... Val Loss: 0.23424\n",
      "Epoch: 180/2000... Step: 180... Loss: 0.25118... Val Loss: 0.23413\n",
      "Epoch: 181/2000... Step: 181... Loss: 0.25114... Val Loss: 0.23402\n",
      "Epoch: 182/2000... Step: 182... Loss: 0.25109... Val Loss: 0.23392\n",
      "Epoch: 183/2000... Step: 183... Loss: 0.25105... Val Loss: 0.23382\n",
      "Epoch: 184/2000... Step: 184... Loss: 0.25100... Val Loss: 0.23373\n",
      "Epoch: 185/2000... Step: 185... Loss: 0.25096... Val Loss: 0.23364\n",
      "Epoch: 186/2000... Step: 186... Loss: 0.25092... Val Loss: 0.23355\n",
      "Epoch: 187/2000... Step: 187... Loss: 0.25088... Val Loss: 0.23347\n",
      "Epoch: 188/2000... Step: 188... Loss: 0.25085... Val Loss: 0.23338\n",
      "Epoch: 189/2000... Step: 189... Loss: 0.25081... Val Loss: 0.23331\n",
      "Epoch: 190/2000... Step: 190... Loss: 0.25077... Val Loss: 0.23323\n",
      "Epoch: 191/2000... Step: 191... Loss: 0.25074... Val Loss: 0.23316\n",
      "Epoch: 192/2000... Step: 192... Loss: 0.25071... Val Loss: 0.23309\n",
      "Epoch: 193/2000... Step: 193... Loss: 0.25067... Val Loss: 0.23302\n",
      "Epoch: 194/2000... Step: 194... Loss: 0.25064... Val Loss: 0.23295\n",
      "Epoch: 195/2000... Step: 195... Loss: 0.25061... Val Loss: 0.23289\n",
      "Epoch: 196/2000... Step: 196... Loss: 0.25058... Val Loss: 0.23283\n",
      "Epoch: 197/2000... Step: 197... Loss: 0.25055... Val Loss: 0.23277\n",
      "Epoch: 198/2000... Step: 198... Loss: 0.25052... Val Loss: 0.23272\n",
      "Epoch: 199/2000... Step: 199... Loss: 0.25050... Val Loss: 0.23266\n",
      "Epoch: 200/2000... Step: 200... Loss: 0.25047... Val Loss: 0.23261\n",
      "Epoch: 201/2000... Step: 201... Loss: 0.25044... Val Loss: 0.23256\n",
      "Epoch: 202/2000... Step: 202... Loss: 0.25041... Val Loss: 0.23251\n",
      "Epoch: 203/2000... Step: 203... Loss: 0.25039... Val Loss: 0.23246\n",
      "Epoch: 204/2000... Step: 204... Loss: 0.25036... Val Loss: 0.23241\n",
      "Epoch: 205/2000... Step: 205... Loss: 0.25034... Val Loss: 0.23237\n",
      "Epoch: 206/2000... Step: 206... Loss: 0.25031... Val Loss: 0.23232\n",
      "Epoch: 207/2000... Step: 207... Loss: 0.25029... Val Loss: 0.23228\n",
      "Epoch: 208/2000... Step: 208... Loss: 0.25026... Val Loss: 0.23224\n",
      "Epoch: 209/2000... Step: 209... Loss: 0.25024... Val Loss: 0.23220\n",
      "Epoch: 210/2000... Step: 210... Loss: 0.25021... Val Loss: 0.23216\n",
      "Epoch: 211/2000... Step: 211... Loss: 0.25019... Val Loss: 0.23212\n",
      "Epoch: 212/2000... Step: 212... Loss: 0.25017... Val Loss: 0.23208\n",
      "Epoch: 213/2000... Step: 213... Loss: 0.25014... Val Loss: 0.23204\n",
      "Epoch: 214/2000... Step: 214... Loss: 0.25012... Val Loss: 0.23200\n",
      "Epoch: 215/2000... Step: 215... Loss: 0.25010... Val Loss: 0.23196\n",
      "Epoch: 216/2000... Step: 216... Loss: 0.25007... Val Loss: 0.23193\n",
      "Epoch: 217/2000... Step: 217... Loss: 0.25005... Val Loss: 0.23189\n",
      "Epoch: 218/2000... Step: 218... Loss: 0.25003... Val Loss: 0.23185\n",
      "Epoch: 219/2000... Step: 219... Loss: 0.25000... Val Loss: 0.23182\n",
      "Epoch: 220/2000... Step: 220... Loss: 0.24998... Val Loss: 0.23178\n",
      "Epoch: 221/2000... Step: 221... Loss: 0.24996... Val Loss: 0.23175\n",
      "Epoch: 222/2000... Step: 222... Loss: 0.24993... Val Loss: 0.23171\n",
      "Epoch: 223/2000... Step: 223... Loss: 0.24991... Val Loss: 0.23167\n",
      "Epoch: 224/2000... Step: 224... Loss: 0.24989... Val Loss: 0.23164\n",
      "Epoch: 225/2000... Step: 225... Loss: 0.24986... Val Loss: 0.23160\n",
      "Epoch: 226/2000... Step: 226... Loss: 0.24984... Val Loss: 0.23157\n",
      "Epoch: 227/2000... Step: 227... Loss: 0.24982... Val Loss: 0.23153\n",
      "Epoch: 228/2000... Step: 228... Loss: 0.24979... Val Loss: 0.23149\n",
      "Epoch: 229/2000... Step: 229... Loss: 0.24977... Val Loss: 0.23146\n",
      "Epoch: 230/2000... Step: 230... Loss: 0.24974... Val Loss: 0.23142\n",
      "Epoch: 231/2000... Step: 231... Loss: 0.24972... Val Loss: 0.23138\n",
      "Epoch: 232/2000... Step: 232... Loss: 0.24969... Val Loss: 0.23134\n",
      "Epoch: 233/2000... Step: 233... Loss: 0.24967... Val Loss: 0.23130\n",
      "Epoch: 234/2000... Step: 234... Loss: 0.24964... Val Loss: 0.23127\n",
      "Epoch: 235/2000... Step: 235... Loss: 0.24961... Val Loss: 0.23123\n",
      "Epoch: 236/2000... Step: 236... Loss: 0.24959... Val Loss: 0.23119\n",
      "Epoch: 237/2000... Step: 237... Loss: 0.24956... Val Loss: 0.23114\n",
      "Epoch: 238/2000... Step: 238... Loss: 0.24953... Val Loss: 0.23110\n",
      "Epoch: 239/2000... Step: 239... Loss: 0.24950... Val Loss: 0.23106\n",
      "Epoch: 240/2000... Step: 240... Loss: 0.24947... Val Loss: 0.23102\n",
      "Epoch: 241/2000... Step: 241... Loss: 0.24944... Val Loss: 0.23097\n",
      "Epoch: 242/2000... Step: 242... Loss: 0.24941... Val Loss: 0.23093\n",
      "Epoch: 243/2000... Step: 243... Loss: 0.24938... Val Loss: 0.23088\n",
      "Epoch: 244/2000... Step: 244... Loss: 0.24935... Val Loss: 0.23083\n",
      "Epoch: 245/2000... Step: 245... Loss: 0.24932... Val Loss: 0.23078\n",
      "Epoch: 246/2000... Step: 246... Loss: 0.24928... Val Loss: 0.23073\n",
      "Epoch: 247/2000... Step: 247... Loss: 0.24925... Val Loss: 0.23067\n",
      "Epoch: 248/2000... Step: 248... Loss: 0.24921... Val Loss: 0.23062\n",
      "Epoch: 249/2000... Step: 249... Loss: 0.24917... Val Loss: 0.23056\n",
      "Epoch: 250/2000... Step: 250... Loss: 0.24913... Val Loss: 0.23050\n",
      "Epoch: 251/2000... Step: 251... Loss: 0.24909... Val Loss: 0.23044\n",
      "Epoch: 252/2000... Step: 252... Loss: 0.24905... Val Loss: 0.23038\n",
      "Epoch: 253/2000... Step: 253... Loss: 0.24901... Val Loss: 0.23031\n",
      "Epoch: 254/2000... Step: 254... Loss: 0.24896... Val Loss: 0.23025\n",
      "Epoch: 255/2000... Step: 255... Loss: 0.24891... Val Loss: 0.23018\n",
      "Epoch: 256/2000... Step: 256... Loss: 0.24887... Val Loss: 0.23010\n",
      "Epoch: 257/2000... Step: 257... Loss: 0.24881... Val Loss: 0.23003\n",
      "Epoch: 258/2000... Step: 258... Loss: 0.24876... Val Loss: 0.22995\n",
      "Epoch: 259/2000... Step: 259... Loss: 0.24870... Val Loss: 0.22987\n",
      "Epoch: 260/2000... Step: 260... Loss: 0.24864... Val Loss: 0.22978\n",
      "Epoch: 261/2000... Step: 261... Loss: 0.24858... Val Loss: 0.22969\n",
      "Epoch: 262/2000... Step: 262... Loss: 0.24852... Val Loss: 0.22960\n",
      "Epoch: 263/2000... Step: 263... Loss: 0.24845... Val Loss: 0.22951\n",
      "Epoch: 264/2000... Step: 264... Loss: 0.24838... Val Loss: 0.22941\n",
      "Epoch: 265/2000... Step: 265... Loss: 0.24831... Val Loss: 0.22931\n",
      "Epoch: 266/2000... Step: 266... Loss: 0.24823... Val Loss: 0.22921\n",
      "Epoch: 267/2000... Step: 267... Loss: 0.24815... Val Loss: 0.22910\n",
      "Epoch: 268/2000... Step: 268... Loss: 0.24806... Val Loss: 0.22899\n",
      "Epoch: 269/2000... Step: 269... Loss: 0.24797... Val Loss: 0.22888\n",
      "Epoch: 270/2000... Step: 270... Loss: 0.24788... Val Loss: 0.22876\n",
      "Epoch: 271/2000... Step: 271... Loss: 0.24778... Val Loss: 0.22864\n",
      "Epoch: 272/2000... Step: 272... Loss: 0.24767... Val Loss: 0.22852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 273/2000... Step: 273... Loss: 0.24757... Val Loss: 0.22839\n",
      "Epoch: 274/2000... Step: 274... Loss: 0.24746... Val Loss: 0.22827\n",
      "Epoch: 275/2000... Step: 275... Loss: 0.24734... Val Loss: 0.22814\n",
      "Epoch: 276/2000... Step: 276... Loss: 0.24722... Val Loss: 0.22801\n",
      "Epoch: 277/2000... Step: 277... Loss: 0.24710... Val Loss: 0.22787\n",
      "Epoch: 278/2000... Step: 278... Loss: 0.24697... Val Loss: 0.22774\n",
      "Epoch: 279/2000... Step: 279... Loss: 0.24683... Val Loss: 0.22760\n",
      "Epoch: 280/2000... Step: 280... Loss: 0.24669... Val Loss: 0.22746\n",
      "Epoch: 281/2000... Step: 281... Loss: 0.24655... Val Loss: 0.22732\n",
      "Epoch: 282/2000... Step: 282... Loss: 0.24640... Val Loss: 0.22718\n",
      "Epoch: 283/2000... Step: 283... Loss: 0.24625... Val Loss: 0.22703\n",
      "Epoch: 284/2000... Step: 284... Loss: 0.24610... Val Loss: 0.22688\n",
      "Epoch: 285/2000... Step: 285... Loss: 0.24594... Val Loss: 0.22673\n",
      "Epoch: 286/2000... Step: 286... Loss: 0.24577... Val Loss: 0.22658\n",
      "Epoch: 287/2000... Step: 287... Loss: 0.24561... Val Loss: 0.22642\n",
      "Epoch: 288/2000... Step: 288... Loss: 0.24544... Val Loss: 0.22626\n",
      "Epoch: 289/2000... Step: 289... Loss: 0.24526... Val Loss: 0.22610\n",
      "Epoch: 290/2000... Step: 290... Loss: 0.24508... Val Loss: 0.22593\n",
      "Epoch: 291/2000... Step: 291... Loss: 0.24490... Val Loss: 0.22576\n",
      "Epoch: 292/2000... Step: 292... Loss: 0.24471... Val Loss: 0.22558\n",
      "Epoch: 293/2000... Step: 293... Loss: 0.24451... Val Loss: 0.22540\n",
      "Epoch: 294/2000... Step: 294... Loss: 0.24432... Val Loss: 0.22521\n",
      "Epoch: 295/2000... Step: 295... Loss: 0.24412... Val Loss: 0.22501\n",
      "Epoch: 296/2000... Step: 296... Loss: 0.24391... Val Loss: 0.22481\n",
      "Epoch: 297/2000... Step: 297... Loss: 0.24370... Val Loss: 0.22461\n",
      "Epoch: 298/2000... Step: 298... Loss: 0.24349... Val Loss: 0.22439\n",
      "Epoch: 299/2000... Step: 299... Loss: 0.24327... Val Loss: 0.22417\n",
      "Epoch: 300/2000... Step: 300... Loss: 0.24305... Val Loss: 0.22395\n",
      "Epoch: 301/2000... Step: 301... Loss: 0.24282... Val Loss: 0.22372\n",
      "Epoch: 302/2000... Step: 302... Loss: 0.24259... Val Loss: 0.22348\n",
      "Epoch: 303/2000... Step: 303... Loss: 0.24236... Val Loss: 0.22323\n",
      "Epoch: 304/2000... Step: 304... Loss: 0.24212... Val Loss: 0.22299\n",
      "Epoch: 305/2000... Step: 305... Loss: 0.24188... Val Loss: 0.22273\n",
      "Epoch: 306/2000... Step: 306... Loss: 0.24163... Val Loss: 0.22247\n",
      "Epoch: 307/2000... Step: 307... Loss: 0.24138... Val Loss: 0.22221\n",
      "Epoch: 308/2000... Step: 308... Loss: 0.24113... Val Loss: 0.22194\n",
      "Epoch: 309/2000... Step: 309... Loss: 0.24087... Val Loss: 0.22166\n",
      "Epoch: 310/2000... Step: 310... Loss: 0.24061... Val Loss: 0.22138\n",
      "Epoch: 311/2000... Step: 311... Loss: 0.24035... Val Loss: 0.22110\n",
      "Epoch: 312/2000... Step: 312... Loss: 0.24008... Val Loss: 0.22081\n",
      "Epoch: 313/2000... Step: 313... Loss: 0.23981... Val Loss: 0.22052\n",
      "Epoch: 314/2000... Step: 314... Loss: 0.23954... Val Loss: 0.22023\n",
      "Epoch: 315/2000... Step: 315... Loss: 0.23926... Val Loss: 0.21993\n",
      "Epoch: 316/2000... Step: 316... Loss: 0.23898... Val Loss: 0.21963\n",
      "Epoch: 317/2000... Step: 317... Loss: 0.23869... Val Loss: 0.21933\n",
      "Epoch: 318/2000... Step: 318... Loss: 0.23840... Val Loss: 0.21902\n",
      "Epoch: 319/2000... Step: 319... Loss: 0.23811... Val Loss: 0.21871\n",
      "Epoch: 320/2000... Step: 320... Loss: 0.23781... Val Loss: 0.21840\n",
      "Epoch: 321/2000... Step: 321... Loss: 0.23751... Val Loss: 0.21808\n",
      "Epoch: 322/2000... Step: 322... Loss: 0.23721... Val Loss: 0.21777\n",
      "Epoch: 323/2000... Step: 323... Loss: 0.23690... Val Loss: 0.21745\n",
      "Epoch: 324/2000... Step: 324... Loss: 0.23659... Val Loss: 0.21712\n",
      "Epoch: 325/2000... Step: 325... Loss: 0.23628... Val Loss: 0.21679\n",
      "Epoch: 326/2000... Step: 326... Loss: 0.23596... Val Loss: 0.21646\n",
      "Epoch: 327/2000... Step: 327... Loss: 0.23564... Val Loss: 0.21613\n",
      "Epoch: 328/2000... Step: 328... Loss: 0.23531... Val Loss: 0.21580\n",
      "Epoch: 329/2000... Step: 329... Loss: 0.23499... Val Loss: 0.21546\n",
      "Epoch: 330/2000... Step: 330... Loss: 0.23466... Val Loss: 0.21511\n",
      "Epoch: 331/2000... Step: 331... Loss: 0.23432... Val Loss: 0.21477\n",
      "Epoch: 332/2000... Step: 332... Loss: 0.23399... Val Loss: 0.21442\n",
      "Epoch: 333/2000... Step: 333... Loss: 0.23365... Val Loss: 0.21407\n",
      "Epoch: 334/2000... Step: 334... Loss: 0.23330... Val Loss: 0.21371\n",
      "Epoch: 335/2000... Step: 335... Loss: 0.23296... Val Loss: 0.21336\n",
      "Epoch: 336/2000... Step: 336... Loss: 0.23261... Val Loss: 0.21299\n",
      "Epoch: 337/2000... Step: 337... Loss: 0.23226... Val Loss: 0.21263\n",
      "Epoch: 338/2000... Step: 338... Loss: 0.23190... Val Loss: 0.21226\n",
      "Epoch: 339/2000... Step: 339... Loss: 0.23155... Val Loss: 0.21190\n",
      "Epoch: 340/2000... Step: 340... Loss: 0.23119... Val Loss: 0.21152\n",
      "Epoch: 341/2000... Step: 341... Loss: 0.23082... Val Loss: 0.21115\n",
      "Epoch: 342/2000... Step: 342... Loss: 0.23046... Val Loss: 0.21077\n",
      "Epoch: 343/2000... Step: 343... Loss: 0.23009... Val Loss: 0.21040\n",
      "Epoch: 344/2000... Step: 344... Loss: 0.22972... Val Loss: 0.21002\n",
      "Epoch: 345/2000... Step: 345... Loss: 0.22935... Val Loss: 0.20963\n",
      "Epoch: 346/2000... Step: 346... Loss: 0.22897... Val Loss: 0.20925\n",
      "Epoch: 347/2000... Step: 347... Loss: 0.22859... Val Loss: 0.20887\n",
      "Epoch: 348/2000... Step: 348... Loss: 0.22821... Val Loss: 0.20848\n",
      "Epoch: 349/2000... Step: 349... Loss: 0.22783... Val Loss: 0.20809\n",
      "Epoch: 350/2000... Step: 350... Loss: 0.22744... Val Loss: 0.20770\n",
      "Epoch: 351/2000... Step: 351... Loss: 0.22706... Val Loss: 0.20731\n",
      "Epoch: 352/2000... Step: 352... Loss: 0.22667... Val Loss: 0.20692\n",
      "Epoch: 353/2000... Step: 353... Loss: 0.22628... Val Loss: 0.20653\n",
      "Epoch: 354/2000... Step: 354... Loss: 0.22588... Val Loss: 0.20614\n",
      "Epoch: 355/2000... Step: 355... Loss: 0.22549... Val Loss: 0.20574\n",
      "Epoch: 356/2000... Step: 356... Loss: 0.22509... Val Loss: 0.20535\n",
      "Epoch: 357/2000... Step: 357... Loss: 0.22470... Val Loss: 0.20495\n",
      "Epoch: 358/2000... Step: 358... Loss: 0.22430... Val Loss: 0.20455\n",
      "Epoch: 359/2000... Step: 359... Loss: 0.22389... Val Loss: 0.20415\n",
      "Epoch: 360/2000... Step: 360... Loss: 0.22349... Val Loss: 0.20376\n",
      "Epoch: 361/2000... Step: 361... Loss: 0.22309... Val Loss: 0.20335\n",
      "Epoch: 362/2000... Step: 362... Loss: 0.22268... Val Loss: 0.20295\n",
      "Epoch: 363/2000... Step: 363... Loss: 0.22227... Val Loss: 0.20255\n",
      "Epoch: 364/2000... Step: 364... Loss: 0.22186... Val Loss: 0.20215\n",
      "Epoch: 365/2000... Step: 365... Loss: 0.22145... Val Loss: 0.20174\n",
      "Epoch: 366/2000... Step: 366... Loss: 0.22104... Val Loss: 0.20134\n",
      "Epoch: 367/2000... Step: 367... Loss: 0.22063... Val Loss: 0.20093\n",
      "Epoch: 368/2000... Step: 368... Loss: 0.22022... Val Loss: 0.20053\n",
      "Epoch: 369/2000... Step: 369... Loss: 0.21980... Val Loss: 0.20012\n",
      "Epoch: 370/2000... Step: 370... Loss: 0.21939... Val Loss: 0.19972\n",
      "Epoch: 371/2000... Step: 371... Loss: 0.21897... Val Loss: 0.19931\n",
      "Epoch: 372/2000... Step: 372... Loss: 0.21855... Val Loss: 0.19891\n",
      "Epoch: 373/2000... Step: 373... Loss: 0.21814... Val Loss: 0.19850\n",
      "Epoch: 374/2000... Step: 374... Loss: 0.21772... Val Loss: 0.19809\n",
      "Epoch: 375/2000... Step: 375... Loss: 0.21730... Val Loss: 0.19769\n",
      "Epoch: 376/2000... Step: 376... Loss: 0.21688... Val Loss: 0.19728\n",
      "Epoch: 377/2000... Step: 377... Loss: 0.21646... Val Loss: 0.19688\n",
      "Epoch: 378/2000... Step: 378... Loss: 0.21603... Val Loss: 0.19647\n",
      "Epoch: 379/2000... Step: 379... Loss: 0.21561... Val Loss: 0.19607\n",
      "Epoch: 380/2000... Step: 380... Loss: 0.21519... Val Loss: 0.19566\n",
      "Epoch: 381/2000... Step: 381... Loss: 0.21477... Val Loss: 0.19526\n",
      "Epoch: 382/2000... Step: 382... Loss: 0.21434... Val Loss: 0.19485\n",
      "Epoch: 383/2000... Step: 383... Loss: 0.21392... Val Loss: 0.19445\n",
      "Epoch: 384/2000... Step: 384... Loss: 0.21350... Val Loss: 0.19404\n",
      "Epoch: 385/2000... Step: 385... Loss: 0.21307... Val Loss: 0.19364\n",
      "Epoch: 386/2000... Step: 386... Loss: 0.21265... Val Loss: 0.19324\n",
      "Epoch: 387/2000... Step: 387... Loss: 0.21222... Val Loss: 0.19283\n",
      "Epoch: 388/2000... Step: 388... Loss: 0.21180... Val Loss: 0.19243\n",
      "Epoch: 389/2000... Step: 389... Loss: 0.21138... Val Loss: 0.19203\n",
      "Epoch: 390/2000... Step: 390... Loss: 0.21095... Val Loss: 0.19163\n",
      "Epoch: 391/2000... Step: 391... Loss: 0.21053... Val Loss: 0.19123\n",
      "Epoch: 392/2000... Step: 392... Loss: 0.21010... Val Loss: 0.19083\n",
      "Epoch: 393/2000... Step: 393... Loss: 0.20968... Val Loss: 0.19043\n",
      "Epoch: 394/2000... Step: 394... Loss: 0.20925... Val Loss: 0.19003\n",
      "Epoch: 395/2000... Step: 395... Loss: 0.20883... Val Loss: 0.18963\n",
      "Epoch: 396/2000... Step: 396... Loss: 0.20840... Val Loss: 0.18923\n",
      "Epoch: 397/2000... Step: 397... Loss: 0.20798... Val Loss: 0.18883\n",
      "Epoch: 398/2000... Step: 398... Loss: 0.20755... Val Loss: 0.18843\n",
      "Epoch: 399/2000... Step: 399... Loss: 0.20713... Val Loss: 0.18803\n",
      "Epoch: 400/2000... Step: 400... Loss: 0.20670... Val Loss: 0.18764\n",
      "Epoch: 401/2000... Step: 401... Loss: 0.20628... Val Loss: 0.18724\n",
      "Epoch: 402/2000... Step: 402... Loss: 0.20585... Val Loss: 0.18685\n",
      "Epoch: 403/2000... Step: 403... Loss: 0.20543... Val Loss: 0.18645\n",
      "Epoch: 404/2000... Step: 404... Loss: 0.20501... Val Loss: 0.18606\n",
      "Epoch: 405/2000... Step: 405... Loss: 0.20458... Val Loss: 0.18567\n",
      "Epoch: 406/2000... Step: 406... Loss: 0.20416... Val Loss: 0.18528\n",
      "Epoch: 407/2000... Step: 407... Loss: 0.20374... Val Loss: 0.18489\n",
      "Epoch: 408/2000... Step: 408... Loss: 0.20332... Val Loss: 0.18449\n",
      "Epoch: 409/2000... Step: 409... Loss: 0.20289... Val Loss: 0.18410\n",
      "Epoch: 410/2000... Step: 410... Loss: 0.20247... Val Loss: 0.18372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 411/2000... Step: 411... Loss: 0.20205... Val Loss: 0.18333\n",
      "Epoch: 412/2000... Step: 412... Loss: 0.20163... Val Loss: 0.18294\n",
      "Epoch: 413/2000... Step: 413... Loss: 0.20121... Val Loss: 0.18255\n",
      "Epoch: 414/2000... Step: 414... Loss: 0.20079... Val Loss: 0.18217\n",
      "Epoch: 415/2000... Step: 415... Loss: 0.20037... Val Loss: 0.18178\n",
      "Epoch: 416/2000... Step: 416... Loss: 0.19995... Val Loss: 0.18140\n",
      "Epoch: 417/2000... Step: 417... Loss: 0.19953... Val Loss: 0.18101\n",
      "Epoch: 418/2000... Step: 418... Loss: 0.19911... Val Loss: 0.18063\n",
      "Epoch: 419/2000... Step: 419... Loss: 0.19869... Val Loss: 0.18024\n",
      "Epoch: 420/2000... Step: 420... Loss: 0.19828... Val Loss: 0.17986\n",
      "Epoch: 421/2000... Step: 421... Loss: 0.19786... Val Loss: 0.17948\n",
      "Epoch: 422/2000... Step: 422... Loss: 0.19744... Val Loss: 0.17910\n",
      "Epoch: 423/2000... Step: 423... Loss: 0.19703... Val Loss: 0.17872\n",
      "Epoch: 424/2000... Step: 424... Loss: 0.19661... Val Loss: 0.17834\n",
      "Epoch: 425/2000... Step: 425... Loss: 0.19620... Val Loss: 0.17796\n",
      "Epoch: 426/2000... Step: 426... Loss: 0.19578... Val Loss: 0.17758\n",
      "Epoch: 427/2000... Step: 427... Loss: 0.19537... Val Loss: 0.17720\n",
      "Epoch: 428/2000... Step: 428... Loss: 0.19495... Val Loss: 0.17682\n",
      "Epoch: 429/2000... Step: 429... Loss: 0.19454... Val Loss: 0.17644\n",
      "Epoch: 430/2000... Step: 430... Loss: 0.19413... Val Loss: 0.17607\n",
      "Epoch: 431/2000... Step: 431... Loss: 0.19372... Val Loss: 0.17569\n",
      "Epoch: 432/2000... Step: 432... Loss: 0.19330... Val Loss: 0.17531\n",
      "Epoch: 433/2000... Step: 433... Loss: 0.19289... Val Loss: 0.17494\n",
      "Epoch: 434/2000... Step: 434... Loss: 0.19248... Val Loss: 0.17456\n",
      "Epoch: 435/2000... Step: 435... Loss: 0.19207... Val Loss: 0.17419\n",
      "Epoch: 436/2000... Step: 436... Loss: 0.19166... Val Loss: 0.17381\n",
      "Epoch: 437/2000... Step: 437... Loss: 0.19125... Val Loss: 0.17343\n",
      "Epoch: 438/2000... Step: 438... Loss: 0.19084... Val Loss: 0.17306\n",
      "Epoch: 439/2000... Step: 439... Loss: 0.19044... Val Loss: 0.17269\n",
      "Epoch: 440/2000... Step: 440... Loss: 0.19003... Val Loss: 0.17231\n",
      "Epoch: 441/2000... Step: 441... Loss: 0.18962... Val Loss: 0.17194\n",
      "Epoch: 442/2000... Step: 442... Loss: 0.18921... Val Loss: 0.17156\n",
      "Epoch: 443/2000... Step: 443... Loss: 0.18881... Val Loss: 0.17119\n",
      "Epoch: 444/2000... Step: 444... Loss: 0.18840... Val Loss: 0.17081\n",
      "Epoch: 445/2000... Step: 445... Loss: 0.18799... Val Loss: 0.17044\n",
      "Epoch: 446/2000... Step: 446... Loss: 0.18759... Val Loss: 0.17007\n",
      "Epoch: 447/2000... Step: 447... Loss: 0.18718... Val Loss: 0.16969\n",
      "Epoch: 448/2000... Step: 448... Loss: 0.18678... Val Loss: 0.16932\n",
      "Epoch: 449/2000... Step: 449... Loss: 0.18637... Val Loss: 0.16895\n",
      "Epoch: 450/2000... Step: 450... Loss: 0.18597... Val Loss: 0.16857\n",
      "Epoch: 451/2000... Step: 451... Loss: 0.18556... Val Loss: 0.16820\n",
      "Epoch: 452/2000... Step: 452... Loss: 0.18516... Val Loss: 0.16783\n",
      "Epoch: 453/2000... Step: 453... Loss: 0.18475... Val Loss: 0.16745\n",
      "Epoch: 454/2000... Step: 454... Loss: 0.18435... Val Loss: 0.16708\n",
      "Epoch: 455/2000... Step: 455... Loss: 0.18394... Val Loss: 0.16671\n",
      "Epoch: 456/2000... Step: 456... Loss: 0.18354... Val Loss: 0.16633\n",
      "Epoch: 457/2000... Step: 457... Loss: 0.18314... Val Loss: 0.16596\n",
      "Epoch: 458/2000... Step: 458... Loss: 0.18273... Val Loss: 0.16559\n",
      "Epoch: 459/2000... Step: 459... Loss: 0.18233... Val Loss: 0.16521\n",
      "Epoch: 460/2000... Step: 460... Loss: 0.18193... Val Loss: 0.16484\n",
      "Epoch: 461/2000... Step: 461... Loss: 0.18153... Val Loss: 0.16447\n",
      "Epoch: 462/2000... Step: 462... Loss: 0.18112... Val Loss: 0.16410\n",
      "Epoch: 463/2000... Step: 463... Loss: 0.18072... Val Loss: 0.16372\n",
      "Epoch: 464/2000... Step: 464... Loss: 0.18032... Val Loss: 0.16335\n",
      "Epoch: 465/2000... Step: 465... Loss: 0.17992... Val Loss: 0.16298\n",
      "Epoch: 466/2000... Step: 466... Loss: 0.17951... Val Loss: 0.16261\n",
      "Epoch: 467/2000... Step: 467... Loss: 0.17911... Val Loss: 0.16224\n",
      "Epoch: 468/2000... Step: 468... Loss: 0.17871... Val Loss: 0.16187\n",
      "Epoch: 469/2000... Step: 469... Loss: 0.17831... Val Loss: 0.16149\n",
      "Epoch: 470/2000... Step: 470... Loss: 0.17790... Val Loss: 0.16112\n",
      "Epoch: 471/2000... Step: 471... Loss: 0.17750... Val Loss: 0.16075\n",
      "Epoch: 472/2000... Step: 472... Loss: 0.17710... Val Loss: 0.16038\n",
      "Epoch: 473/2000... Step: 473... Loss: 0.17670... Val Loss: 0.16001\n",
      "Epoch: 474/2000... Step: 474... Loss: 0.17630... Val Loss: 0.15964\n",
      "Epoch: 475/2000... Step: 475... Loss: 0.17590... Val Loss: 0.15927\n",
      "Epoch: 476/2000... Step: 476... Loss: 0.17550... Val Loss: 0.15890\n",
      "Epoch: 477/2000... Step: 477... Loss: 0.17509... Val Loss: 0.15853\n",
      "Epoch: 478/2000... Step: 478... Loss: 0.17469... Val Loss: 0.15816\n",
      "Epoch: 479/2000... Step: 479... Loss: 0.17429... Val Loss: 0.15779\n",
      "Epoch: 480/2000... Step: 480... Loss: 0.17389... Val Loss: 0.15742\n",
      "Epoch: 481/2000... Step: 481... Loss: 0.17349... Val Loss: 0.15705\n",
      "Epoch: 482/2000... Step: 482... Loss: 0.17309... Val Loss: 0.15668\n",
      "Epoch: 483/2000... Step: 483... Loss: 0.17269... Val Loss: 0.15631\n",
      "Epoch: 484/2000... Step: 484... Loss: 0.17229... Val Loss: 0.15594\n",
      "Epoch: 485/2000... Step: 485... Loss: 0.17189... Val Loss: 0.15557\n",
      "Epoch: 486/2000... Step: 486... Loss: 0.17149... Val Loss: 0.15520\n",
      "Epoch: 487/2000... Step: 487... Loss: 0.17109... Val Loss: 0.15483\n",
      "Epoch: 488/2000... Step: 488... Loss: 0.17069... Val Loss: 0.15446\n",
      "Epoch: 489/2000... Step: 489... Loss: 0.17029... Val Loss: 0.15409\n",
      "Epoch: 490/2000... Step: 490... Loss: 0.16989... Val Loss: 0.15373\n",
      "Epoch: 491/2000... Step: 491... Loss: 0.16949... Val Loss: 0.15336\n",
      "Epoch: 492/2000... Step: 492... Loss: 0.16909... Val Loss: 0.15299\n",
      "Epoch: 493/2000... Step: 493... Loss: 0.16869... Val Loss: 0.15262\n",
      "Epoch: 494/2000... Step: 494... Loss: 0.16829... Val Loss: 0.15225\n",
      "Epoch: 495/2000... Step: 495... Loss: 0.16789... Val Loss: 0.15189\n",
      "Epoch: 496/2000... Step: 496... Loss: 0.16749... Val Loss: 0.15152\n",
      "Epoch: 497/2000... Step: 497... Loss: 0.16709... Val Loss: 0.15115\n",
      "Epoch: 498/2000... Step: 498... Loss: 0.16669... Val Loss: 0.15079\n",
      "Epoch: 499/2000... Step: 499... Loss: 0.16629... Val Loss: 0.15042\n",
      "Epoch: 500/2000... Step: 500... Loss: 0.16590... Val Loss: 0.15005\n",
      "Epoch: 501/2000... Step: 501... Loss: 0.16550... Val Loss: 0.14969\n",
      "Epoch: 502/2000... Step: 502... Loss: 0.16510... Val Loss: 0.14932\n",
      "Epoch: 503/2000... Step: 503... Loss: 0.16470... Val Loss: 0.14896\n",
      "Epoch: 504/2000... Step: 504... Loss: 0.16431... Val Loss: 0.14859\n",
      "Epoch: 505/2000... Step: 505... Loss: 0.16391... Val Loss: 0.14823\n",
      "Epoch: 506/2000... Step: 506... Loss: 0.16351... Val Loss: 0.14786\n",
      "Epoch: 507/2000... Step: 507... Loss: 0.16312... Val Loss: 0.14750\n",
      "Epoch: 508/2000... Step: 508... Loss: 0.16272... Val Loss: 0.14713\n",
      "Epoch: 509/2000... Step: 509... Loss: 0.16233... Val Loss: 0.14677\n",
      "Epoch: 510/2000... Step: 510... Loss: 0.16193... Val Loss: 0.14640\n",
      "Epoch: 511/2000... Step: 511... Loss: 0.16154... Val Loss: 0.14604\n",
      "Epoch: 512/2000... Step: 512... Loss: 0.16114... Val Loss: 0.14568\n",
      "Epoch: 513/2000... Step: 513... Loss: 0.16075... Val Loss: 0.14531\n",
      "Epoch: 514/2000... Step: 514... Loss: 0.16035... Val Loss: 0.14495\n",
      "Epoch: 515/2000... Step: 515... Loss: 0.15996... Val Loss: 0.14459\n",
      "Epoch: 516/2000... Step: 516... Loss: 0.15957... Val Loss: 0.14423\n",
      "Epoch: 517/2000... Step: 517... Loss: 0.15917... Val Loss: 0.14387\n",
      "Epoch: 518/2000... Step: 518... Loss: 0.15878... Val Loss: 0.14351\n",
      "Epoch: 519/2000... Step: 519... Loss: 0.15839... Val Loss: 0.14314\n",
      "Epoch: 520/2000... Step: 520... Loss: 0.15800... Val Loss: 0.14278\n",
      "Epoch: 521/2000... Step: 521... Loss: 0.15761... Val Loss: 0.14242\n",
      "Epoch: 522/2000... Step: 522... Loss: 0.15722... Val Loss: 0.14206\n",
      "Epoch: 523/2000... Step: 523... Loss: 0.15683... Val Loss: 0.14171\n",
      "Epoch: 524/2000... Step: 524... Loss: 0.15644... Val Loss: 0.14135\n",
      "Epoch: 525/2000... Step: 525... Loss: 0.15605... Val Loss: 0.14099\n",
      "Epoch: 526/2000... Step: 526... Loss: 0.15566... Val Loss: 0.14063\n",
      "Epoch: 527/2000... Step: 527... Loss: 0.15528... Val Loss: 0.14027\n",
      "Epoch: 528/2000... Step: 528... Loss: 0.15489... Val Loss: 0.13992\n",
      "Epoch: 529/2000... Step: 529... Loss: 0.15450... Val Loss: 0.13956\n",
      "Epoch: 530/2000... Step: 530... Loss: 0.15412... Val Loss: 0.13920\n",
      "Epoch: 531/2000... Step: 531... Loss: 0.15373... Val Loss: 0.13885\n",
      "Epoch: 532/2000... Step: 532... Loss: 0.15334... Val Loss: 0.13849\n",
      "Epoch: 533/2000... Step: 533... Loss: 0.15296... Val Loss: 0.13814\n",
      "Epoch: 534/2000... Step: 534... Loss: 0.15258... Val Loss: 0.13778\n",
      "Epoch: 535/2000... Step: 535... Loss: 0.15219... Val Loss: 0.13743\n",
      "Epoch: 536/2000... Step: 536... Loss: 0.15181... Val Loss: 0.13708\n",
      "Epoch: 537/2000... Step: 537... Loss: 0.15143... Val Loss: 0.13673\n",
      "Epoch: 538/2000... Step: 538... Loss: 0.15105... Val Loss: 0.13637\n",
      "Epoch: 539/2000... Step: 539... Loss: 0.15067... Val Loss: 0.13602\n",
      "Epoch: 540/2000... Step: 540... Loss: 0.15029... Val Loss: 0.13567\n",
      "Epoch: 541/2000... Step: 541... Loss: 0.14991... Val Loss: 0.13532\n",
      "Epoch: 542/2000... Step: 542... Loss: 0.14953... Val Loss: 0.13497\n",
      "Epoch: 543/2000... Step: 543... Loss: 0.14915... Val Loss: 0.13462\n",
      "Epoch: 544/2000... Step: 544... Loss: 0.14877... Val Loss: 0.13427\n",
      "Epoch: 545/2000... Step: 545... Loss: 0.14840... Val Loss: 0.13393\n",
      "Epoch: 546/2000... Step: 546... Loss: 0.14802... Val Loss: 0.13358\n",
      "Epoch: 547/2000... Step: 547... Loss: 0.14765... Val Loss: 0.13323\n",
      "Epoch: 548/2000... Step: 548... Loss: 0.14727... Val Loss: 0.13289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 549/2000... Step: 549... Loss: 0.14690... Val Loss: 0.13254\n",
      "Epoch: 550/2000... Step: 550... Loss: 0.14653... Val Loss: 0.13220\n",
      "Epoch: 551/2000... Step: 551... Loss: 0.14615... Val Loss: 0.13185\n",
      "Epoch: 552/2000... Step: 552... Loss: 0.14578... Val Loss: 0.13151\n",
      "Epoch: 553/2000... Step: 553... Loss: 0.14541... Val Loss: 0.13116\n",
      "Epoch: 554/2000... Step: 554... Loss: 0.14504... Val Loss: 0.13082\n",
      "Epoch: 555/2000... Step: 555... Loss: 0.14467... Val Loss: 0.13048\n",
      "Epoch: 556/2000... Step: 556... Loss: 0.14430... Val Loss: 0.13014\n",
      "Epoch: 557/2000... Step: 557... Loss: 0.14394... Val Loss: 0.12980\n",
      "Epoch: 558/2000... Step: 558... Loss: 0.14357... Val Loss: 0.12946\n",
      "Epoch: 559/2000... Step: 559... Loss: 0.14320... Val Loss: 0.12912\n",
      "Epoch: 560/2000... Step: 560... Loss: 0.14284... Val Loss: 0.12878\n",
      "Epoch: 561/2000... Step: 561... Loss: 0.14247... Val Loss: 0.12845\n",
      "Epoch: 562/2000... Step: 562... Loss: 0.14211... Val Loss: 0.12811\n",
      "Epoch: 563/2000... Step: 563... Loss: 0.14175... Val Loss: 0.12777\n",
      "Epoch: 564/2000... Step: 564... Loss: 0.14139... Val Loss: 0.12744\n",
      "Epoch: 565/2000... Step: 565... Loss: 0.14103... Val Loss: 0.12710\n",
      "Epoch: 566/2000... Step: 566... Loss: 0.14067... Val Loss: 0.12677\n",
      "Epoch: 567/2000... Step: 567... Loss: 0.14031... Val Loss: 0.12644\n",
      "Epoch: 568/2000... Step: 568... Loss: 0.13995... Val Loss: 0.12610\n",
      "Epoch: 569/2000... Step: 569... Loss: 0.13959... Val Loss: 0.12577\n",
      "Epoch: 570/2000... Step: 570... Loss: 0.13924... Val Loss: 0.12544\n",
      "Epoch: 571/2000... Step: 571... Loss: 0.13888... Val Loss: 0.12511\n",
      "Epoch: 572/2000... Step: 572... Loss: 0.13853... Val Loss: 0.12478\n",
      "Epoch: 573/2000... Step: 573... Loss: 0.13817... Val Loss: 0.12445\n",
      "Epoch: 574/2000... Step: 574... Loss: 0.13782... Val Loss: 0.12412\n",
      "Epoch: 575/2000... Step: 575... Loss: 0.13747... Val Loss: 0.12380\n",
      "Epoch: 576/2000... Step: 576... Loss: 0.13712... Val Loss: 0.12347\n",
      "Epoch: 577/2000... Step: 577... Loss: 0.13677... Val Loss: 0.12315\n",
      "Epoch: 578/2000... Step: 578... Loss: 0.13642... Val Loss: 0.12282\n",
      "Epoch: 579/2000... Step: 579... Loss: 0.13607... Val Loss: 0.12250\n",
      "Epoch: 580/2000... Step: 580... Loss: 0.13573... Val Loss: 0.12217\n",
      "Epoch: 581/2000... Step: 581... Loss: 0.13538... Val Loss: 0.12185\n",
      "Epoch: 582/2000... Step: 582... Loss: 0.13504... Val Loss: 0.12153\n",
      "Epoch: 583/2000... Step: 583... Loss: 0.13469... Val Loss: 0.12121\n",
      "Epoch: 584/2000... Step: 584... Loss: 0.13435... Val Loss: 0.12089\n",
      "Epoch: 585/2000... Step: 585... Loss: 0.13401... Val Loss: 0.12057\n",
      "Epoch: 586/2000... Step: 586... Loss: 0.13367... Val Loss: 0.12025\n",
      "Epoch: 587/2000... Step: 587... Loss: 0.13333... Val Loss: 0.11993\n",
      "Epoch: 588/2000... Step: 588... Loss: 0.13299... Val Loss: 0.11962\n",
      "Epoch: 589/2000... Step: 589... Loss: 0.13265... Val Loss: 0.11930\n",
      "Epoch: 590/2000... Step: 590... Loss: 0.13232... Val Loss: 0.11899\n",
      "Epoch: 591/2000... Step: 591... Loss: 0.13198... Val Loss: 0.11867\n",
      "Epoch: 592/2000... Step: 592... Loss: 0.13165... Val Loss: 0.11836\n",
      "Epoch: 593/2000... Step: 593... Loss: 0.13131... Val Loss: 0.11805\n",
      "Epoch: 594/2000... Step: 594... Loss: 0.13098... Val Loss: 0.11773\n",
      "Epoch: 595/2000... Step: 595... Loss: 0.13065... Val Loss: 0.11742\n",
      "Epoch: 596/2000... Step: 596... Loss: 0.13032... Val Loss: 0.11711\n",
      "Epoch: 597/2000... Step: 597... Loss: 0.12999... Val Loss: 0.11680\n",
      "Epoch: 598/2000... Step: 598... Loss: 0.12966... Val Loss: 0.11650\n",
      "Epoch: 599/2000... Step: 599... Loss: 0.12934... Val Loss: 0.11619\n",
      "Epoch: 600/2000... Step: 600... Loss: 0.12901... Val Loss: 0.11588\n",
      "Epoch: 601/2000... Step: 601... Loss: 0.12869... Val Loss: 0.11558\n",
      "Epoch: 602/2000... Step: 602... Loss: 0.12836... Val Loss: 0.11527\n",
      "Epoch: 603/2000... Step: 603... Loss: 0.12804... Val Loss: 0.11497\n",
      "Epoch: 604/2000... Step: 604... Loss: 0.12772... Val Loss: 0.11466\n",
      "Epoch: 605/2000... Step: 605... Loss: 0.12740... Val Loss: 0.11436\n",
      "Epoch: 606/2000... Step: 606... Loss: 0.12708... Val Loss: 0.11406\n",
      "Epoch: 607/2000... Step: 607... Loss: 0.12676... Val Loss: 0.11376\n",
      "Epoch: 608/2000... Step: 608... Loss: 0.12645... Val Loss: 0.11346\n",
      "Epoch: 609/2000... Step: 609... Loss: 0.12613... Val Loss: 0.11316\n",
      "Epoch: 610/2000... Step: 610... Loss: 0.12582... Val Loss: 0.11286\n",
      "Epoch: 611/2000... Step: 611... Loss: 0.12551... Val Loss: 0.11257\n",
      "Epoch: 612/2000... Step: 612... Loss: 0.12519... Val Loss: 0.11227\n",
      "Epoch: 613/2000... Step: 613... Loss: 0.12488... Val Loss: 0.11198\n",
      "Epoch: 614/2000... Step: 614... Loss: 0.12457... Val Loss: 0.11168\n",
      "Epoch: 615/2000... Step: 615... Loss: 0.12426... Val Loss: 0.11139\n",
      "Epoch: 616/2000... Step: 616... Loss: 0.12396... Val Loss: 0.11110\n",
      "Epoch: 617/2000... Step: 617... Loss: 0.12365... Val Loss: 0.11081\n",
      "Epoch: 618/2000... Step: 618... Loss: 0.12335... Val Loss: 0.11052\n",
      "Epoch: 619/2000... Step: 619... Loss: 0.12304... Val Loss: 0.11023\n",
      "Epoch: 620/2000... Step: 620... Loss: 0.12274... Val Loss: 0.10994\n",
      "Epoch: 621/2000... Step: 621... Loss: 0.12244... Val Loss: 0.10965\n",
      "Epoch: 622/2000... Step: 622... Loss: 0.12214... Val Loss: 0.10937\n",
      "Epoch: 623/2000... Step: 623... Loss: 0.12184... Val Loss: 0.10908\n",
      "Epoch: 624/2000... Step: 624... Loss: 0.12154... Val Loss: 0.10880\n",
      "Epoch: 625/2000... Step: 625... Loss: 0.12124... Val Loss: 0.10851\n",
      "Epoch: 626/2000... Step: 626... Loss: 0.12095... Val Loss: 0.10823\n",
      "Epoch: 627/2000... Step: 627... Loss: 0.12065... Val Loss: 0.10795\n",
      "Epoch: 628/2000... Step: 628... Loss: 0.12036... Val Loss: 0.10767\n",
      "Epoch: 629/2000... Step: 629... Loss: 0.12007... Val Loss: 0.10739\n",
      "Epoch: 630/2000... Step: 630... Loss: 0.11978... Val Loss: 0.10711\n",
      "Epoch: 631/2000... Step: 631... Loss: 0.11949... Val Loss: 0.10683\n",
      "Epoch: 632/2000... Step: 632... Loss: 0.11920... Val Loss: 0.10655\n",
      "Epoch: 633/2000... Step: 633... Loss: 0.11891... Val Loss: 0.10628\n",
      "Epoch: 634/2000... Step: 634... Loss: 0.11862... Val Loss: 0.10600\n",
      "Epoch: 635/2000... Step: 635... Loss: 0.11834... Val Loss: 0.10573\n",
      "Epoch: 636/2000... Step: 636... Loss: 0.11806... Val Loss: 0.10546\n",
      "Epoch: 637/2000... Step: 637... Loss: 0.11777... Val Loss: 0.10518\n",
      "Epoch: 638/2000... Step: 638... Loss: 0.11749... Val Loss: 0.10491\n",
      "Epoch: 639/2000... Step: 639... Loss: 0.11721... Val Loss: 0.10464\n",
      "Epoch: 640/2000... Step: 640... Loss: 0.11693... Val Loss: 0.10437\n",
      "Epoch: 641/2000... Step: 641... Loss: 0.11665... Val Loss: 0.10411\n",
      "Epoch: 642/2000... Step: 642... Loss: 0.11637... Val Loss: 0.10384\n",
      "Epoch: 643/2000... Step: 643... Loss: 0.11610... Val Loss: 0.10357\n",
      "Epoch: 644/2000... Step: 644... Loss: 0.11582... Val Loss: 0.10331\n",
      "Epoch: 645/2000... Step: 645... Loss: 0.11555... Val Loss: 0.10305\n",
      "Epoch: 646/2000... Step: 646... Loss: 0.11528... Val Loss: 0.10278\n",
      "Epoch: 647/2000... Step: 647... Loss: 0.11501... Val Loss: 0.10252\n",
      "Epoch: 648/2000... Step: 648... Loss: 0.11474... Val Loss: 0.10226\n",
      "Epoch: 649/2000... Step: 649... Loss: 0.11447... Val Loss: 0.10200\n",
      "Epoch: 650/2000... Step: 650... Loss: 0.11420... Val Loss: 0.10174\n",
      "Epoch: 651/2000... Step: 651... Loss: 0.11393... Val Loss: 0.10148\n",
      "Epoch: 652/2000... Step: 652... Loss: 0.11367... Val Loss: 0.10123\n",
      "Epoch: 653/2000... Step: 653... Loss: 0.11340... Val Loss: 0.10097\n",
      "Epoch: 654/2000... Step: 654... Loss: 0.11314... Val Loss: 0.10072\n",
      "Epoch: 655/2000... Step: 655... Loss: 0.11288... Val Loss: 0.10046\n",
      "Epoch: 656/2000... Step: 656... Loss: 0.11261... Val Loss: 0.10021\n",
      "Epoch: 657/2000... Step: 657... Loss: 0.11235... Val Loss: 0.09996\n",
      "Epoch: 658/2000... Step: 658... Loss: 0.11210... Val Loss: 0.09971\n",
      "Epoch: 659/2000... Step: 659... Loss: 0.11184... Val Loss: 0.09946\n",
      "Epoch: 660/2000... Step: 660... Loss: 0.11158... Val Loss: 0.09921\n",
      "Epoch: 661/2000... Step: 661... Loss: 0.11133... Val Loss: 0.09896\n",
      "Epoch: 662/2000... Step: 662... Loss: 0.11107... Val Loss: 0.09871\n",
      "Epoch: 663/2000... Step: 663... Loss: 0.11082... Val Loss: 0.09847\n",
      "Epoch: 664/2000... Step: 664... Loss: 0.11056... Val Loss: 0.09822\n",
      "Epoch: 665/2000... Step: 665... Loss: 0.11031... Val Loss: 0.09798\n",
      "Epoch: 666/2000... Step: 666... Loss: 0.11006... Val Loss: 0.09774\n",
      "Epoch: 667/2000... Step: 667... Loss: 0.10981... Val Loss: 0.09749\n",
      "Epoch: 668/2000... Step: 668... Loss: 0.10957... Val Loss: 0.09725\n",
      "Epoch: 669/2000... Step: 669... Loss: 0.10932... Val Loss: 0.09701\n",
      "Epoch: 670/2000... Step: 670... Loss: 0.10907... Val Loss: 0.09677\n",
      "Epoch: 671/2000... Step: 671... Loss: 0.10883... Val Loss: 0.09654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 672/2000... Step: 672... Loss: 0.10859... Val Loss: 0.09630\n",
      "Epoch: 673/2000... Step: 673... Loss: 0.10834... Val Loss: 0.09606\n",
      "Epoch: 674/2000... Step: 674... Loss: 0.10810... Val Loss: 0.09583\n",
      "Epoch: 675/2000... Step: 675... Loss: 0.10786... Val Loss: 0.09559\n",
      "Epoch: 676/2000... Step: 676... Loss: 0.10762... Val Loss: 0.09536\n",
      "Epoch: 677/2000... Step: 677... Loss: 0.10738... Val Loss: 0.09513\n",
      "Epoch: 678/2000... Step: 678... Loss: 0.10715... Val Loss: 0.09490\n",
      "Epoch: 679/2000... Step: 679... Loss: 0.10691... Val Loss: 0.09467\n",
      "Epoch: 680/2000... Step: 680... Loss: 0.10667... Val Loss: 0.09444\n",
      "Epoch: 681/2000... Step: 681... Loss: 0.10644... Val Loss: 0.09421\n",
      "Epoch: 682/2000... Step: 682... Loss: 0.10621... Val Loss: 0.09398\n",
      "Epoch: 683/2000... Step: 683... Loss: 0.10598... Val Loss: 0.09376\n",
      "Epoch: 684/2000... Step: 684... Loss: 0.10574... Val Loss: 0.09353\n",
      "Epoch: 685/2000... Step: 685... Loss: 0.10551... Val Loss: 0.09331\n",
      "Epoch: 686/2000... Step: 686... Loss: 0.10529... Val Loss: 0.09308\n",
      "Epoch: 687/2000... Step: 687... Loss: 0.10506... Val Loss: 0.09286\n",
      "Epoch: 688/2000... Step: 688... Loss: 0.10483... Val Loss: 0.09264\n",
      "Epoch: 689/2000... Step: 689... Loss: 0.10460... Val Loss: 0.09242\n",
      "Epoch: 690/2000... Step: 690... Loss: 0.10438... Val Loss: 0.09220\n",
      "Epoch: 691/2000... Step: 691... Loss: 0.10416... Val Loss: 0.09198\n",
      "Epoch: 692/2000... Step: 692... Loss: 0.10393... Val Loss: 0.09176\n",
      "Epoch: 693/2000... Step: 693... Loss: 0.10371... Val Loss: 0.09154\n",
      "Epoch: 694/2000... Step: 694... Loss: 0.10349... Val Loss: 0.09133\n",
      "Epoch: 695/2000... Step: 695... Loss: 0.10327... Val Loss: 0.09111\n",
      "Epoch: 696/2000... Step: 696... Loss: 0.10305... Val Loss: 0.09090\n",
      "Epoch: 697/2000... Step: 697... Loss: 0.10283... Val Loss: 0.09068\n",
      "Epoch: 698/2000... Step: 698... Loss: 0.10262... Val Loss: 0.09047\n",
      "Epoch: 699/2000... Step: 699... Loss: 0.10240... Val Loss: 0.09026\n",
      "Epoch: 700/2000... Step: 700... Loss: 0.10219... Val Loss: 0.09005\n",
      "Epoch: 701/2000... Step: 701... Loss: 0.10197... Val Loss: 0.08984\n",
      "Epoch: 702/2000... Step: 702... Loss: 0.10176... Val Loss: 0.08963\n",
      "Epoch: 703/2000... Step: 703... Loss: 0.10155... Val Loss: 0.08942\n",
      "Epoch: 704/2000... Step: 704... Loss: 0.10134... Val Loss: 0.08921\n",
      "Epoch: 705/2000... Step: 705... Loss: 0.10113... Val Loss: 0.08901\n",
      "Epoch: 706/2000... Step: 706... Loss: 0.10092... Val Loss: 0.08880\n",
      "Epoch: 707/2000... Step: 707... Loss: 0.10071... Val Loss: 0.08860\n",
      "Epoch: 708/2000... Step: 708... Loss: 0.10050... Val Loss: 0.08839\n",
      "Epoch: 709/2000... Step: 709... Loss: 0.10030... Val Loss: 0.08819\n",
      "Epoch: 710/2000... Step: 710... Loss: 0.10009... Val Loss: 0.08799\n",
      "Epoch: 711/2000... Step: 711... Loss: 0.09989... Val Loss: 0.08779\n",
      "Epoch: 712/2000... Step: 712... Loss: 0.09968... Val Loss: 0.08759\n",
      "Epoch: 713/2000... Step: 713... Loss: 0.09948... Val Loss: 0.08739\n",
      "Epoch: 714/2000... Step: 714... Loss: 0.09928... Val Loss: 0.08719\n",
      "Epoch: 715/2000... Step: 715... Loss: 0.09908... Val Loss: 0.08699\n",
      "Epoch: 716/2000... Step: 716... Loss: 0.09888... Val Loss: 0.08679\n",
      "Epoch: 717/2000... Step: 717... Loss: 0.09868... Val Loss: 0.08660\n",
      "Epoch: 718/2000... Step: 718... Loss: 0.09848... Val Loss: 0.08640\n",
      "Epoch: 719/2000... Step: 719... Loss: 0.09828... Val Loss: 0.08621\n",
      "Epoch: 720/2000... Step: 720... Loss: 0.09809... Val Loss: 0.08601\n",
      "Epoch: 721/2000... Step: 721... Loss: 0.09789... Val Loss: 0.08582\n",
      "Epoch: 722/2000... Step: 722... Loss: 0.09770... Val Loss: 0.08563\n",
      "Epoch: 723/2000... Step: 723... Loss: 0.09750... Val Loss: 0.08544\n",
      "Epoch: 724/2000... Step: 724... Loss: 0.09731... Val Loss: 0.08525\n",
      "Epoch: 725/2000... Step: 725... Loss: 0.09712... Val Loss: 0.08506\n",
      "Epoch: 726/2000... Step: 726... Loss: 0.09693... Val Loss: 0.08487\n",
      "Epoch: 727/2000... Step: 727... Loss: 0.09674... Val Loss: 0.08468\n",
      "Epoch: 728/2000... Step: 728... Loss: 0.09655... Val Loss: 0.08449\n",
      "Epoch: 729/2000... Step: 729... Loss: 0.09636... Val Loss: 0.08430\n",
      "Epoch: 730/2000... Step: 730... Loss: 0.09617... Val Loss: 0.08412\n",
      "Epoch: 731/2000... Step: 731... Loss: 0.09599... Val Loss: 0.08393\n",
      "Epoch: 732/2000... Step: 732... Loss: 0.09580... Val Loss: 0.08375\n",
      "Epoch: 733/2000... Step: 733... Loss: 0.09562... Val Loss: 0.08357\n",
      "Epoch: 734/2000... Step: 734... Loss: 0.09543... Val Loss: 0.08338\n",
      "Epoch: 735/2000... Step: 735... Loss: 0.09525... Val Loss: 0.08320\n",
      "Epoch: 736/2000... Step: 736... Loss: 0.09507... Val Loss: 0.08302\n",
      "Epoch: 737/2000... Step: 737... Loss: 0.09489... Val Loss: 0.08284\n",
      "Epoch: 738/2000... Step: 738... Loss: 0.09471... Val Loss: 0.08266\n",
      "Epoch: 739/2000... Step: 739... Loss: 0.09453... Val Loss: 0.08248\n",
      "Epoch: 740/2000... Step: 740... Loss: 0.09435... Val Loss: 0.08230\n",
      "Epoch: 741/2000... Step: 741... Loss: 0.09417... Val Loss: 0.08212\n",
      "Epoch: 742/2000... Step: 742... Loss: 0.09399... Val Loss: 0.08195\n",
      "Epoch: 743/2000... Step: 743... Loss: 0.09382... Val Loss: 0.08177\n",
      "Epoch: 744/2000... Step: 744... Loss: 0.09364... Val Loss: 0.08159\n",
      "Epoch: 745/2000... Step: 745... Loss: 0.09347... Val Loss: 0.08142\n",
      "Epoch: 746/2000... Step: 746... Loss: 0.09329... Val Loss: 0.08124\n",
      "Epoch: 747/2000... Step: 747... Loss: 0.09312... Val Loss: 0.08107\n",
      "Epoch: 748/2000... Step: 748... Loss: 0.09295... Val Loss: 0.08090\n",
      "Epoch: 749/2000... Step: 749... Loss: 0.09278... Val Loss: 0.08073\n",
      "Epoch: 750/2000... Step: 750... Loss: 0.09261... Val Loss: 0.08055\n",
      "Epoch: 751/2000... Step: 751... Loss: 0.09244... Val Loss: 0.08038\n",
      "Epoch: 752/2000... Step: 752... Loss: 0.09227... Val Loss: 0.08021\n",
      "Epoch: 753/2000... Step: 753... Loss: 0.09210... Val Loss: 0.08004\n",
      "Epoch: 754/2000... Step: 754... Loss: 0.09193... Val Loss: 0.07987\n",
      "Epoch: 755/2000... Step: 755... Loss: 0.09176... Val Loss: 0.07971\n",
      "Epoch: 756/2000... Step: 756... Loss: 0.09160... Val Loss: 0.07954\n",
      "Epoch: 757/2000... Step: 757... Loss: 0.09143... Val Loss: 0.07937\n",
      "Epoch: 758/2000... Step: 758... Loss: 0.09127... Val Loss: 0.07921\n",
      "Epoch: 759/2000... Step: 759... Loss: 0.09110... Val Loss: 0.07904\n",
      "Epoch: 760/2000... Step: 760... Loss: 0.09094... Val Loss: 0.07888\n",
      "Epoch: 761/2000... Step: 761... Loss: 0.09078... Val Loss: 0.07871\n",
      "Epoch: 762/2000... Step: 762... Loss: 0.09062... Val Loss: 0.07855\n",
      "Epoch: 763/2000... Step: 763... Loss: 0.09046... Val Loss: 0.07838\n",
      "Epoch: 764/2000... Step: 764... Loss: 0.09030... Val Loss: 0.07822\n",
      "Epoch: 765/2000... Step: 765... Loss: 0.09014... Val Loss: 0.07806\n",
      "Epoch: 766/2000... Step: 766... Loss: 0.08998... Val Loss: 0.07790\n",
      "Epoch: 767/2000... Step: 767... Loss: 0.08982... Val Loss: 0.07774\n",
      "Epoch: 768/2000... Step: 768... Loss: 0.08966... Val Loss: 0.07758\n",
      "Epoch: 769/2000... Step: 769... Loss: 0.08951... Val Loss: 0.07742\n",
      "Epoch: 770/2000... Step: 770... Loss: 0.08935... Val Loss: 0.07726\n",
      "Epoch: 771/2000... Step: 771... Loss: 0.08920... Val Loss: 0.07710\n",
      "Epoch: 772/2000... Step: 772... Loss: 0.08904... Val Loss: 0.07695\n",
      "Epoch: 773/2000... Step: 773... Loss: 0.08889... Val Loss: 0.07679\n",
      "Epoch: 774/2000... Step: 774... Loss: 0.08874... Val Loss: 0.07663\n",
      "Epoch: 775/2000... Step: 775... Loss: 0.08859... Val Loss: 0.07648\n",
      "Epoch: 776/2000... Step: 776... Loss: 0.08844... Val Loss: 0.07632\n",
      "Epoch: 777/2000... Step: 777... Loss: 0.08828... Val Loss: 0.07617\n",
      "Epoch: 778/2000... Step: 778... Loss: 0.08813... Val Loss: 0.07602\n",
      "Epoch: 779/2000... Step: 779... Loss: 0.08799... Val Loss: 0.07586\n",
      "Epoch: 780/2000... Step: 780... Loss: 0.08784... Val Loss: 0.07571\n",
      "Epoch: 781/2000... Step: 781... Loss: 0.08769... Val Loss: 0.07556\n",
      "Epoch: 782/2000... Step: 782... Loss: 0.08754... Val Loss: 0.07541\n",
      "Epoch: 783/2000... Step: 783... Loss: 0.08740... Val Loss: 0.07526\n",
      "Epoch: 784/2000... Step: 784... Loss: 0.08725... Val Loss: 0.07511\n",
      "Epoch: 785/2000... Step: 785... Loss: 0.08711... Val Loss: 0.07496\n",
      "Epoch: 786/2000... Step: 786... Loss: 0.08696... Val Loss: 0.07481\n",
      "Epoch: 787/2000... Step: 787... Loss: 0.08682... Val Loss: 0.07466\n",
      "Epoch: 788/2000... Step: 788... Loss: 0.08668... Val Loss: 0.07451\n",
      "Epoch: 789/2000... Step: 789... Loss: 0.08653... Val Loss: 0.07436\n",
      "Epoch: 790/2000... Step: 790... Loss: 0.08639... Val Loss: 0.07422\n",
      "Epoch: 791/2000... Step: 791... Loss: 0.08625... Val Loss: 0.07407\n",
      "Epoch: 792/2000... Step: 792... Loss: 0.08611... Val Loss: 0.07392\n",
      "Epoch: 793/2000... Step: 793... Loss: 0.08597... Val Loss: 0.07378\n",
      "Epoch: 794/2000... Step: 794... Loss: 0.08583... Val Loss: 0.07363\n",
      "Epoch: 795/2000... Step: 795... Loss: 0.08569... Val Loss: 0.07349\n",
      "Epoch: 796/2000... Step: 796... Loss: 0.08556... Val Loss: 0.07335\n",
      "Epoch: 797/2000... Step: 797... Loss: 0.08542... Val Loss: 0.07320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 798/2000... Step: 798... Loss: 0.08528... Val Loss: 0.07306\n",
      "Epoch: 799/2000... Step: 799... Loss: 0.08515... Val Loss: 0.07292\n",
      "Epoch: 800/2000... Step: 800... Loss: 0.08501... Val Loss: 0.07278\n",
      "Epoch: 801/2000... Step: 801... Loss: 0.08488... Val Loss: 0.07264\n",
      "Epoch: 802/2000... Step: 802... Loss: 0.08474... Val Loss: 0.07250\n",
      "Epoch: 803/2000... Step: 803... Loss: 0.08461... Val Loss: 0.07236\n",
      "Epoch: 804/2000... Step: 804... Loss: 0.08448... Val Loss: 0.07222\n",
      "Epoch: 805/2000... Step: 805... Loss: 0.08435... Val Loss: 0.07208\n",
      "Epoch: 806/2000... Step: 806... Loss: 0.08422... Val Loss: 0.07194\n",
      "Epoch: 807/2000... Step: 807... Loss: 0.08409... Val Loss: 0.07180\n",
      "Epoch: 808/2000... Step: 808... Loss: 0.08396... Val Loss: 0.07166\n",
      "Epoch: 809/2000... Step: 809... Loss: 0.08383... Val Loss: 0.07153\n",
      "Epoch: 810/2000... Step: 810... Loss: 0.08370... Val Loss: 0.07139\n",
      "Epoch: 811/2000... Step: 811... Loss: 0.08357... Val Loss: 0.07125\n",
      "Epoch: 812/2000... Step: 812... Loss: 0.08344... Val Loss: 0.07112\n",
      "Epoch: 813/2000... Step: 813... Loss: 0.08331... Val Loss: 0.07098\n",
      "Epoch: 814/2000... Step: 814... Loss: 0.08319... Val Loss: 0.07085\n",
      "Epoch: 815/2000... Step: 815... Loss: 0.08306... Val Loss: 0.07072\n",
      "Epoch: 816/2000... Step: 816... Loss: 0.08294... Val Loss: 0.07058\n",
      "Epoch: 817/2000... Step: 817... Loss: 0.08281... Val Loss: 0.07045\n",
      "Epoch: 818/2000... Step: 818... Loss: 0.08269... Val Loss: 0.07032\n",
      "Epoch: 819/2000... Step: 819... Loss: 0.08257... Val Loss: 0.07018\n",
      "Epoch: 820/2000... Step: 820... Loss: 0.08244... Val Loss: 0.07005\n",
      "Epoch: 821/2000... Step: 821... Loss: 0.08232... Val Loss: 0.06992\n",
      "Epoch: 822/2000... Step: 822... Loss: 0.08220... Val Loss: 0.06979\n",
      "Epoch: 823/2000... Step: 823... Loss: 0.08208... Val Loss: 0.06966\n",
      "Epoch: 824/2000... Step: 824... Loss: 0.08196... Val Loss: 0.06953\n",
      "Epoch: 825/2000... Step: 825... Loss: 0.08184... Val Loss: 0.06940\n",
      "Epoch: 826/2000... Step: 826... Loss: 0.08172... Val Loss: 0.06927\n",
      "Epoch: 827/2000... Step: 827... Loss: 0.08160... Val Loss: 0.06914\n",
      "Epoch: 828/2000... Step: 828... Loss: 0.08148... Val Loss: 0.06902\n",
      "Epoch: 829/2000... Step: 829... Loss: 0.08136... Val Loss: 0.06889\n",
      "Epoch: 830/2000... Step: 830... Loss: 0.08125... Val Loss: 0.06876\n",
      "Epoch: 831/2000... Step: 831... Loss: 0.08113... Val Loss: 0.06864\n",
      "Epoch: 832/2000... Step: 832... Loss: 0.08101... Val Loss: 0.06851\n",
      "Epoch: 833/2000... Step: 833... Loss: 0.08090... Val Loss: 0.06838\n",
      "Epoch: 834/2000... Step: 834... Loss: 0.08078... Val Loss: 0.06826\n",
      "Epoch: 835/2000... Step: 835... Loss: 0.08067... Val Loss: 0.06813\n",
      "Epoch: 836/2000... Step: 836... Loss: 0.08056... Val Loss: 0.06801\n",
      "Epoch: 837/2000... Step: 837... Loss: 0.08044... Val Loss: 0.06789\n",
      "Epoch: 838/2000... Step: 838... Loss: 0.08033... Val Loss: 0.06776\n",
      "Epoch: 839/2000... Step: 839... Loss: 0.08022... Val Loss: 0.06764\n",
      "Epoch: 840/2000... Step: 840... Loss: 0.08011... Val Loss: 0.06752\n",
      "Epoch: 841/2000... Step: 841... Loss: 0.07999... Val Loss: 0.06739\n",
      "Epoch: 842/2000... Step: 842... Loss: 0.07988... Val Loss: 0.06727\n",
      "Epoch: 843/2000... Step: 843... Loss: 0.07977... Val Loss: 0.06715\n",
      "Epoch: 844/2000... Step: 844... Loss: 0.07966... Val Loss: 0.06703\n",
      "Epoch: 845/2000... Step: 845... Loss: 0.07956... Val Loss: 0.06691\n",
      "Epoch: 846/2000... Step: 846... Loss: 0.07945... Val Loss: 0.06679\n",
      "Epoch: 847/2000... Step: 847... Loss: 0.07934... Val Loss: 0.06667\n",
      "Epoch: 848/2000... Step: 848... Loss: 0.07923... Val Loss: 0.06655\n",
      "Epoch: 849/2000... Step: 849... Loss: 0.07912... Val Loss: 0.06643\n",
      "Epoch: 850/2000... Step: 850... Loss: 0.07902... Val Loss: 0.06631\n",
      "Epoch: 851/2000... Step: 851... Loss: 0.07891... Val Loss: 0.06620\n",
      "Epoch: 852/2000... Step: 852... Loss: 0.07881... Val Loss: 0.06608\n",
      "Epoch: 853/2000... Step: 853... Loss: 0.07870... Val Loss: 0.06596\n",
      "Epoch: 854/2000... Step: 854... Loss: 0.07860... Val Loss: 0.06584\n",
      "Epoch: 855/2000... Step: 855... Loss: 0.07849... Val Loss: 0.06573\n",
      "Epoch: 856/2000... Step: 856... Loss: 0.07839... Val Loss: 0.06561\n",
      "Epoch: 857/2000... Step: 857... Loss: 0.07829... Val Loss: 0.06550\n",
      "Epoch: 858/2000... Step: 858... Loss: 0.07818... Val Loss: 0.06538\n",
      "Epoch: 859/2000... Step: 859... Loss: 0.07808... Val Loss: 0.06527\n",
      "Epoch: 860/2000... Step: 860... Loss: 0.07798... Val Loss: 0.06515\n",
      "Epoch: 861/2000... Step: 861... Loss: 0.07788... Val Loss: 0.06504\n",
      "Epoch: 862/2000... Step: 862... Loss: 0.07778... Val Loss: 0.06492\n",
      "Epoch: 863/2000... Step: 863... Loss: 0.07768... Val Loss: 0.06481\n",
      "Epoch: 864/2000... Step: 864... Loss: 0.07758... Val Loss: 0.06470\n",
      "Epoch: 865/2000... Step: 865... Loss: 0.07748... Val Loss: 0.06459\n",
      "Epoch: 866/2000... Step: 866... Loss: 0.07738... Val Loss: 0.06447\n",
      "Epoch: 867/2000... Step: 867... Loss: 0.07728... Val Loss: 0.06436\n",
      "Epoch: 868/2000... Step: 868... Loss: 0.07719... Val Loss: 0.06425\n",
      "Epoch: 869/2000... Step: 869... Loss: 0.07709... Val Loss: 0.06414\n",
      "Epoch: 870/2000... Step: 870... Loss: 0.07699... Val Loss: 0.06403\n",
      "Epoch: 871/2000... Step: 871... Loss: 0.07689... Val Loss: 0.06392\n",
      "Epoch: 872/2000... Step: 872... Loss: 0.07680... Val Loss: 0.06381\n",
      "Epoch: 873/2000... Step: 873... Loss: 0.07670... Val Loss: 0.06370\n",
      "Epoch: 874/2000... Step: 874... Loss: 0.07661... Val Loss: 0.06359\n",
      "Epoch: 875/2000... Step: 875... Loss: 0.07651... Val Loss: 0.06348\n",
      "Epoch: 876/2000... Step: 876... Loss: 0.07642... Val Loss: 0.06337\n",
      "Epoch: 877/2000... Step: 877... Loss: 0.07633... Val Loss: 0.06327\n",
      "Epoch: 878/2000... Step: 878... Loss: 0.07623... Val Loss: 0.06316\n",
      "Epoch: 879/2000... Step: 879... Loss: 0.07614... Val Loss: 0.06305\n",
      "Epoch: 880/2000... Step: 880... Loss: 0.07605... Val Loss: 0.06294\n",
      "Epoch: 881/2000... Step: 881... Loss: 0.07596... Val Loss: 0.06284\n",
      "Epoch: 882/2000... Step: 882... Loss: 0.07586... Val Loss: 0.06273\n",
      "Epoch: 883/2000... Step: 883... Loss: 0.07577... Val Loss: 0.06263\n",
      "Epoch: 884/2000... Step: 884... Loss: 0.07568... Val Loss: 0.06252\n",
      "Epoch: 885/2000... Step: 885... Loss: 0.07559... Val Loss: 0.06242\n",
      "Epoch: 886/2000... Step: 886... Loss: 0.07550... Val Loss: 0.06231\n",
      "Epoch: 887/2000... Step: 887... Loss: 0.07541... Val Loss: 0.06221\n",
      "Epoch: 888/2000... Step: 888... Loss: 0.07532... Val Loss: 0.06210\n",
      "Epoch: 889/2000... Step: 889... Loss: 0.07524... Val Loss: 0.06200\n",
      "Epoch: 890/2000... Step: 890... Loss: 0.07515... Val Loss: 0.06190\n",
      "Epoch: 891/2000... Step: 891... Loss: 0.07506... Val Loss: 0.06179\n",
      "Epoch: 892/2000... Step: 892... Loss: 0.07497... Val Loss: 0.06169\n",
      "Epoch: 893/2000... Step: 893... Loss: 0.07489... Val Loss: 0.06159\n",
      "Epoch: 894/2000... Step: 894... Loss: 0.07480... Val Loss: 0.06149\n",
      "Epoch: 895/2000... Step: 895... Loss: 0.07471... Val Loss: 0.06138\n",
      "Epoch: 896/2000... Step: 896... Loss: 0.07463... Val Loss: 0.06128\n",
      "Epoch: 897/2000... Step: 897... Loss: 0.07454... Val Loss: 0.06118\n",
      "Epoch: 898/2000... Step: 898... Loss: 0.07446... Val Loss: 0.06108\n",
      "Epoch: 899/2000... Step: 899... Loss: 0.07437... Val Loss: 0.06098\n",
      "Epoch: 900/2000... Step: 900... Loss: 0.07429... Val Loss: 0.06088\n",
      "Epoch: 901/2000... Step: 901... Loss: 0.07420... Val Loss: 0.06078\n",
      "Epoch: 902/2000... Step: 902... Loss: 0.07412... Val Loss: 0.06068\n",
      "Epoch: 903/2000... Step: 903... Loss: 0.07404... Val Loss: 0.06058\n",
      "Epoch: 904/2000... Step: 904... Loss: 0.07396... Val Loss: 0.06049\n",
      "Epoch: 905/2000... Step: 905... Loss: 0.07387... Val Loss: 0.06039\n",
      "Epoch: 906/2000... Step: 906... Loss: 0.07379... Val Loss: 0.06029\n",
      "Epoch: 907/2000... Step: 907... Loss: 0.07371... Val Loss: 0.06019\n",
      "Epoch: 908/2000... Step: 908... Loss: 0.07363... Val Loss: 0.06010\n",
      "Epoch: 909/2000... Step: 909... Loss: 0.07355... Val Loss: 0.06000\n",
      "Epoch: 910/2000... Step: 910... Loss: 0.07347... Val Loss: 0.05990\n",
      "Epoch: 911/2000... Step: 911... Loss: 0.07339... Val Loss: 0.05981\n",
      "Epoch: 912/2000... Step: 912... Loss: 0.07331... Val Loss: 0.05971\n",
      "Epoch: 913/2000... Step: 913... Loss: 0.07323... Val Loss: 0.05961\n",
      "Epoch: 914/2000... Step: 914... Loss: 0.07315... Val Loss: 0.05952\n",
      "Epoch: 915/2000... Step: 915... Loss: 0.07307... Val Loss: 0.05942\n",
      "Epoch: 916/2000... Step: 916... Loss: 0.07299... Val Loss: 0.05933\n",
      "Epoch: 917/2000... Step: 917... Loss: 0.07292... Val Loss: 0.05924\n",
      "Epoch: 918/2000... Step: 918... Loss: 0.07284... Val Loss: 0.05914\n",
      "Epoch: 919/2000... Step: 919... Loss: 0.07276... Val Loss: 0.05905\n",
      "Epoch: 920/2000... Step: 920... Loss: 0.07269... Val Loss: 0.05895\n",
      "Epoch: 921/2000... Step: 921... Loss: 0.07261... Val Loss: 0.05886\n",
      "Epoch: 922/2000... Step: 922... Loss: 0.07253... Val Loss: 0.05877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 923/2000... Step: 923... Loss: 0.07246... Val Loss: 0.05868\n",
      "Epoch: 924/2000... Step: 924... Loss: 0.07238... Val Loss: 0.05858\n",
      "Epoch: 925/2000... Step: 925... Loss: 0.07231... Val Loss: 0.05849\n",
      "Epoch: 926/2000... Step: 926... Loss: 0.07223... Val Loss: 0.05840\n",
      "Epoch: 927/2000... Step: 927... Loss: 0.07216... Val Loss: 0.05831\n",
      "Epoch: 928/2000... Step: 928... Loss: 0.07209... Val Loss: 0.05822\n",
      "Epoch: 929/2000... Step: 929... Loss: 0.07201... Val Loss: 0.05813\n",
      "Epoch: 930/2000... Step: 930... Loss: 0.07194... Val Loss: 0.05804\n",
      "Epoch: 931/2000... Step: 931... Loss: 0.07187... Val Loss: 0.05795\n",
      "Epoch: 932/2000... Step: 932... Loss: 0.07179... Val Loss: 0.05786\n",
      "Epoch: 933/2000... Step: 933... Loss: 0.07172... Val Loss: 0.05777\n",
      "Epoch: 934/2000... Step: 934... Loss: 0.07165... Val Loss: 0.05768\n",
      "Epoch: 935/2000... Step: 935... Loss: 0.07158... Val Loss: 0.05759\n",
      "Epoch: 936/2000... Step: 936... Loss: 0.07151... Val Loss: 0.05750\n",
      "Epoch: 937/2000... Step: 937... Loss: 0.07144... Val Loss: 0.05742\n",
      "Epoch: 938/2000... Step: 938... Loss: 0.07137... Val Loss: 0.05733\n",
      "Epoch: 939/2000... Step: 939... Loss: 0.07130... Val Loss: 0.05724\n",
      "Epoch: 940/2000... Step: 940... Loss: 0.07123... Val Loss: 0.05715\n",
      "Epoch: 941/2000... Step: 941... Loss: 0.07116... Val Loss: 0.05707\n",
      "Epoch: 942/2000... Step: 942... Loss: 0.07109... Val Loss: 0.05698\n",
      "Epoch: 943/2000... Step: 943... Loss: 0.07102... Val Loss: 0.05689\n",
      "Epoch: 944/2000... Step: 944... Loss: 0.07095... Val Loss: 0.05681\n",
      "Epoch: 945/2000... Step: 945... Loss: 0.07088... Val Loss: 0.05672\n",
      "Epoch: 946/2000... Step: 946... Loss: 0.07081... Val Loss: 0.05664\n",
      "Epoch: 947/2000... Step: 947... Loss: 0.07074... Val Loss: 0.05655\n",
      "Epoch: 948/2000... Step: 948... Loss: 0.07068... Val Loss: 0.05647\n",
      "Epoch: 949/2000... Step: 949... Loss: 0.07061... Val Loss: 0.05638\n",
      "Epoch: 950/2000... Step: 950... Loss: 0.07054... Val Loss: 0.05630\n",
      "Epoch: 951/2000... Step: 951... Loss: 0.07048... Val Loss: 0.05621\n",
      "Epoch: 952/2000... Step: 952... Loss: 0.07041... Val Loss: 0.05613\n",
      "Epoch: 953/2000... Step: 953... Loss: 0.07034... Val Loss: 0.05605\n",
      "Epoch: 954/2000... Step: 954... Loss: 0.07028... Val Loss: 0.05596\n",
      "Epoch: 955/2000... Step: 955... Loss: 0.07021... Val Loss: 0.05588\n",
      "Epoch: 956/2000... Step: 956... Loss: 0.07015... Val Loss: 0.05580\n",
      "Epoch: 957/2000... Step: 957... Loss: 0.07008... Val Loss: 0.05572\n",
      "Epoch: 958/2000... Step: 958... Loss: 0.07002... Val Loss: 0.05563\n",
      "Epoch: 959/2000... Step: 959... Loss: 0.06996... Val Loss: 0.05555\n",
      "Epoch: 960/2000... Step: 960... Loss: 0.06989... Val Loss: 0.05547\n",
      "Epoch: 961/2000... Step: 961... Loss: 0.06983... Val Loss: 0.05539\n",
      "Epoch: 962/2000... Step: 962... Loss: 0.06977... Val Loss: 0.05531\n",
      "Epoch: 963/2000... Step: 963... Loss: 0.06970... Val Loss: 0.05523\n",
      "Epoch: 964/2000... Step: 964... Loss: 0.06964... Val Loss: 0.05515\n",
      "Epoch: 965/2000... Step: 965... Loss: 0.06958... Val Loss: 0.05507\n",
      "Epoch: 966/2000... Step: 966... Loss: 0.06951... Val Loss: 0.05499\n",
      "Epoch: 967/2000... Step: 967... Loss: 0.06945... Val Loss: 0.05491\n",
      "Epoch: 968/2000... Step: 968... Loss: 0.06939... Val Loss: 0.05483\n",
      "Epoch: 969/2000... Step: 969... Loss: 0.06933... Val Loss: 0.05475\n",
      "Epoch: 970/2000... Step: 970... Loss: 0.06927... Val Loss: 0.05467\n",
      "Epoch: 971/2000... Step: 971... Loss: 0.06921... Val Loss: 0.05459\n",
      "Epoch: 972/2000... Step: 972... Loss: 0.06915... Val Loss: 0.05452\n",
      "Epoch: 973/2000... Step: 973... Loss: 0.06909... Val Loss: 0.05444\n",
      "Epoch: 974/2000... Step: 974... Loss: 0.06903... Val Loss: 0.05436\n",
      "Epoch: 975/2000... Step: 975... Loss: 0.06897... Val Loss: 0.05428\n",
      "Epoch: 976/2000... Step: 976... Loss: 0.06891... Val Loss: 0.05421\n",
      "Epoch: 977/2000... Step: 977... Loss: 0.06885... Val Loss: 0.05413\n",
      "Epoch: 978/2000... Step: 978... Loss: 0.06879... Val Loss: 0.05405\n",
      "Epoch: 979/2000... Step: 979... Loss: 0.06873... Val Loss: 0.05398\n",
      "Epoch: 980/2000... Step: 980... Loss: 0.06867... Val Loss: 0.05390\n",
      "Epoch: 981/2000... Step: 981... Loss: 0.06862... Val Loss: 0.05382\n",
      "Epoch: 982/2000... Step: 982... Loss: 0.06856... Val Loss: 0.05375\n",
      "Epoch: 983/2000... Step: 983... Loss: 0.06850... Val Loss: 0.05367\n",
      "Epoch: 984/2000... Step: 984... Loss: 0.06844... Val Loss: 0.05360\n",
      "Epoch: 985/2000... Step: 985... Loss: 0.06839... Val Loss: 0.05352\n",
      "Epoch: 986/2000... Step: 986... Loss: 0.06833... Val Loss: 0.05345\n",
      "Epoch: 987/2000... Step: 987... Loss: 0.06827... Val Loss: 0.05337\n",
      "Epoch: 988/2000... Step: 988... Loss: 0.06822... Val Loss: 0.05330\n",
      "Epoch: 989/2000... Step: 989... Loss: 0.06816... Val Loss: 0.05323\n",
      "Epoch: 990/2000... Step: 990... Loss: 0.06810... Val Loss: 0.05315\n",
      "Epoch: 991/2000... Step: 991... Loss: 0.06805... Val Loss: 0.05308\n",
      "Epoch: 992/2000... Step: 992... Loss: 0.06799... Val Loss: 0.05301\n",
      "Epoch: 993/2000... Step: 993... Loss: 0.06794... Val Loss: 0.05293\n",
      "Epoch: 994/2000... Step: 994... Loss: 0.06788... Val Loss: 0.05286\n",
      "Epoch: 995/2000... Step: 995... Loss: 0.06783... Val Loss: 0.05279\n",
      "Epoch: 996/2000... Step: 996... Loss: 0.06777... Val Loss: 0.05272\n",
      "Epoch: 997/2000... Step: 997... Loss: 0.06772... Val Loss: 0.05265\n",
      "Epoch: 998/2000... Step: 998... Loss: 0.06767... Val Loss: 0.05257\n",
      "Epoch: 999/2000... Step: 999... Loss: 0.06761... Val Loss: 0.05250\n",
      "Epoch: 1000/2000... Step: 1000... Loss: 0.06756... Val Loss: 0.05243\n",
      "Epoch: 1001/2000... Step: 1001... Loss: 0.06750... Val Loss: 0.05236\n",
      "Epoch: 1002/2000... Step: 1002... Loss: 0.06745... Val Loss: 0.05229\n",
      "Epoch: 1003/2000... Step: 1003... Loss: 0.06740... Val Loss: 0.05222\n",
      "Epoch: 1004/2000... Step: 1004... Loss: 0.06735... Val Loss: 0.05215\n",
      "Epoch: 1005/2000... Step: 1005... Loss: 0.06729... Val Loss: 0.05208\n",
      "Epoch: 1006/2000... Step: 1006... Loss: 0.06724... Val Loss: 0.05201\n",
      "Epoch: 1007/2000... Step: 1007... Loss: 0.06719... Val Loss: 0.05194\n",
      "Epoch: 1008/2000... Step: 1008... Loss: 0.06714... Val Loss: 0.05187\n",
      "Epoch: 1009/2000... Step: 1009... Loss: 0.06709... Val Loss: 0.05180\n",
      "Epoch: 1010/2000... Step: 1010... Loss: 0.06704... Val Loss: 0.05174\n",
      "Epoch: 1011/2000... Step: 1011... Loss: 0.06698... Val Loss: 0.05167\n",
      "Epoch: 1012/2000... Step: 1012... Loss: 0.06693... Val Loss: 0.05160\n",
      "Epoch: 1013/2000... Step: 1013... Loss: 0.06688... Val Loss: 0.05153\n",
      "Epoch: 1014/2000... Step: 1014... Loss: 0.06683... Val Loss: 0.05146\n",
      "Epoch: 1015/2000... Step: 1015... Loss: 0.06678... Val Loss: 0.05140\n",
      "Epoch: 1016/2000... Step: 1016... Loss: 0.06673... Val Loss: 0.05133\n",
      "Epoch: 1017/2000... Step: 1017... Loss: 0.06668... Val Loss: 0.05126\n",
      "Epoch: 1018/2000... Step: 1018... Loss: 0.06663... Val Loss: 0.05120\n",
      "Epoch: 1019/2000... Step: 1019... Loss: 0.06658... Val Loss: 0.05113\n",
      "Epoch: 1020/2000... Step: 1020... Loss: 0.06654... Val Loss: 0.05106\n",
      "Epoch: 1021/2000... Step: 1021... Loss: 0.06649... Val Loss: 0.05100\n",
      "Epoch: 1022/2000... Step: 1022... Loss: 0.06644... Val Loss: 0.05093\n",
      "Epoch: 1023/2000... Step: 1023... Loss: 0.06639... Val Loss: 0.05087\n",
      "Epoch: 1024/2000... Step: 1024... Loss: 0.06634... Val Loss: 0.05080\n",
      "Epoch: 1025/2000... Step: 1025... Loss: 0.06629... Val Loss: 0.05074\n",
      "Epoch: 1026/2000... Step: 1026... Loss: 0.06625... Val Loss: 0.05067\n",
      "Epoch: 1027/2000... Step: 1027... Loss: 0.06620... Val Loss: 0.05061\n",
      "Epoch: 1028/2000... Step: 1028... Loss: 0.06615... Val Loss: 0.05054\n",
      "Epoch: 1029/2000... Step: 1029... Loss: 0.06610... Val Loss: 0.05048\n",
      "Epoch: 1030/2000... Step: 1030... Loss: 0.06606... Val Loss: 0.05041\n",
      "Epoch: 1031/2000... Step: 1031... Loss: 0.06601... Val Loss: 0.05035\n",
      "Epoch: 1032/2000... Step: 1032... Loss: 0.06596... Val Loss: 0.05029\n",
      "Epoch: 1033/2000... Step: 1033... Loss: 0.06592... Val Loss: 0.05022\n",
      "Epoch: 1034/2000... Step: 1034... Loss: 0.06587... Val Loss: 0.05016\n",
      "Epoch: 1035/2000... Step: 1035... Loss: 0.06582... Val Loss: 0.05010\n",
      "Epoch: 1036/2000... Step: 1036... Loss: 0.06578... Val Loss: 0.05003\n",
      "Epoch: 1037/2000... Step: 1037... Loss: 0.06573... Val Loss: 0.04997\n",
      "Epoch: 1038/2000... Step: 1038... Loss: 0.06569... Val Loss: 0.04991\n",
      "Epoch: 1039/2000... Step: 1039... Loss: 0.06564... Val Loss: 0.04985\n",
      "Epoch: 1040/2000... Step: 1040... Loss: 0.06560... Val Loss: 0.04979\n",
      "Epoch: 1041/2000... Step: 1041... Loss: 0.06555... Val Loss: 0.04972\n",
      "Epoch: 1042/2000... Step: 1042... Loss: 0.06551... Val Loss: 0.04966\n",
      "Epoch: 1043/2000... Step: 1043... Loss: 0.06546... Val Loss: 0.04960\n",
      "Epoch: 1044/2000... Step: 1044... Loss: 0.06542... Val Loss: 0.04954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1045/2000... Step: 1045... Loss: 0.06537... Val Loss: 0.04948\n",
      "Epoch: 1046/2000... Step: 1046... Loss: 0.06533... Val Loss: 0.04942\n",
      "Epoch: 1047/2000... Step: 1047... Loss: 0.06529... Val Loss: 0.04936\n",
      "Epoch: 1048/2000... Step: 1048... Loss: 0.06524... Val Loss: 0.04930\n",
      "Epoch: 1049/2000... Step: 1049... Loss: 0.06520... Val Loss: 0.04924\n",
      "Epoch: 1050/2000... Step: 1050... Loss: 0.06516... Val Loss: 0.04918\n",
      "Epoch: 1051/2000... Step: 1051... Loss: 0.06511... Val Loss: 0.04912\n",
      "Epoch: 1052/2000... Step: 1052... Loss: 0.06507... Val Loss: 0.04906\n",
      "Epoch: 1053/2000... Step: 1053... Loss: 0.06503... Val Loss: 0.04900\n",
      "Epoch: 1054/2000... Step: 1054... Loss: 0.06498... Val Loss: 0.04894\n",
      "Epoch: 1055/2000... Step: 1055... Loss: 0.06494... Val Loss: 0.04889\n",
      "Epoch: 1056/2000... Step: 1056... Loss: 0.06490... Val Loss: 0.04883\n",
      "Epoch: 1057/2000... Step: 1057... Loss: 0.06486... Val Loss: 0.04877\n",
      "Epoch: 1058/2000... Step: 1058... Loss: 0.06482... Val Loss: 0.04871\n",
      "Epoch: 1059/2000... Step: 1059... Loss: 0.06477... Val Loss: 0.04865\n",
      "Epoch: 1060/2000... Step: 1060... Loss: 0.06473... Val Loss: 0.04860\n",
      "Epoch: 1061/2000... Step: 1061... Loss: 0.06469... Val Loss: 0.04854\n",
      "Epoch: 1062/2000... Step: 1062... Loss: 0.06465... Val Loss: 0.04848\n",
      "Epoch: 1063/2000... Step: 1063... Loss: 0.06461... Val Loss: 0.04842\n",
      "Epoch: 1064/2000... Step: 1064... Loss: 0.06457... Val Loss: 0.04837\n",
      "Epoch: 1065/2000... Step: 1065... Loss: 0.06453... Val Loss: 0.04831\n",
      "Epoch: 1066/2000... Step: 1066... Loss: 0.06449... Val Loss: 0.04826\n",
      "Epoch: 1067/2000... Step: 1067... Loss: 0.06445... Val Loss: 0.04820\n",
      "Epoch: 1068/2000... Step: 1068... Loss: 0.06441... Val Loss: 0.04814\n",
      "Epoch: 1069/2000... Step: 1069... Loss: 0.06437... Val Loss: 0.04809\n",
      "Epoch: 1070/2000... Step: 1070... Loss: 0.06433... Val Loss: 0.04803\n",
      "Epoch: 1071/2000... Step: 1071... Loss: 0.06429... Val Loss: 0.04798\n",
      "Epoch: 1072/2000... Step: 1072... Loss: 0.06425... Val Loss: 0.04792\n",
      "Epoch: 1073/2000... Step: 1073... Loss: 0.06421... Val Loss: 0.04787\n",
      "Epoch: 1074/2000... Step: 1074... Loss: 0.06417... Val Loss: 0.04781\n",
      "Epoch: 1075/2000... Step: 1075... Loss: 0.06413... Val Loss: 0.04776\n",
      "Epoch: 1076/2000... Step: 1076... Loss: 0.06409... Val Loss: 0.04770\n",
      "Epoch: 1077/2000... Step: 1077... Loss: 0.06405... Val Loss: 0.04765\n",
      "Epoch: 1078/2000... Step: 1078... Loss: 0.06401... Val Loss: 0.04759\n",
      "Epoch: 1079/2000... Step: 1079... Loss: 0.06397... Val Loss: 0.04754\n",
      "Epoch: 1080/2000... Step: 1080... Loss: 0.06394... Val Loss: 0.04749\n",
      "Epoch: 1081/2000... Step: 1081... Loss: 0.06390... Val Loss: 0.04743\n",
      "Epoch: 1082/2000... Step: 1082... Loss: 0.06386... Val Loss: 0.04738\n",
      "Epoch: 1083/2000... Step: 1083... Loss: 0.06382... Val Loss: 0.04733\n",
      "Epoch: 1084/2000... Step: 1084... Loss: 0.06378... Val Loss: 0.04727\n",
      "Epoch: 1085/2000... Step: 1085... Loss: 0.06375... Val Loss: 0.04722\n",
      "Epoch: 1086/2000... Step: 1086... Loss: 0.06371... Val Loss: 0.04717\n",
      "Epoch: 1087/2000... Step: 1087... Loss: 0.06367... Val Loss: 0.04712\n",
      "Epoch: 1088/2000... Step: 1088... Loss: 0.06363... Val Loss: 0.04706\n",
      "Epoch: 1089/2000... Step: 1089... Loss: 0.06360... Val Loss: 0.04701\n",
      "Epoch: 1090/2000... Step: 1090... Loss: 0.06356... Val Loss: 0.04696\n",
      "Epoch: 1091/2000... Step: 1091... Loss: 0.06352... Val Loss: 0.04691\n",
      "Epoch: 1092/2000... Step: 1092... Loss: 0.06349... Val Loss: 0.04686\n",
      "Epoch: 1093/2000... Step: 1093... Loss: 0.06345... Val Loss: 0.04681\n",
      "Epoch: 1094/2000... Step: 1094... Loss: 0.06341... Val Loss: 0.04676\n",
      "Epoch: 1095/2000... Step: 1095... Loss: 0.06338... Val Loss: 0.04671\n",
      "Epoch: 1096/2000... Step: 1096... Loss: 0.06334... Val Loss: 0.04665\n",
      "Epoch: 1097/2000... Step: 1097... Loss: 0.06331... Val Loss: 0.04660\n",
      "Epoch: 1098/2000... Step: 1098... Loss: 0.06327... Val Loss: 0.04655\n",
      "Epoch: 1099/2000... Step: 1099... Loss: 0.06324... Val Loss: 0.04650\n",
      "Epoch: 1100/2000... Step: 1100... Loss: 0.06320... Val Loss: 0.04645\n",
      "Epoch: 1101/2000... Step: 1101... Loss: 0.06316... Val Loss: 0.04640\n",
      "Epoch: 1102/2000... Step: 1102... Loss: 0.06313... Val Loss: 0.04635\n",
      "Epoch: 1103/2000... Step: 1103... Loss: 0.06309... Val Loss: 0.04630\n",
      "Epoch: 1104/2000... Step: 1104... Loss: 0.06306... Val Loss: 0.04626\n",
      "Epoch: 1105/2000... Step: 1105... Loss: 0.06302... Val Loss: 0.04621\n",
      "Epoch: 1106/2000... Step: 1106... Loss: 0.06299... Val Loss: 0.04616\n",
      "Epoch: 1107/2000... Step: 1107... Loss: 0.06296... Val Loss: 0.04611\n",
      "Epoch: 1108/2000... Step: 1108... Loss: 0.06292... Val Loss: 0.04606\n",
      "Epoch: 1109/2000... Step: 1109... Loss: 0.06289... Val Loss: 0.04601\n",
      "Epoch: 1110/2000... Step: 1110... Loss: 0.06285... Val Loss: 0.04596\n",
      "Epoch: 1111/2000... Step: 1111... Loss: 0.06282... Val Loss: 0.04592\n",
      "Epoch: 1112/2000... Step: 1112... Loss: 0.06278... Val Loss: 0.04587\n",
      "Epoch: 1113/2000... Step: 1113... Loss: 0.06275... Val Loss: 0.04582\n",
      "Epoch: 1114/2000... Step: 1114... Loss: 0.06272... Val Loss: 0.04577\n",
      "Epoch: 1115/2000... Step: 1115... Loss: 0.06268... Val Loss: 0.04572\n",
      "Epoch: 1116/2000... Step: 1116... Loss: 0.06265... Val Loss: 0.04568\n",
      "Epoch: 1117/2000... Step: 1117... Loss: 0.06262... Val Loss: 0.04563\n",
      "Epoch: 1118/2000... Step: 1118... Loss: 0.06258... Val Loss: 0.04558\n",
      "Epoch: 1119/2000... Step: 1119... Loss: 0.06255... Val Loss: 0.04554\n",
      "Epoch: 1120/2000... Step: 1120... Loss: 0.06252... Val Loss: 0.04549\n",
      "Epoch: 1121/2000... Step: 1121... Loss: 0.06249... Val Loss: 0.04544\n",
      "Epoch: 1122/2000... Step: 1122... Loss: 0.06245... Val Loss: 0.04540\n",
      "Epoch: 1123/2000... Step: 1123... Loss: 0.06242... Val Loss: 0.04535\n",
      "Epoch: 1124/2000... Step: 1124... Loss: 0.06239... Val Loss: 0.04531\n",
      "Epoch: 1125/2000... Step: 1125... Loss: 0.06236... Val Loss: 0.04526\n",
      "Epoch: 1126/2000... Step: 1126... Loss: 0.06232... Val Loss: 0.04521\n",
      "Epoch: 1127/2000... Step: 1127... Loss: 0.06229... Val Loss: 0.04517\n",
      "Epoch: 1128/2000... Step: 1128... Loss: 0.06226... Val Loss: 0.04512\n",
      "Epoch: 1129/2000... Step: 1129... Loss: 0.06223... Val Loss: 0.04508\n",
      "Epoch: 1130/2000... Step: 1130... Loss: 0.06220... Val Loss: 0.04503\n",
      "Epoch: 1131/2000... Step: 1131... Loss: 0.06217... Val Loss: 0.04499\n",
      "Epoch: 1132/2000... Step: 1132... Loss: 0.06213... Val Loss: 0.04494\n",
      "Epoch: 1133/2000... Step: 1133... Loss: 0.06210... Val Loss: 0.04490\n",
      "Epoch: 1134/2000... Step: 1134... Loss: 0.06207... Val Loss: 0.04486\n",
      "Epoch: 1135/2000... Step: 1135... Loss: 0.06204... Val Loss: 0.04481\n",
      "Epoch: 1136/2000... Step: 1136... Loss: 0.06201... Val Loss: 0.04477\n",
      "Epoch: 1137/2000... Step: 1137... Loss: 0.06198... Val Loss: 0.04472\n",
      "Epoch: 1138/2000... Step: 1138... Loss: 0.06195... Val Loss: 0.04468\n",
      "Epoch: 1139/2000... Step: 1139... Loss: 0.06192... Val Loss: 0.04464\n",
      "Epoch: 1140/2000... Step: 1140... Loss: 0.06189... Val Loss: 0.04459\n",
      "Epoch: 1141/2000... Step: 1141... Loss: 0.06186... Val Loss: 0.04455\n",
      "Epoch: 1142/2000... Step: 1142... Loss: 0.06183... Val Loss: 0.04451\n",
      "Epoch: 1143/2000... Step: 1143... Loss: 0.06180... Val Loss: 0.04446\n",
      "Epoch: 1144/2000... Step: 1144... Loss: 0.06177... Val Loss: 0.04442\n",
      "Epoch: 1145/2000... Step: 1145... Loss: 0.06174... Val Loss: 0.04438\n",
      "Epoch: 1146/2000... Step: 1146... Loss: 0.06171... Val Loss: 0.04434\n",
      "Epoch: 1147/2000... Step: 1147... Loss: 0.06168... Val Loss: 0.04429\n",
      "Epoch: 1148/2000... Step: 1148... Loss: 0.06165... Val Loss: 0.04425\n",
      "Epoch: 1149/2000... Step: 1149... Loss: 0.06162... Val Loss: 0.04421\n",
      "Epoch: 1150/2000... Step: 1150... Loss: 0.06159... Val Loss: 0.04417\n",
      "Epoch: 1151/2000... Step: 1151... Loss: 0.06156... Val Loss: 0.04413\n",
      "Epoch: 1152/2000... Step: 1152... Loss: 0.06153... Val Loss: 0.04408\n",
      "Epoch: 1153/2000... Step: 1153... Loss: 0.06150... Val Loss: 0.04404\n",
      "Epoch: 1154/2000... Step: 1154... Loss: 0.06147... Val Loss: 0.04400\n",
      "Epoch: 1155/2000... Step: 1155... Loss: 0.06144... Val Loss: 0.04396\n",
      "Epoch: 1156/2000... Step: 1156... Loss: 0.06141... Val Loss: 0.04392\n",
      "Epoch: 1157/2000... Step: 1157... Loss: 0.06139... Val Loss: 0.04388\n",
      "Epoch: 1158/2000... Step: 1158... Loss: 0.06136... Val Loss: 0.04384\n",
      "Epoch: 1159/2000... Step: 1159... Loss: 0.06133... Val Loss: 0.04380\n",
      "Epoch: 1160/2000... Step: 1160... Loss: 0.06130... Val Loss: 0.04376\n",
      "Epoch: 1161/2000... Step: 1161... Loss: 0.06127... Val Loss: 0.04372\n",
      "Epoch: 1162/2000... Step: 1162... Loss: 0.06124... Val Loss: 0.04368\n",
      "Epoch: 1163/2000... Step: 1163... Loss: 0.06122... Val Loss: 0.04364\n",
      "Epoch: 1164/2000... Step: 1164... Loss: 0.06119... Val Loss: 0.04360\n",
      "Epoch: 1165/2000... Step: 1165... Loss: 0.06116... Val Loss: 0.04356\n",
      "Epoch: 1166/2000... Step: 1166... Loss: 0.06113... Val Loss: 0.04352\n",
      "Epoch: 1167/2000... Step: 1167... Loss: 0.06110... Val Loss: 0.04348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1168/2000... Step: 1168... Loss: 0.06108... Val Loss: 0.04344\n",
      "Epoch: 1169/2000... Step: 1169... Loss: 0.06105... Val Loss: 0.04340\n",
      "Epoch: 1170/2000... Step: 1170... Loss: 0.06102... Val Loss: 0.04336\n",
      "Epoch: 1171/2000... Step: 1171... Loss: 0.06100... Val Loss: 0.04332\n",
      "Epoch: 1172/2000... Step: 1172... Loss: 0.06097... Val Loss: 0.04328\n",
      "Epoch: 1173/2000... Step: 1173... Loss: 0.06094... Val Loss: 0.04324\n",
      "Epoch: 1174/2000... Step: 1174... Loss: 0.06091... Val Loss: 0.04321\n",
      "Epoch: 1175/2000... Step: 1175... Loss: 0.06089... Val Loss: 0.04317\n",
      "Epoch: 1176/2000... Step: 1176... Loss: 0.06086... Val Loss: 0.04313\n",
      "Epoch: 1177/2000... Step: 1177... Loss: 0.06083... Val Loss: 0.04309\n",
      "Epoch: 1178/2000... Step: 1178... Loss: 0.06081... Val Loss: 0.04305\n",
      "Epoch: 1179/2000... Step: 1179... Loss: 0.06078... Val Loss: 0.04301\n",
      "Epoch: 1180/2000... Step: 1180... Loss: 0.06075... Val Loss: 0.04298\n",
      "Epoch: 1181/2000... Step: 1181... Loss: 0.06073... Val Loss: 0.04294\n",
      "Epoch: 1182/2000... Step: 1182... Loss: 0.06070... Val Loss: 0.04290\n",
      "Epoch: 1183/2000... Step: 1183... Loss: 0.06068... Val Loss: 0.04286\n",
      "Epoch: 1184/2000... Step: 1184... Loss: 0.06065... Val Loss: 0.04283\n",
      "Epoch: 1185/2000... Step: 1185... Loss: 0.06062... Val Loss: 0.04279\n",
      "Epoch: 1186/2000... Step: 1186... Loss: 0.06060... Val Loss: 0.04275\n",
      "Epoch: 1187/2000... Step: 1187... Loss: 0.06057... Val Loss: 0.04272\n",
      "Epoch: 1188/2000... Step: 1188... Loss: 0.06055... Val Loss: 0.04268\n",
      "Epoch: 1189/2000... Step: 1189... Loss: 0.06052... Val Loss: 0.04264\n",
      "Epoch: 1190/2000... Step: 1190... Loss: 0.06050... Val Loss: 0.04261\n",
      "Epoch: 1191/2000... Step: 1191... Loss: 0.06047... Val Loss: 0.04257\n",
      "Epoch: 1192/2000... Step: 1192... Loss: 0.06044... Val Loss: 0.04253\n",
      "Epoch: 1193/2000... Step: 1193... Loss: 0.06042... Val Loss: 0.04250\n",
      "Epoch: 1194/2000... Step: 1194... Loss: 0.06039... Val Loss: 0.04246\n",
      "Epoch: 1195/2000... Step: 1195... Loss: 0.06037... Val Loss: 0.04243\n",
      "Epoch: 1196/2000... Step: 1196... Loss: 0.06034... Val Loss: 0.04239\n",
      "Epoch: 1197/2000... Step: 1197... Loss: 0.06032... Val Loss: 0.04236\n",
      "Epoch: 1198/2000... Step: 1198... Loss: 0.06029... Val Loss: 0.04232\n",
      "Epoch: 1199/2000... Step: 1199... Loss: 0.06027... Val Loss: 0.04228\n",
      "Epoch: 1200/2000... Step: 1200... Loss: 0.06024... Val Loss: 0.04225\n",
      "Epoch: 1201/2000... Step: 1201... Loss: 0.06022... Val Loss: 0.04221\n",
      "Epoch: 1202/2000... Step: 1202... Loss: 0.06020... Val Loss: 0.04218\n",
      "Epoch: 1203/2000... Step: 1203... Loss: 0.06017... Val Loss: 0.04214\n",
      "Epoch: 1204/2000... Step: 1204... Loss: 0.06015... Val Loss: 0.04211\n",
      "Epoch: 1205/2000... Step: 1205... Loss: 0.06012... Val Loss: 0.04208\n",
      "Epoch: 1206/2000... Step: 1206... Loss: 0.06010... Val Loss: 0.04204\n",
      "Epoch: 1207/2000... Step: 1207... Loss: 0.06007... Val Loss: 0.04201\n",
      "Epoch: 1208/2000... Step: 1208... Loss: 0.06005... Val Loss: 0.04197\n",
      "Epoch: 1209/2000... Step: 1209... Loss: 0.06003... Val Loss: 0.04194\n",
      "Epoch: 1210/2000... Step: 1210... Loss: 0.06000... Val Loss: 0.04190\n",
      "Epoch: 1211/2000... Step: 1211... Loss: 0.05998... Val Loss: 0.04187\n",
      "Epoch: 1212/2000... Step: 1212... Loss: 0.05995... Val Loss: 0.04184\n",
      "Epoch: 1213/2000... Step: 1213... Loss: 0.05993... Val Loss: 0.04180\n",
      "Epoch: 1214/2000... Step: 1214... Loss: 0.05991... Val Loss: 0.04177\n",
      "Epoch: 1215/2000... Step: 1215... Loss: 0.05988... Val Loss: 0.04174\n",
      "Epoch: 1216/2000... Step: 1216... Loss: 0.05986... Val Loss: 0.04170\n",
      "Epoch: 1217/2000... Step: 1217... Loss: 0.05984... Val Loss: 0.04167\n",
      "Epoch: 1218/2000... Step: 1218... Loss: 0.05981... Val Loss: 0.04164\n",
      "Epoch: 1219/2000... Step: 1219... Loss: 0.05979... Val Loss: 0.04160\n",
      "Epoch: 1220/2000... Step: 1220... Loss: 0.05977... Val Loss: 0.04157\n",
      "Epoch: 1221/2000... Step: 1221... Loss: 0.05975... Val Loss: 0.04154\n",
      "Epoch: 1222/2000... Step: 1222... Loss: 0.05972... Val Loss: 0.04151\n",
      "Epoch: 1223/2000... Step: 1223... Loss: 0.05970... Val Loss: 0.04147\n",
      "Epoch: 1224/2000... Step: 1224... Loss: 0.05968... Val Loss: 0.04144\n",
      "Epoch: 1225/2000... Step: 1225... Loss: 0.05965... Val Loss: 0.04141\n",
      "Epoch: 1226/2000... Step: 1226... Loss: 0.05963... Val Loss: 0.04138\n",
      "Epoch: 1227/2000... Step: 1227... Loss: 0.05961... Val Loss: 0.04134\n",
      "Epoch: 1228/2000... Step: 1228... Loss: 0.05959... Val Loss: 0.04131\n",
      "Epoch: 1229/2000... Step: 1229... Loss: 0.05956... Val Loss: 0.04128\n",
      "Epoch: 1230/2000... Step: 1230... Loss: 0.05954... Val Loss: 0.04125\n",
      "Epoch: 1231/2000... Step: 1231... Loss: 0.05952... Val Loss: 0.04122\n",
      "Epoch: 1232/2000... Step: 1232... Loss: 0.05950... Val Loss: 0.04119\n",
      "Epoch: 1233/2000... Step: 1233... Loss: 0.05947... Val Loss: 0.04115\n",
      "Epoch: 1234/2000... Step: 1234... Loss: 0.05945... Val Loss: 0.04112\n",
      "Epoch: 1235/2000... Step: 1235... Loss: 0.05943... Val Loss: 0.04109\n",
      "Epoch: 1236/2000... Step: 1236... Loss: 0.05941... Val Loss: 0.04106\n",
      "Epoch: 1237/2000... Step: 1237... Loss: 0.05939... Val Loss: 0.04103\n",
      "Epoch: 1238/2000... Step: 1238... Loss: 0.05937... Val Loss: 0.04100\n",
      "Epoch: 1239/2000... Step: 1239... Loss: 0.05934... Val Loss: 0.04097\n",
      "Epoch: 1240/2000... Step: 1240... Loss: 0.05932... Val Loss: 0.04094\n",
      "Epoch: 1241/2000... Step: 1241... Loss: 0.05930... Val Loss: 0.04091\n",
      "Epoch: 1242/2000... Step: 1242... Loss: 0.05928... Val Loss: 0.04088\n",
      "Epoch: 1243/2000... Step: 1243... Loss: 0.05926... Val Loss: 0.04085\n",
      "Epoch: 1244/2000... Step: 1244... Loss: 0.05924... Val Loss: 0.04082\n",
      "Epoch: 1245/2000... Step: 1245... Loss: 0.05921... Val Loss: 0.04079\n",
      "Epoch: 1246/2000... Step: 1246... Loss: 0.05919... Val Loss: 0.04076\n",
      "Epoch: 1247/2000... Step: 1247... Loss: 0.05917... Val Loss: 0.04073\n",
      "Epoch: 1248/2000... Step: 1248... Loss: 0.05915... Val Loss: 0.04070\n",
      "Epoch: 1249/2000... Step: 1249... Loss: 0.05913... Val Loss: 0.04067\n",
      "Epoch: 1250/2000... Step: 1250... Loss: 0.05911... Val Loss: 0.04064\n",
      "Epoch: 1251/2000... Step: 1251... Loss: 0.05909... Val Loss: 0.04061\n",
      "Epoch: 1252/2000... Step: 1252... Loss: 0.05907... Val Loss: 0.04058\n",
      "Epoch: 1253/2000... Step: 1253... Loss: 0.05905... Val Loss: 0.04055\n",
      "Epoch: 1254/2000... Step: 1254... Loss: 0.05903... Val Loss: 0.04052\n",
      "Epoch: 1255/2000... Step: 1255... Loss: 0.05901... Val Loss: 0.04049\n",
      "Epoch: 1256/2000... Step: 1256... Loss: 0.05898... Val Loss: 0.04046\n",
      "Epoch: 1257/2000... Step: 1257... Loss: 0.05896... Val Loss: 0.04043\n",
      "Epoch: 1258/2000... Step: 1258... Loss: 0.05894... Val Loss: 0.04040\n",
      "Epoch: 1259/2000... Step: 1259... Loss: 0.05892... Val Loss: 0.04038\n",
      "Epoch: 1260/2000... Step: 1260... Loss: 0.05890... Val Loss: 0.04035\n",
      "Epoch: 1261/2000... Step: 1261... Loss: 0.05888... Val Loss: 0.04032\n",
      "Epoch: 1262/2000... Step: 1262... Loss: 0.05886... Val Loss: 0.04029\n",
      "Epoch: 1263/2000... Step: 1263... Loss: 0.05884... Val Loss: 0.04026\n",
      "Epoch: 1264/2000... Step: 1264... Loss: 0.05882... Val Loss: 0.04023\n",
      "Epoch: 1265/2000... Step: 1265... Loss: 0.05880... Val Loss: 0.04020\n",
      "Epoch: 1266/2000... Step: 1266... Loss: 0.05878... Val Loss: 0.04018\n",
      "Epoch: 1267/2000... Step: 1267... Loss: 0.05876... Val Loss: 0.04015\n",
      "Epoch: 1268/2000... Step: 1268... Loss: 0.05874... Val Loss: 0.04012\n",
      "Epoch: 1269/2000... Step: 1269... Loss: 0.05872... Val Loss: 0.04009\n",
      "Epoch: 1270/2000... Step: 1270... Loss: 0.05870... Val Loss: 0.04007\n",
      "Epoch: 1271/2000... Step: 1271... Loss: 0.05868... Val Loss: 0.04004\n",
      "Epoch: 1272/2000... Step: 1272... Loss: 0.05866... Val Loss: 0.04001\n",
      "Epoch: 1273/2000... Step: 1273... Loss: 0.05864... Val Loss: 0.03998\n",
      "Epoch: 1274/2000... Step: 1274... Loss: 0.05862... Val Loss: 0.03996\n",
      "Epoch: 1275/2000... Step: 1275... Loss: 0.05861... Val Loss: 0.03993\n",
      "Epoch: 1276/2000... Step: 1276... Loss: 0.05859... Val Loss: 0.03990\n",
      "Epoch: 1277/2000... Step: 1277... Loss: 0.05857... Val Loss: 0.03987\n",
      "Epoch: 1278/2000... Step: 1278... Loss: 0.05855... Val Loss: 0.03985\n",
      "Epoch: 1279/2000... Step: 1279... Loss: 0.05853... Val Loss: 0.03982\n",
      "Epoch: 1280/2000... Step: 1280... Loss: 0.05851... Val Loss: 0.03979\n",
      "Epoch: 1281/2000... Step: 1281... Loss: 0.05849... Val Loss: 0.03977\n",
      "Epoch: 1282/2000... Step: 1282... Loss: 0.05847... Val Loss: 0.03974\n",
      "Epoch: 1283/2000... Step: 1283... Loss: 0.05845... Val Loss: 0.03971\n",
      "Epoch: 1284/2000... Step: 1284... Loss: 0.05843... Val Loss: 0.03969\n",
      "Epoch: 1285/2000... Step: 1285... Loss: 0.05841... Val Loss: 0.03966\n",
      "Epoch: 1286/2000... Step: 1286... Loss: 0.05840... Val Loss: 0.03964\n",
      "Epoch: 1287/2000... Step: 1287... Loss: 0.05838... Val Loss: 0.03961\n",
      "Epoch: 1288/2000... Step: 1288... Loss: 0.05836... Val Loss: 0.03958\n",
      "Epoch: 1289/2000... Step: 1289... Loss: 0.05834... Val Loss: 0.03956\n",
      "Epoch: 1290/2000... Step: 1290... Loss: 0.05832... Val Loss: 0.03953\n",
      "Epoch: 1291/2000... Step: 1291... Loss: 0.05830... Val Loss: 0.03951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1292/2000... Step: 1292... Loss: 0.05828... Val Loss: 0.03948\n",
      "Epoch: 1293/2000... Step: 1293... Loss: 0.05827... Val Loss: 0.03945\n",
      "Epoch: 1294/2000... Step: 1294... Loss: 0.05825... Val Loss: 0.03943\n",
      "Epoch: 1295/2000... Step: 1295... Loss: 0.05823... Val Loss: 0.03940\n",
      "Epoch: 1296/2000... Step: 1296... Loss: 0.05821... Val Loss: 0.03938\n",
      "Epoch: 1297/2000... Step: 1297... Loss: 0.05819... Val Loss: 0.03935\n",
      "Epoch: 1298/2000... Step: 1298... Loss: 0.05817... Val Loss: 0.03933\n",
      "Epoch: 1299/2000... Step: 1299... Loss: 0.05816... Val Loss: 0.03930\n",
      "Epoch: 1300/2000... Step: 1300... Loss: 0.05814... Val Loss: 0.03928\n",
      "Epoch: 1301/2000... Step: 1301... Loss: 0.05812... Val Loss: 0.03925\n",
      "Epoch: 1302/2000... Step: 1302... Loss: 0.05810... Val Loss: 0.03923\n",
      "Epoch: 1303/2000... Step: 1303... Loss: 0.05808... Val Loss: 0.03920\n",
      "Epoch: 1304/2000... Step: 1304... Loss: 0.05807... Val Loss: 0.03918\n",
      "Epoch: 1305/2000... Step: 1305... Loss: 0.05805... Val Loss: 0.03915\n",
      "Epoch: 1306/2000... Step: 1306... Loss: 0.05803... Val Loss: 0.03913\n",
      "Epoch: 1307/2000... Step: 1307... Loss: 0.05801... Val Loss: 0.03910\n",
      "Epoch: 1308/2000... Step: 1308... Loss: 0.05800... Val Loss: 0.03908\n",
      "Epoch: 1309/2000... Step: 1309... Loss: 0.05798... Val Loss: 0.03906\n",
      "Epoch: 1310/2000... Step: 1310... Loss: 0.05796... Val Loss: 0.03903\n",
      "Epoch: 1311/2000... Step: 1311... Loss: 0.05794... Val Loss: 0.03901\n",
      "Epoch: 1312/2000... Step: 1312... Loss: 0.05793... Val Loss: 0.03898\n",
      "Epoch: 1313/2000... Step: 1313... Loss: 0.05791... Val Loss: 0.03896\n",
      "Epoch: 1314/2000... Step: 1314... Loss: 0.05789... Val Loss: 0.03894\n",
      "Epoch: 1315/2000... Step: 1315... Loss: 0.05787... Val Loss: 0.03891\n",
      "Epoch: 1316/2000... Step: 1316... Loss: 0.05786... Val Loss: 0.03889\n",
      "Epoch: 1317/2000... Step: 1317... Loss: 0.05784... Val Loss: 0.03886\n",
      "Epoch: 1318/2000... Step: 1318... Loss: 0.05782... Val Loss: 0.03884\n",
      "Epoch: 1319/2000... Step: 1319... Loss: 0.05780... Val Loss: 0.03882\n",
      "Epoch: 1320/2000... Step: 1320... Loss: 0.05779... Val Loss: 0.03879\n",
      "Epoch: 1321/2000... Step: 1321... Loss: 0.05777... Val Loss: 0.03877\n",
      "Epoch: 1322/2000... Step: 1322... Loss: 0.05775... Val Loss: 0.03875\n",
      "Epoch: 1323/2000... Step: 1323... Loss: 0.05774... Val Loss: 0.03872\n",
      "Epoch: 1324/2000... Step: 1324... Loss: 0.05772... Val Loss: 0.03870\n",
      "Epoch: 1325/2000... Step: 1325... Loss: 0.05770... Val Loss: 0.03868\n",
      "Epoch: 1326/2000... Step: 1326... Loss: 0.05769... Val Loss: 0.03866\n",
      "Epoch: 1327/2000... Step: 1327... Loss: 0.05767... Val Loss: 0.03863\n",
      "Epoch: 1328/2000... Step: 1328... Loss: 0.05765... Val Loss: 0.03861\n",
      "Epoch: 1329/2000... Step: 1329... Loss: 0.05764... Val Loss: 0.03859\n",
      "Epoch: 1330/2000... Step: 1330... Loss: 0.05762... Val Loss: 0.03856\n",
      "Epoch: 1331/2000... Step: 1331... Loss: 0.05760... Val Loss: 0.03854\n",
      "Epoch: 1332/2000... Step: 1332... Loss: 0.05759... Val Loss: 0.03852\n",
      "Epoch: 1333/2000... Step: 1333... Loss: 0.05757... Val Loss: 0.03850\n",
      "Epoch: 1334/2000... Step: 1334... Loss: 0.05755... Val Loss: 0.03847\n",
      "Epoch: 1335/2000... Step: 1335... Loss: 0.05754... Val Loss: 0.03845\n",
      "Epoch: 1336/2000... Step: 1336... Loss: 0.05752... Val Loss: 0.03843\n",
      "Epoch: 1337/2000... Step: 1337... Loss: 0.05750... Val Loss: 0.03841\n",
      "Epoch: 1338/2000... Step: 1338... Loss: 0.05749... Val Loss: 0.03839\n",
      "Epoch: 1339/2000... Step: 1339... Loss: 0.05747... Val Loss: 0.03836\n",
      "Epoch: 1340/2000... Step: 1340... Loss: 0.05746... Val Loss: 0.03834\n",
      "Epoch: 1341/2000... Step: 1341... Loss: 0.05744... Val Loss: 0.03832\n",
      "Epoch: 1342/2000... Step: 1342... Loss: 0.05742... Val Loss: 0.03830\n",
      "Epoch: 1343/2000... Step: 1343... Loss: 0.05741... Val Loss: 0.03828\n",
      "Epoch: 1344/2000... Step: 1344... Loss: 0.05739... Val Loss: 0.03825\n",
      "Epoch: 1345/2000... Step: 1345... Loss: 0.05738... Val Loss: 0.03823\n",
      "Epoch: 1346/2000... Step: 1346... Loss: 0.05736... Val Loss: 0.03821\n",
      "Epoch: 1347/2000... Step: 1347... Loss: 0.05734... Val Loss: 0.03819\n",
      "Epoch: 1348/2000... Step: 1348... Loss: 0.05733... Val Loss: 0.03817\n",
      "Epoch: 1349/2000... Step: 1349... Loss: 0.05731... Val Loss: 0.03815\n",
      "Epoch: 1350/2000... Step: 1350... Loss: 0.05730... Val Loss: 0.03813\n",
      "Epoch: 1351/2000... Step: 1351... Loss: 0.05728... Val Loss: 0.03811\n",
      "Epoch: 1352/2000... Step: 1352... Loss: 0.05727... Val Loss: 0.03808\n",
      "Epoch: 1353/2000... Step: 1353... Loss: 0.05725... Val Loss: 0.03806\n",
      "Epoch: 1354/2000... Step: 1354... Loss: 0.05723... Val Loss: 0.03804\n",
      "Epoch: 1355/2000... Step: 1355... Loss: 0.05722... Val Loss: 0.03802\n",
      "Epoch: 1356/2000... Step: 1356... Loss: 0.05720... Val Loss: 0.03800\n",
      "Epoch: 1357/2000... Step: 1357... Loss: 0.05719... Val Loss: 0.03798\n",
      "Epoch: 1358/2000... Step: 1358... Loss: 0.05717... Val Loss: 0.03796\n",
      "Epoch: 1359/2000... Step: 1359... Loss: 0.05716... Val Loss: 0.03794\n",
      "Epoch: 1360/2000... Step: 1360... Loss: 0.05714... Val Loss: 0.03792\n",
      "Epoch: 1361/2000... Step: 1361... Loss: 0.05713... Val Loss: 0.03790\n",
      "Epoch: 1362/2000... Step: 1362... Loss: 0.05711... Val Loss: 0.03788\n",
      "Epoch: 1363/2000... Step: 1363... Loss: 0.05710... Val Loss: 0.03786\n",
      "Epoch: 1364/2000... Step: 1364... Loss: 0.05708... Val Loss: 0.03784\n",
      "Epoch: 1365/2000... Step: 1365... Loss: 0.05707... Val Loss: 0.03782\n",
      "Epoch: 1366/2000... Step: 1366... Loss: 0.05705... Val Loss: 0.03780\n",
      "Epoch: 1367/2000... Step: 1367... Loss: 0.05703... Val Loss: 0.03778\n",
      "Epoch: 1368/2000... Step: 1368... Loss: 0.05702... Val Loss: 0.03776\n",
      "Epoch: 1369/2000... Step: 1369... Loss: 0.05700... Val Loss: 0.03774\n",
      "Epoch: 1370/2000... Step: 1370... Loss: 0.05699... Val Loss: 0.03772\n",
      "Epoch: 1371/2000... Step: 1371... Loss: 0.05698... Val Loss: 0.03770\n",
      "Epoch: 1372/2000... Step: 1372... Loss: 0.05696... Val Loss: 0.03768\n",
      "Epoch: 1373/2000... Step: 1373... Loss: 0.05695... Val Loss: 0.03766\n",
      "Epoch: 1374/2000... Step: 1374... Loss: 0.05693... Val Loss: 0.03764\n",
      "Epoch: 1375/2000... Step: 1375... Loss: 0.05692... Val Loss: 0.03762\n",
      "Epoch: 1376/2000... Step: 1376... Loss: 0.05690... Val Loss: 0.03760\n",
      "Epoch: 1377/2000... Step: 1377... Loss: 0.05689... Val Loss: 0.03758\n",
      "Epoch: 1378/2000... Step: 1378... Loss: 0.05687... Val Loss: 0.03756\n",
      "Epoch: 1379/2000... Step: 1379... Loss: 0.05686... Val Loss: 0.03754\n",
      "Epoch: 1380/2000... Step: 1380... Loss: 0.05684... Val Loss: 0.03752\n",
      "Epoch: 1381/2000... Step: 1381... Loss: 0.05683... Val Loss: 0.03750\n",
      "Epoch: 1382/2000... Step: 1382... Loss: 0.05681... Val Loss: 0.03748\n",
      "Epoch: 1383/2000... Step: 1383... Loss: 0.05680... Val Loss: 0.03746\n",
      "Epoch: 1384/2000... Step: 1384... Loss: 0.05678... Val Loss: 0.03744\n",
      "Epoch: 1385/2000... Step: 1385... Loss: 0.05677... Val Loss: 0.03743\n",
      "Epoch: 1386/2000... Step: 1386... Loss: 0.05676... Val Loss: 0.03741\n",
      "Epoch: 1387/2000... Step: 1387... Loss: 0.05674... Val Loss: 0.03739\n",
      "Epoch: 1388/2000... Step: 1388... Loss: 0.05673... Val Loss: 0.03737\n",
      "Epoch: 1389/2000... Step: 1389... Loss: 0.05671... Val Loss: 0.03735\n",
      "Epoch: 1390/2000... Step: 1390... Loss: 0.05670... Val Loss: 0.03733\n",
      "Epoch: 1391/2000... Step: 1391... Loss: 0.05668... Val Loss: 0.03731\n",
      "Epoch: 1392/2000... Step: 1392... Loss: 0.05667... Val Loss: 0.03730\n",
      "Epoch: 1393/2000... Step: 1393... Loss: 0.05666... Val Loss: 0.03728\n",
      "Epoch: 1394/2000... Step: 1394... Loss: 0.05664... Val Loss: 0.03726\n",
      "Epoch: 1395/2000... Step: 1395... Loss: 0.05663... Val Loss: 0.03724\n",
      "Epoch: 1396/2000... Step: 1396... Loss: 0.05661... Val Loss: 0.03722\n",
      "Epoch: 1397/2000... Step: 1397... Loss: 0.05660... Val Loss: 0.03720\n",
      "Epoch: 1398/2000... Step: 1398... Loss: 0.05659... Val Loss: 0.03719\n",
      "Epoch: 1399/2000... Step: 1399... Loss: 0.05657... Val Loss: 0.03717\n",
      "Epoch: 1400/2000... Step: 1400... Loss: 0.05656... Val Loss: 0.03715\n",
      "Epoch: 1401/2000... Step: 1401... Loss: 0.05654... Val Loss: 0.03713\n",
      "Epoch: 1402/2000... Step: 1402... Loss: 0.05653... Val Loss: 0.03711\n",
      "Epoch: 1403/2000... Step: 1403... Loss: 0.05652... Val Loss: 0.03710\n",
      "Epoch: 1404/2000... Step: 1404... Loss: 0.05650... Val Loss: 0.03708\n",
      "Epoch: 1405/2000... Step: 1405... Loss: 0.05649... Val Loss: 0.03706\n",
      "Epoch: 1406/2000... Step: 1406... Loss: 0.05648... Val Loss: 0.03704\n",
      "Epoch: 1407/2000... Step: 1407... Loss: 0.05646... Val Loss: 0.03702\n",
      "Epoch: 1408/2000... Step: 1408... Loss: 0.05645... Val Loss: 0.03701\n",
      "Epoch: 1409/2000... Step: 1409... Loss: 0.05643... Val Loss: 0.03699\n",
      "Epoch: 1410/2000... Step: 1410... Loss: 0.05642... Val Loss: 0.03697\n",
      "Epoch: 1411/2000... Step: 1411... Loss: 0.05641... Val Loss: 0.03695\n",
      "Epoch: 1412/2000... Step: 1412... Loss: 0.05639... Val Loss: 0.03694\n",
      "Epoch: 1413/2000... Step: 1413... Loss: 0.05638... Val Loss: 0.03692\n",
      "Epoch: 1414/2000... Step: 1414... Loss: 0.05637... Val Loss: 0.03690\n",
      "Epoch: 1415/2000... Step: 1415... Loss: 0.05635... Val Loss: 0.03689\n",
      "Epoch: 1416/2000... Step: 1416... Loss: 0.05634... Val Loss: 0.03687\n",
      "Epoch: 1417/2000... Step: 1417... Loss: 0.05633... Val Loss: 0.03685\n",
      "Epoch: 1418/2000... Step: 1418... Loss: 0.05631... Val Loss: 0.03683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1419/2000... Step: 1419... Loss: 0.05630... Val Loss: 0.03682\n",
      "Epoch: 1420/2000... Step: 1420... Loss: 0.05629... Val Loss: 0.03680\n",
      "Epoch: 1421/2000... Step: 1421... Loss: 0.05627... Val Loss: 0.03678\n",
      "Epoch: 1422/2000... Step: 1422... Loss: 0.05626... Val Loss: 0.03677\n",
      "Epoch: 1423/2000... Step: 1423... Loss: 0.05625... Val Loss: 0.03675\n",
      "Epoch: 1424/2000... Step: 1424... Loss: 0.05623... Val Loss: 0.03673\n",
      "Epoch: 1425/2000... Step: 1425... Loss: 0.05622... Val Loss: 0.03672\n",
      "Epoch: 1426/2000... Step: 1426... Loss: 0.05621... Val Loss: 0.03670\n",
      "Epoch: 1427/2000... Step: 1427... Loss: 0.05620... Val Loss: 0.03668\n",
      "Epoch: 1428/2000... Step: 1428... Loss: 0.05618... Val Loss: 0.03667\n",
      "Epoch: 1429/2000... Step: 1429... Loss: 0.05617... Val Loss: 0.03665\n",
      "Epoch: 1430/2000... Step: 1430... Loss: 0.05616... Val Loss: 0.03663\n",
      "Epoch: 1431/2000... Step: 1431... Loss: 0.05614... Val Loss: 0.03662\n",
      "Epoch: 1432/2000... Step: 1432... Loss: 0.05613... Val Loss: 0.03660\n",
      "Epoch: 1433/2000... Step: 1433... Loss: 0.05612... Val Loss: 0.03658\n",
      "Epoch: 1434/2000... Step: 1434... Loss: 0.05611... Val Loss: 0.03657\n",
      "Epoch: 1435/2000... Step: 1435... Loss: 0.05609... Val Loss: 0.03655\n",
      "Epoch: 1436/2000... Step: 1436... Loss: 0.05608... Val Loss: 0.03654\n",
      "Epoch: 1437/2000... Step: 1437... Loss: 0.05607... Val Loss: 0.03652\n",
      "Epoch: 1438/2000... Step: 1438... Loss: 0.05605... Val Loss: 0.03650\n",
      "Epoch: 1439/2000... Step: 1439... Loss: 0.05604... Val Loss: 0.03649\n",
      "Epoch: 1440/2000... Step: 1440... Loss: 0.05603... Val Loss: 0.03647\n",
      "Epoch: 1441/2000... Step: 1441... Loss: 0.05602... Val Loss: 0.03646\n",
      "Epoch: 1442/2000... Step: 1442... Loss: 0.05600... Val Loss: 0.03644\n",
      "Epoch: 1443/2000... Step: 1443... Loss: 0.05599... Val Loss: 0.03643\n",
      "Epoch: 1444/2000... Step: 1444... Loss: 0.05598... Val Loss: 0.03641\n",
      "Epoch: 1445/2000... Step: 1445... Loss: 0.05597... Val Loss: 0.03639\n",
      "Epoch: 1446/2000... Step: 1446... Loss: 0.05595... Val Loss: 0.03638\n",
      "Epoch: 1447/2000... Step: 1447... Loss: 0.05594... Val Loss: 0.03636\n",
      "Epoch: 1448/2000... Step: 1448... Loss: 0.05593... Val Loss: 0.03635\n",
      "Epoch: 1449/2000... Step: 1449... Loss: 0.05592... Val Loss: 0.03633\n",
      "Epoch: 1450/2000... Step: 1450... Loss: 0.05590... Val Loss: 0.03632\n",
      "Epoch: 1451/2000... Step: 1451... Loss: 0.05589... Val Loss: 0.03630\n",
      "Epoch: 1452/2000... Step: 1452... Loss: 0.05588... Val Loss: 0.03629\n",
      "Epoch: 1453/2000... Step: 1453... Loss: 0.05587... Val Loss: 0.03627\n",
      "Epoch: 1454/2000... Step: 1454... Loss: 0.05585... Val Loss: 0.03626\n",
      "Epoch: 1455/2000... Step: 1455... Loss: 0.05584... Val Loss: 0.03624\n",
      "Epoch: 1456/2000... Step: 1456... Loss: 0.05583... Val Loss: 0.03623\n",
      "Epoch: 1457/2000... Step: 1457... Loss: 0.05582... Val Loss: 0.03621\n",
      "Epoch: 1458/2000... Step: 1458... Loss: 0.05581... Val Loss: 0.03620\n",
      "Epoch: 1459/2000... Step: 1459... Loss: 0.05579... Val Loss: 0.03618\n",
      "Epoch: 1460/2000... Step: 1460... Loss: 0.05578... Val Loss: 0.03617\n",
      "Epoch: 1461/2000... Step: 1461... Loss: 0.05577... Val Loss: 0.03615\n",
      "Epoch: 1462/2000... Step: 1462... Loss: 0.05576... Val Loss: 0.03614\n",
      "Epoch: 1463/2000... Step: 1463... Loss: 0.05574... Val Loss: 0.03612\n",
      "Epoch: 1464/2000... Step: 1464... Loss: 0.05573... Val Loss: 0.03611\n",
      "Epoch: 1465/2000... Step: 1465... Loss: 0.05572... Val Loss: 0.03609\n",
      "Epoch: 1466/2000... Step: 1466... Loss: 0.05571... Val Loss: 0.03608\n",
      "Epoch: 1467/2000... Step: 1467... Loss: 0.05570... Val Loss: 0.03606\n",
      "Epoch: 1468/2000... Step: 1468... Loss: 0.05568... Val Loss: 0.03605\n",
      "Epoch: 1469/2000... Step: 1469... Loss: 0.05567... Val Loss: 0.03603\n",
      "Epoch: 1470/2000... Step: 1470... Loss: 0.05566... Val Loss: 0.03602\n",
      "Epoch: 1471/2000... Step: 1471... Loss: 0.05565... Val Loss: 0.03600\n",
      "Epoch: 1472/2000... Step: 1472... Loss: 0.05564... Val Loss: 0.03599\n",
      "Epoch: 1473/2000... Step: 1473... Loss: 0.05563... Val Loss: 0.03598\n",
      "Epoch: 1474/2000... Step: 1474... Loss: 0.05561... Val Loss: 0.03596\n",
      "Epoch: 1475/2000... Step: 1475... Loss: 0.05560... Val Loss: 0.03595\n",
      "Epoch: 1476/2000... Step: 1476... Loss: 0.05559... Val Loss: 0.03593\n",
      "Epoch: 1477/2000... Step: 1477... Loss: 0.05558... Val Loss: 0.03592\n",
      "Epoch: 1478/2000... Step: 1478... Loss: 0.05557... Val Loss: 0.03590\n",
      "Epoch: 1479/2000... Step: 1479... Loss: 0.05556... Val Loss: 0.03589\n",
      "Epoch: 1480/2000... Step: 1480... Loss: 0.05554... Val Loss: 0.03588\n",
      "Epoch: 1481/2000... Step: 1481... Loss: 0.05553... Val Loss: 0.03586\n",
      "Epoch: 1482/2000... Step: 1482... Loss: 0.05552... Val Loss: 0.03585\n",
      "Epoch: 1483/2000... Step: 1483... Loss: 0.05551... Val Loss: 0.03583\n",
      "Epoch: 1484/2000... Step: 1484... Loss: 0.05550... Val Loss: 0.03582\n",
      "Epoch: 1485/2000... Step: 1485... Loss: 0.05549... Val Loss: 0.03581\n",
      "Epoch: 1486/2000... Step: 1486... Loss: 0.05547... Val Loss: 0.03579\n",
      "Epoch: 1487/2000... Step: 1487... Loss: 0.05546... Val Loss: 0.03578\n",
      "Epoch: 1488/2000... Step: 1488... Loss: 0.05545... Val Loss: 0.03577\n",
      "Epoch: 1489/2000... Step: 1489... Loss: 0.05544... Val Loss: 0.03575\n",
      "Epoch: 1490/2000... Step: 1490... Loss: 0.05543... Val Loss: 0.03574\n",
      "Epoch: 1491/2000... Step: 1491... Loss: 0.05542... Val Loss: 0.03573\n",
      "Epoch: 1492/2000... Step: 1492... Loss: 0.05541... Val Loss: 0.03571\n",
      "Epoch: 1493/2000... Step: 1493... Loss: 0.05539... Val Loss: 0.03570\n",
      "Epoch: 1494/2000... Step: 1494... Loss: 0.05538... Val Loss: 0.03568\n",
      "Epoch: 1495/2000... Step: 1495... Loss: 0.05537... Val Loss: 0.03567\n",
      "Epoch: 1496/2000... Step: 1496... Loss: 0.05536... Val Loss: 0.03566\n",
      "Epoch: 1497/2000... Step: 1497... Loss: 0.05535... Val Loss: 0.03565\n",
      "Epoch: 1498/2000... Step: 1498... Loss: 0.05534... Val Loss: 0.03563\n",
      "Epoch: 1499/2000... Step: 1499... Loss: 0.05533... Val Loss: 0.03562\n",
      "Epoch: 1500/2000... Step: 1500... Loss: 0.05532... Val Loss: 0.03561\n",
      "Epoch: 1501/2000... Step: 1501... Loss: 0.05530... Val Loss: 0.03559\n",
      "Epoch: 1502/2000... Step: 1502... Loss: 0.05529... Val Loss: 0.03558\n",
      "Epoch: 1503/2000... Step: 1503... Loss: 0.05528... Val Loss: 0.03557\n",
      "Epoch: 1504/2000... Step: 1504... Loss: 0.05527... Val Loss: 0.03555\n",
      "Epoch: 1505/2000... Step: 1505... Loss: 0.05526... Val Loss: 0.03554\n",
      "Epoch: 1506/2000... Step: 1506... Loss: 0.05525... Val Loss: 0.03553\n",
      "Epoch: 1507/2000... Step: 1507... Loss: 0.05524... Val Loss: 0.03551\n",
      "Epoch: 1508/2000... Step: 1508... Loss: 0.05523... Val Loss: 0.03550\n",
      "Epoch: 1509/2000... Step: 1509... Loss: 0.05522... Val Loss: 0.03549\n",
      "Epoch: 1510/2000... Step: 1510... Loss: 0.05521... Val Loss: 0.03548\n",
      "Epoch: 1511/2000... Step: 1511... Loss: 0.05519... Val Loss: 0.03546\n",
      "Epoch: 1512/2000... Step: 1512... Loss: 0.05518... Val Loss: 0.03545\n",
      "Epoch: 1513/2000... Step: 1513... Loss: 0.05517... Val Loss: 0.03544\n",
      "Epoch: 1514/2000... Step: 1514... Loss: 0.05516... Val Loss: 0.03543\n",
      "Epoch: 1515/2000... Step: 1515... Loss: 0.05515... Val Loss: 0.03541\n",
      "Epoch: 1516/2000... Step: 1516... Loss: 0.05514... Val Loss: 0.03540\n",
      "Epoch: 1517/2000... Step: 1517... Loss: 0.05513... Val Loss: 0.03539\n",
      "Epoch: 1518/2000... Step: 1518... Loss: 0.05512... Val Loss: 0.03538\n",
      "Epoch: 1519/2000... Step: 1519... Loss: 0.05511... Val Loss: 0.03536\n",
      "Epoch: 1520/2000... Step: 1520... Loss: 0.05510... Val Loss: 0.03535\n",
      "Epoch: 1521/2000... Step: 1521... Loss: 0.05509... Val Loss: 0.03534\n",
      "Epoch: 1522/2000... Step: 1522... Loss: 0.05508... Val Loss: 0.03533\n",
      "Epoch: 1523/2000... Step: 1523... Loss: 0.05506... Val Loss: 0.03531\n",
      "Epoch: 1524/2000... Step: 1524... Loss: 0.05505... Val Loss: 0.03530\n",
      "Epoch: 1525/2000... Step: 1525... Loss: 0.05504... Val Loss: 0.03529\n",
      "Epoch: 1526/2000... Step: 1526... Loss: 0.05503... Val Loss: 0.03528\n",
      "Epoch: 1527/2000... Step: 1527... Loss: 0.05502... Val Loss: 0.03526\n",
      "Epoch: 1528/2000... Step: 1528... Loss: 0.05501... Val Loss: 0.03525\n",
      "Epoch: 1529/2000... Step: 1529... Loss: 0.05500... Val Loss: 0.03524\n",
      "Epoch: 1530/2000... Step: 1530... Loss: 0.05499... Val Loss: 0.03523\n",
      "Epoch: 1531/2000... Step: 1531... Loss: 0.05498... Val Loss: 0.03522\n",
      "Epoch: 1532/2000... Step: 1532... Loss: 0.05497... Val Loss: 0.03520\n",
      "Epoch: 1533/2000... Step: 1533... Loss: 0.05496... Val Loss: 0.03519\n",
      "Epoch: 1534/2000... Step: 1534... Loss: 0.05495... Val Loss: 0.03518\n",
      "Epoch: 1535/2000... Step: 1535... Loss: 0.05494... Val Loss: 0.03517\n",
      "Epoch: 1536/2000... Step: 1536... Loss: 0.05493... Val Loss: 0.03516\n",
      "Epoch: 1537/2000... Step: 1537... Loss: 0.05492... Val Loss: 0.03515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1538/2000... Step: 1538... Loss: 0.05491... Val Loss: 0.03513\n",
      "Epoch: 1539/2000... Step: 1539... Loss: 0.05490... Val Loss: 0.03512\n",
      "Epoch: 1540/2000... Step: 1540... Loss: 0.05489... Val Loss: 0.03511\n",
      "Epoch: 1541/2000... Step: 1541... Loss: 0.05488... Val Loss: 0.03510\n",
      "Epoch: 1542/2000... Step: 1542... Loss: 0.05486... Val Loss: 0.03509\n",
      "Epoch: 1543/2000... Step: 1543... Loss: 0.05485... Val Loss: 0.03508\n",
      "Epoch: 1544/2000... Step: 1544... Loss: 0.05484... Val Loss: 0.03506\n",
      "Epoch: 1545/2000... Step: 1545... Loss: 0.05483... Val Loss: 0.03505\n",
      "Epoch: 1546/2000... Step: 1546... Loss: 0.05482... Val Loss: 0.03504\n",
      "Epoch: 1547/2000... Step: 1547... Loss: 0.05481... Val Loss: 0.03503\n",
      "Epoch: 1548/2000... Step: 1548... Loss: 0.05480... Val Loss: 0.03502\n",
      "Epoch: 1549/2000... Step: 1549... Loss: 0.05479... Val Loss: 0.03501\n",
      "Epoch: 1550/2000... Step: 1550... Loss: 0.05478... Val Loss: 0.03500\n",
      "Epoch: 1551/2000... Step: 1551... Loss: 0.05477... Val Loss: 0.03499\n",
      "Epoch: 1552/2000... Step: 1552... Loss: 0.05476... Val Loss: 0.03497\n",
      "Epoch: 1553/2000... Step: 1553... Loss: 0.05475... Val Loss: 0.03496\n",
      "Epoch: 1554/2000... Step: 1554... Loss: 0.05474... Val Loss: 0.03495\n",
      "Epoch: 1555/2000... Step: 1555... Loss: 0.05473... Val Loss: 0.03494\n",
      "Epoch: 1556/2000... Step: 1556... Loss: 0.05472... Val Loss: 0.03493\n",
      "Epoch: 1557/2000... Step: 1557... Loss: 0.05471... Val Loss: 0.03492\n",
      "Epoch: 1558/2000... Step: 1558... Loss: 0.05470... Val Loss: 0.03491\n",
      "Epoch: 1559/2000... Step: 1559... Loss: 0.05469... Val Loss: 0.03490\n",
      "Epoch: 1560/2000... Step: 1560... Loss: 0.05468... Val Loss: 0.03489\n",
      "Epoch: 1561/2000... Step: 1561... Loss: 0.05467... Val Loss: 0.03487\n",
      "Epoch: 1562/2000... Step: 1562... Loss: 0.05466... Val Loss: 0.03486\n",
      "Epoch: 1563/2000... Step: 1563... Loss: 0.05465... Val Loss: 0.03485\n",
      "Epoch: 1564/2000... Step: 1564... Loss: 0.05464... Val Loss: 0.03484\n",
      "Epoch: 1565/2000... Step: 1565... Loss: 0.05463... Val Loss: 0.03483\n",
      "Epoch: 1566/2000... Step: 1566... Loss: 0.05462... Val Loss: 0.03482\n",
      "Epoch: 1567/2000... Step: 1567... Loss: 0.05461... Val Loss: 0.03481\n",
      "Epoch: 1568/2000... Step: 1568... Loss: 0.05460... Val Loss: 0.03480\n",
      "Epoch: 1569/2000... Step: 1569... Loss: 0.05459... Val Loss: 0.03479\n",
      "Epoch: 1570/2000... Step: 1570... Loss: 0.05458... Val Loss: 0.03478\n",
      "Epoch: 1571/2000... Step: 1571... Loss: 0.05457... Val Loss: 0.03477\n",
      "Epoch: 1572/2000... Step: 1572... Loss: 0.05456... Val Loss: 0.03476\n",
      "Epoch: 1573/2000... Step: 1573... Loss: 0.05455... Val Loss: 0.03475\n",
      "Epoch: 1574/2000... Step: 1574... Loss: 0.05454... Val Loss: 0.03474\n",
      "Epoch: 1575/2000... Step: 1575... Loss: 0.05453... Val Loss: 0.03473\n",
      "Epoch: 1576/2000... Step: 1576... Loss: 0.05452... Val Loss: 0.03471\n",
      "Epoch: 1577/2000... Step: 1577... Loss: 0.05451... Val Loss: 0.03470\n",
      "Epoch: 1578/2000... Step: 1578... Loss: 0.05450... Val Loss: 0.03469\n",
      "Epoch: 1579/2000... Step: 1579... Loss: 0.05449... Val Loss: 0.03468\n",
      "Epoch: 1580/2000... Step: 1580... Loss: 0.05449... Val Loss: 0.03467\n",
      "Epoch: 1581/2000... Step: 1581... Loss: 0.05448... Val Loss: 0.03466\n",
      "Epoch: 1582/2000... Step: 1582... Loss: 0.05447... Val Loss: 0.03465\n",
      "Epoch: 1583/2000... Step: 1583... Loss: 0.05446... Val Loss: 0.03464\n",
      "Epoch: 1584/2000... Step: 1584... Loss: 0.05445... Val Loss: 0.03463\n",
      "Epoch: 1585/2000... Step: 1585... Loss: 0.05444... Val Loss: 0.03462\n",
      "Epoch: 1586/2000... Step: 1586... Loss: 0.05443... Val Loss: 0.03461\n",
      "Epoch: 1587/2000... Step: 1587... Loss: 0.05442... Val Loss: 0.03460\n",
      "Epoch: 1588/2000... Step: 1588... Loss: 0.05441... Val Loss: 0.03459\n",
      "Epoch: 1589/2000... Step: 1589... Loss: 0.05440... Val Loss: 0.03458\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1590/2000... Step: 1590... Loss: 0.05439... Val Loss: 0.03457\n",
      "Epoch: 1591/2000... Step: 1591... Loss: 0.05438... Val Loss: 0.03456\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1592/2000... Step: 1592... Loss: 0.05437... Val Loss: 0.03455\n",
      "Epoch: 1593/2000... Step: 1593... Loss: 0.05436... Val Loss: 0.03454\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1594/2000... Step: 1594... Loss: 0.05435... Val Loss: 0.03453\n",
      "Epoch: 1595/2000... Step: 1595... Loss: 0.05434... Val Loss: 0.03452\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1596/2000... Step: 1596... Loss: 0.05433... Val Loss: 0.03451\n",
      "Epoch: 1597/2000... Step: 1597... Loss: 0.05432... Val Loss: 0.03450\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1598/2000... Step: 1598... Loss: 0.05431... Val Loss: 0.03449\n",
      "Epoch: 1599/2000... Step: 1599... Loss: 0.05430... Val Loss: 0.03448\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1600/2000... Step: 1600... Loss: 0.05429... Val Loss: 0.03447\n",
      "Epoch: 1601/2000... Step: 1601... Loss: 0.05429... Val Loss: 0.03446\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1602/2000... Step: 1602... Loss: 0.05428... Val Loss: 0.03445\n",
      "Epoch: 1603/2000... Step: 1603... Loss: 0.05427... Val Loss: 0.03445\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1604/2000... Step: 1604... Loss: 0.05426... Val Loss: 0.03444\n",
      "Epoch: 1605/2000... Step: 1605... Loss: 0.05425... Val Loss: 0.03443\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1606/2000... Step: 1606... Loss: 0.05424... Val Loss: 0.03442\n",
      "Epoch: 1607/2000... Step: 1607... Loss: 0.05423... Val Loss: 0.03441\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1608/2000... Step: 1608... Loss: 0.05422... Val Loss: 0.03440\n",
      "Epoch: 1609/2000... Step: 1609... Loss: 0.05421... Val Loss: 0.03439\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1610/2000... Step: 1610... Loss: 0.05420... Val Loss: 0.03438\n",
      "Epoch: 1611/2000... Step: 1611... Loss: 0.05419... Val Loss: 0.03437\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1612/2000... Step: 1612... Loss: 0.05418... Val Loss: 0.03436\n",
      "Epoch: 1613/2000... Step: 1613... Loss: 0.05417... Val Loss: 0.03435\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1614/2000... Step: 1614... Loss: 0.05416... Val Loss: 0.03434\n",
      "Epoch: 1615/2000... Step: 1615... Loss: 0.05416... Val Loss: 0.03433\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1616/2000... Step: 1616... Loss: 0.05415... Val Loss: 0.03432\n",
      "Epoch: 1617/2000... Step: 1617... Loss: 0.05414... Val Loss: 0.03431\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1618/2000... Step: 1618... Loss: 0.05413... Val Loss: 0.03430\n",
      "Epoch: 1619/2000... Step: 1619... Loss: 0.05412... Val Loss: 0.03430\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1620/2000... Step: 1620... Loss: 0.05411... Val Loss: 0.03429\n",
      "Epoch: 1621/2000... Step: 1621... Loss: 0.05410... Val Loss: 0.03428\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1622/2000... Step: 1622... Loss: 0.05409... Val Loss: 0.03427\n",
      "Epoch: 1623/2000... Step: 1623... Loss: 0.05408... Val Loss: 0.03426\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1624/2000... Step: 1624... Loss: 0.05407... Val Loss: 0.03425\n",
      "Epoch: 1625/2000... Step: 1625... Loss: 0.05406... Val Loss: 0.03424\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1626/2000... Step: 1626... Loss: 0.05406... Val Loss: 0.03423\n",
      "Epoch: 1627/2000... Step: 1627... Loss: 0.05405... Val Loss: 0.03422\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1628/2000... Step: 1628... Loss: 0.05404... Val Loss: 0.03421\n",
      "Epoch: 1629/2000... Step: 1629... Loss: 0.05403... Val Loss: 0.03421\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1630/2000... Step: 1630... Loss: 0.05402... Val Loss: 0.03420\n",
      "Epoch: 1631/2000... Step: 1631... Loss: 0.05401... Val Loss: 0.03419\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1632/2000... Step: 1632... Loss: 0.05400... Val Loss: 0.03418\n",
      "Epoch: 1633/2000... Step: 1633... Loss: 0.05399... Val Loss: 0.03417\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1634/2000... Step: 1634... Loss: 0.05398... Val Loss: 0.03416\n",
      "Epoch: 1635/2000... Step: 1635... Loss: 0.05397... Val Loss: 0.03415\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1636/2000... Step: 1636... Loss: 0.05397... Val Loss: 0.03414\n",
      "Epoch: 1637/2000... Step: 1637... Loss: 0.05396... Val Loss: 0.03414\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1638/2000... Step: 1638... Loss: 0.05395... Val Loss: 0.03413\n",
      "Epoch: 1639/2000... Step: 1639... Loss: 0.05394... Val Loss: 0.03412\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1640/2000... Step: 1640... Loss: 0.05393... Val Loss: 0.03411\n",
      "Epoch: 1641/2000... Step: 1641... Loss: 0.05392... Val Loss: 0.03410\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1642/2000... Step: 1642... Loss: 0.05391... Val Loss: 0.03409\n",
      "Epoch: 1643/2000... Step: 1643... Loss: 0.05390... Val Loss: 0.03408\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1644/2000... Step: 1644... Loss: 0.05390... Val Loss: 0.03408\n",
      "Epoch: 1645/2000... Step: 1645... Loss: 0.05389... Val Loss: 0.03407\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1646/2000... Step: 1646... Loss: 0.05388... Val Loss: 0.03406\n",
      "Epoch: 1647/2000... Step: 1647... Loss: 0.05387... Val Loss: 0.03405\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1648/2000... Step: 1648... Loss: 0.05386... Val Loss: 0.03404\n",
      "Epoch: 1649/2000... Step: 1649... Loss: 0.05385... Val Loss: 0.03403\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1650/2000... Step: 1650... Loss: 0.05384... Val Loss: 0.03403\n",
      "Epoch: 1651/2000... Step: 1651... Loss: 0.05383... Val Loss: 0.03402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1652/2000... Step: 1652... Loss: 0.05383... Val Loss: 0.03401\n",
      "Epoch: 1653/2000... Step: 1653... Loss: 0.05382... Val Loss: 0.03400\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1654/2000... Step: 1654... Loss: 0.05381... Val Loss: 0.03399\n",
      "Epoch: 1655/2000... Step: 1655... Loss: 0.05380... Val Loss: 0.03398\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1656/2000... Step: 1656... Loss: 0.05379... Val Loss: 0.03398\n",
      "Epoch: 1657/2000... Step: 1657... Loss: 0.05378... Val Loss: 0.03397\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1658/2000... Step: 1658... Loss: 0.05377... Val Loss: 0.03396\n",
      "Epoch: 1659/2000... Step: 1659... Loss: 0.05376... Val Loss: 0.03395\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1660/2000... Step: 1660... Loss: 0.05376... Val Loss: 0.03394\n",
      "Epoch: 1661/2000... Step: 1661... Loss: 0.05375... Val Loss: 0.03394\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1662/2000... Step: 1662... Loss: 0.05374... Val Loss: 0.03393\n",
      "Epoch: 1663/2000... Step: 1663... Loss: 0.05373... Val Loss: 0.03392\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1664/2000... Step: 1664... Loss: 0.05372... Val Loss: 0.03391\n",
      "Epoch: 1665/2000... Step: 1665... Loss: 0.05371... Val Loss: 0.03390\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1666/2000... Step: 1666... Loss: 0.05370... Val Loss: 0.03390\n",
      "Epoch: 1667/2000... Step: 1667... Loss: 0.05370... Val Loss: 0.03389\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1668/2000... Step: 1668... Loss: 0.05369... Val Loss: 0.03388\n",
      "Epoch: 1669/2000... Step: 1669... Loss: 0.05368... Val Loss: 0.03387\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1670/2000... Step: 1670... Loss: 0.05367... Val Loss: 0.03386\n",
      "Epoch: 1671/2000... Step: 1671... Loss: 0.05366... Val Loss: 0.03386\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1672/2000... Step: 1672... Loss: 0.05365... Val Loss: 0.03385\n",
      "Epoch: 1673/2000... Step: 1673... Loss: 0.05364... Val Loss: 0.03384\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1674/2000... Step: 1674... Loss: 0.05364... Val Loss: 0.03383\n",
      "Epoch: 1675/2000... Step: 1675... Loss: 0.05363... Val Loss: 0.03383\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1676/2000... Step: 1676... Loss: 0.05362... Val Loss: 0.03382\n",
      "Epoch: 1677/2000... Step: 1677... Loss: 0.05361... Val Loss: 0.03381\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1678/2000... Step: 1678... Loss: 0.05360... Val Loss: 0.03380\n",
      "Epoch: 1679/2000... Step: 1679... Loss: 0.05359... Val Loss: 0.03379\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1680/2000... Step: 1680... Loss: 0.05359... Val Loss: 0.03379\n",
      "Epoch: 1681/2000... Step: 1681... Loss: 0.05358... Val Loss: 0.03378\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1682/2000... Step: 1682... Loss: 0.05357... Val Loss: 0.03377\n",
      "Epoch: 1683/2000... Step: 1683... Loss: 0.05356... Val Loss: 0.03376\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1684/2000... Step: 1684... Loss: 0.05355... Val Loss: 0.03376\n",
      "Epoch: 1685/2000... Step: 1685... Loss: 0.05354... Val Loss: 0.03375\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1686/2000... Step: 1686... Loss: 0.05354... Val Loss: 0.03374\n",
      "Epoch: 1687/2000... Step: 1687... Loss: 0.05353... Val Loss: 0.03373\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1688/2000... Step: 1688... Loss: 0.05352... Val Loss: 0.03373\n",
      "Epoch: 1689/2000... Step: 1689... Loss: 0.05351... Val Loss: 0.03372\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1690/2000... Step: 1690... Loss: 0.05350... Val Loss: 0.03371\n",
      "Epoch: 1691/2000... Step: 1691... Loss: 0.05349... Val Loss: 0.03371\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1692/2000... Step: 1692... Loss: 0.05349... Val Loss: 0.03370\n",
      "Epoch: 1693/2000... Step: 1693... Loss: 0.05348... Val Loss: 0.03369\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1694/2000... Step: 1694... Loss: 0.05347... Val Loss: 0.03368\n",
      "Epoch: 1695/2000... Step: 1695... Loss: 0.05346... Val Loss: 0.03368\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1696/2000... Step: 1696... Loss: 0.05345... Val Loss: 0.03367\n",
      "Epoch: 1697/2000... Step: 1697... Loss: 0.05344... Val Loss: 0.03366\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1698/2000... Step: 1698... Loss: 0.05344... Val Loss: 0.03365\n",
      "Epoch: 1699/2000... Step: 1699... Loss: 0.05343... Val Loss: 0.03365\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1700/2000... Step: 1700... Loss: 0.05342... Val Loss: 0.03364\n",
      "Epoch: 1701/2000... Step: 1701... Loss: 0.05341... Val Loss: 0.03363\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1702/2000... Step: 1702... Loss: 0.05340... Val Loss: 0.03363\n",
      "Epoch: 1703/2000... Step: 1703... Loss: 0.05340... Val Loss: 0.03362\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1704/2000... Step: 1704... Loss: 0.05339... Val Loss: 0.03361\n",
      "Epoch: 1705/2000... Step: 1705... Loss: 0.05338... Val Loss: 0.03360\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1706/2000... Step: 1706... Loss: 0.05337... Val Loss: 0.03360\n",
      "Epoch: 1707/2000... Step: 1707... Loss: 0.05336... Val Loss: 0.03359\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1708/2000... Step: 1708... Loss: 0.05335... Val Loss: 0.03358\n",
      "Epoch: 1709/2000... Step: 1709... Loss: 0.05335... Val Loss: 0.03358\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1710/2000... Step: 1710... Loss: 0.05334... Val Loss: 0.03357\n",
      "Epoch: 1711/2000... Step: 1711... Loss: 0.05333... Val Loss: 0.03356\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1712/2000... Step: 1712... Loss: 0.05332... Val Loss: 0.03356\n",
      "Epoch: 1713/2000... Step: 1713... Loss: 0.05331... Val Loss: 0.03355\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1714/2000... Step: 1714... Loss: 0.05331... Val Loss: 0.03354\n",
      "Epoch: 1715/2000... Step: 1715... Loss: 0.05330... Val Loss: 0.03354\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1716/2000... Step: 1716... Loss: 0.05329... Val Loss: 0.03353\n",
      "Epoch: 1717/2000... Step: 1717... Loss: 0.05328... Val Loss: 0.03352\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1718/2000... Step: 1718... Loss: 0.05327... Val Loss: 0.03352\n",
      "Epoch: 1719/2000... Step: 1719... Loss: 0.05327... Val Loss: 0.03351\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1720/2000... Step: 1720... Loss: 0.05326... Val Loss: 0.03350\n",
      "Epoch: 1721/2000... Step: 1721... Loss: 0.05325... Val Loss: 0.03350\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1722/2000... Step: 1722... Loss: 0.05324... Val Loss: 0.03349\n",
      "Epoch: 1723/2000... Step: 1723... Loss: 0.05323... Val Loss: 0.03348\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1724/2000... Step: 1724... Loss: 0.05323... Val Loss: 0.03348\n",
      "Epoch: 1725/2000... Step: 1725... Loss: 0.05322... Val Loss: 0.03347\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1726/2000... Step: 1726... Loss: 0.05321... Val Loss: 0.03346\n",
      "Epoch: 1727/2000... Step: 1727... Loss: 0.05320... Val Loss: 0.03346\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1728/2000... Step: 1728... Loss: 0.05319... Val Loss: 0.03345\n",
      "Epoch: 1729/2000... Step: 1729... Loss: 0.05319... Val Loss: 0.03344\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1730/2000... Step: 1730... Loss: 0.05318... Val Loss: 0.03344\n",
      "Epoch: 1731/2000... Step: 1731... Loss: 0.05317... Val Loss: 0.03343\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1732/2000... Step: 1732... Loss: 0.05316... Val Loss: 0.03342\n",
      "Epoch: 1733/2000... Step: 1733... Loss: 0.05315... Val Loss: 0.03342\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1734/2000... Step: 1734... Loss: 0.05315... Val Loss: 0.03341\n",
      "Epoch: 1735/2000... Step: 1735... Loss: 0.05314... Val Loss: 0.03340\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1736/2000... Step: 1736... Loss: 0.05313... Val Loss: 0.03340\n",
      "Epoch: 1737/2000... Step: 1737... Loss: 0.05312... Val Loss: 0.03339\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1738/2000... Step: 1738... Loss: 0.05311... Val Loss: 0.03338\n",
      "Epoch: 1739/2000... Step: 1739... Loss: 0.05311... Val Loss: 0.03338\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1740/2000... Step: 1740... Loss: 0.05310... Val Loss: 0.03337\n",
      "Epoch: 1741/2000... Step: 1741... Loss: 0.05309... Val Loss: 0.03337\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1742/2000... Step: 1742... Loss: 0.05308... Val Loss: 0.03336\n",
      "Epoch: 1743/2000... Step: 1743... Loss: 0.05308... Val Loss: 0.03335\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1744/2000... Step: 1744... Loss: 0.05307... Val Loss: 0.03335\n",
      "Epoch: 1745/2000... Step: 1745... Loss: 0.05306... Val Loss: 0.03334\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1746/2000... Step: 1746... Loss: 0.05305... Val Loss: 0.03333\n",
      "Epoch: 1747/2000... Step: 1747... Loss: 0.05304... Val Loss: 0.03333\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1748/2000... Step: 1748... Loss: 0.05304... Val Loss: 0.03332\n",
      "Epoch: 1749/2000... Step: 1749... Loss: 0.05303... Val Loss: 0.03332\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1750/2000... Step: 1750... Loss: 0.05302... Val Loss: 0.03331\n",
      "Epoch: 1751/2000... Step: 1751... Loss: 0.05301... Val Loss: 0.03330\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1752/2000... Step: 1752... Loss: 0.05301... Val Loss: 0.03330\n",
      "Epoch: 1753/2000... Step: 1753... Loss: 0.05300... Val Loss: 0.03329\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1754/2000... Step: 1754... Loss: 0.05299... Val Loss: 0.03329\n",
      "Epoch: 1755/2000... Step: 1755... Loss: 0.05298... Val Loss: 0.03328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1756/2000... Step: 1756... Loss: 0.05297... Val Loss: 0.03327\n",
      "Epoch: 1757/2000... Step: 1757... Loss: 0.05297... Val Loss: 0.03327\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1758/2000... Step: 1758... Loss: 0.05296... Val Loss: 0.03326\n",
      "Epoch: 1759/2000... Step: 1759... Loss: 0.05295... Val Loss: 0.03326\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1760/2000... Step: 1760... Loss: 0.05294... Val Loss: 0.03325\n",
      "Epoch: 1761/2000... Step: 1761... Loss: 0.05294... Val Loss: 0.03324\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1762/2000... Step: 1762... Loss: 0.05293... Val Loss: 0.03324\n",
      "Epoch: 1763/2000... Step: 1763... Loss: 0.05292... Val Loss: 0.03323\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1764/2000... Step: 1764... Loss: 0.05291... Val Loss: 0.03323\n",
      "Epoch: 1765/2000... Step: 1765... Loss: 0.05290... Val Loss: 0.03322\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1766/2000... Step: 1766... Loss: 0.05290... Val Loss: 0.03322\n",
      "Epoch: 1767/2000... Step: 1767... Loss: 0.05289... Val Loss: 0.03321\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1768/2000... Step: 1768... Loss: 0.05288... Val Loss: 0.03320\n",
      "Epoch: 1769/2000... Step: 1769... Loss: 0.05287... Val Loss: 0.03320\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1770/2000... Step: 1770... Loss: 0.05287... Val Loss: 0.03319\n",
      "Epoch: 1771/2000... Step: 1771... Loss: 0.05286... Val Loss: 0.03319\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1772/2000... Step: 1772... Loss: 0.05285... Val Loss: 0.03318\n",
      "Epoch: 1773/2000... Step: 1773... Loss: 0.05284... Val Loss: 0.03318\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1774/2000... Step: 1774... Loss: 0.05284... Val Loss: 0.03317\n",
      "Epoch: 1775/2000... Step: 1775... Loss: 0.05283... Val Loss: 0.03316\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1776/2000... Step: 1776... Loss: 0.05282... Val Loss: 0.03316\n",
      "Epoch: 1777/2000... Step: 1777... Loss: 0.05281... Val Loss: 0.03315\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1778/2000... Step: 1778... Loss: 0.05281... Val Loss: 0.03315\n",
      "Epoch: 1779/2000... Step: 1779... Loss: 0.05280... Val Loss: 0.03314\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1780/2000... Step: 1780... Loss: 0.05279... Val Loss: 0.03314\n",
      "Epoch: 1781/2000... Step: 1781... Loss: 0.05278... Val Loss: 0.03313\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1782/2000... Step: 1782... Loss: 0.05277... Val Loss: 0.03313\n",
      "Epoch: 1783/2000... Step: 1783... Loss: 0.05277... Val Loss: 0.03312\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1784/2000... Step: 1784... Loss: 0.05276... Val Loss: 0.03311\n",
      "Epoch: 1785/2000... Step: 1785... Loss: 0.05275... Val Loss: 0.03311\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1786/2000... Step: 1786... Loss: 0.05274... Val Loss: 0.03310\n",
      "Epoch: 1787/2000... Step: 1787... Loss: 0.05274... Val Loss: 0.03310\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1788/2000... Step: 1788... Loss: 0.05273... Val Loss: 0.03309\n",
      "Epoch: 1789/2000... Step: 1789... Loss: 0.05272... Val Loss: 0.03309\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1790/2000... Step: 1790... Loss: 0.05271... Val Loss: 0.03308\n",
      "Epoch: 1791/2000... Step: 1791... Loss: 0.05271... Val Loss: 0.03308\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1792/2000... Step: 1792... Loss: 0.05270... Val Loss: 0.03307\n",
      "Epoch: 1793/2000... Step: 1793... Loss: 0.05269... Val Loss: 0.03307\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1794/2000... Step: 1794... Loss: 0.05268... Val Loss: 0.03306\n",
      "Epoch: 1795/2000... Step: 1795... Loss: 0.05268... Val Loss: 0.03306\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1796/2000... Step: 1796... Loss: 0.05267... Val Loss: 0.03305\n",
      "Epoch: 1797/2000... Step: 1797... Loss: 0.05266... Val Loss: 0.03304\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1798/2000... Step: 1798... Loss: 0.05265... Val Loss: 0.03304\n",
      "Epoch: 1799/2000... Step: 1799... Loss: 0.05265... Val Loss: 0.03303\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1800/2000... Step: 1800... Loss: 0.05264... Val Loss: 0.03303\n",
      "Epoch: 1801/2000... Step: 1801... Loss: 0.05263... Val Loss: 0.03302\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1802/2000... Step: 1802... Loss: 0.05262... Val Loss: 0.03302\n",
      "Epoch: 1803/2000... Step: 1803... Loss: 0.05262... Val Loss: 0.03301\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1804/2000... Step: 1804... Loss: 0.05261... Val Loss: 0.03301\n",
      "Epoch: 1805/2000... Step: 1805... Loss: 0.05260... Val Loss: 0.03300\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1806/2000... Step: 1806... Loss: 0.05259... Val Loss: 0.03300\n",
      "Epoch: 1807/2000... Step: 1807... Loss: 0.05259... Val Loss: 0.03299\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1808/2000... Step: 1808... Loss: 0.05258... Val Loss: 0.03299\n",
      "Epoch: 1809/2000... Step: 1809... Loss: 0.05257... Val Loss: 0.03298\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1810/2000... Step: 1810... Loss: 0.05256... Val Loss: 0.03298\n",
      "Epoch: 1811/2000... Step: 1811... Loss: 0.05256... Val Loss: 0.03297\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1812/2000... Step: 1812... Loss: 0.05255... Val Loss: 0.03297\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1813/2000... Step: 1813... Loss: 0.05254... Val Loss: 0.03296\n",
      "Epoch: 1814/2000... Step: 1814... Loss: 0.05253... Val Loss: 0.03296\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1815/2000... Step: 1815... Loss: 0.05253... Val Loss: 0.03295\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1816/2000... Step: 1816... Loss: 0.05252... Val Loss: 0.03295\n",
      "Epoch: 1817/2000... Step: 1817... Loss: 0.05251... Val Loss: 0.03294\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1818/2000... Step: 1818... Loss: 0.05250... Val Loss: 0.03294\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1819/2000... Step: 1819... Loss: 0.05250... Val Loss: 0.03293\n",
      "Epoch: 1820/2000... Step: 1820... Loss: 0.05249... Val Loss: 0.03293\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1821/2000... Step: 1821... Loss: 0.05248... Val Loss: 0.03292\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1822/2000... Step: 1822... Loss: 0.05247... Val Loss: 0.03292\n",
      "Epoch: 1823/2000... Step: 1823... Loss: 0.05247... Val Loss: 0.03291\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1824/2000... Step: 1824... Loss: 0.05246... Val Loss: 0.03291\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1825/2000... Step: 1825... Loss: 0.05245... Val Loss: 0.03291\n",
      "Epoch: 1826/2000... Step: 1826... Loss: 0.05245... Val Loss: 0.03290\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1827/2000... Step: 1827... Loss: 0.05244... Val Loss: 0.03290\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1828/2000... Step: 1828... Loss: 0.05243... Val Loss: 0.03289\n",
      "Epoch: 1829/2000... Step: 1829... Loss: 0.05242... Val Loss: 0.03289\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1830/2000... Step: 1830... Loss: 0.05242... Val Loss: 0.03288\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1831/2000... Step: 1831... Loss: 0.05241... Val Loss: 0.03288\n",
      "Epoch: 1832/2000... Step: 1832... Loss: 0.05240... Val Loss: 0.03287\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1833/2000... Step: 1833... Loss: 0.05239... Val Loss: 0.03287\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1834/2000... Step: 1834... Loss: 0.05239... Val Loss: 0.03286\n",
      "Epoch: 1835/2000... Step: 1835... Loss: 0.05238... Val Loss: 0.03286\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1836/2000... Step: 1836... Loss: 0.05237... Val Loss: 0.03285\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1837/2000... Step: 1837... Loss: 0.05236... Val Loss: 0.03285\n",
      "Epoch: 1838/2000... Step: 1838... Loss: 0.05236... Val Loss: 0.03285\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1839/2000... Step: 1839... Loss: 0.05235... Val Loss: 0.03284\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1840/2000... Step: 1840... Loss: 0.05234... Val Loss: 0.03284\n",
      "Epoch: 1841/2000... Step: 1841... Loss: 0.05233... Val Loss: 0.03283\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1842/2000... Step: 1842... Loss: 0.05233... Val Loss: 0.03283\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1843/2000... Step: 1843... Loss: 0.05232... Val Loss: 0.03282\n",
      "Epoch: 1844/2000... Step: 1844... Loss: 0.05231... Val Loss: 0.03282\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1845/2000... Step: 1845... Loss: 0.05231... Val Loss: 0.03281\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1846/2000... Step: 1846... Loss: 0.05230... Val Loss: 0.03281\n",
      "Epoch: 1847/2000... Step: 1847... Loss: 0.05229... Val Loss: 0.03281\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1848/2000... Step: 1848... Loss: 0.05228... Val Loss: 0.03280\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1849/2000... Step: 1849... Loss: 0.05228... Val Loss: 0.03280\n",
      "Epoch: 1850/2000... Step: 1850... Loss: 0.05227... Val Loss: 0.03279\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1851/2000... Step: 1851... Loss: 0.05226... Val Loss: 0.03279\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1852/2000... Step: 1852... Loss: 0.05225... Val Loss: 0.03278\n",
      "Epoch: 1853/2000... Step: 1853... Loss: 0.05225... Val Loss: 0.03278\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1854/2000... Step: 1854... Loss: 0.05224... Val Loss: 0.03277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1855/2000... Step: 1855... Loss: 0.05223... Val Loss: 0.03277\n",
      "Epoch: 1856/2000... Step: 1856... Loss: 0.05222... Val Loss: 0.03277\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1857/2000... Step: 1857... Loss: 0.05222... Val Loss: 0.03276\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1858/2000... Step: 1858... Loss: 0.05221... Val Loss: 0.03276\n",
      "Epoch: 1859/2000... Step: 1859... Loss: 0.05220... Val Loss: 0.03275\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1860/2000... Step: 1860... Loss: 0.05220... Val Loss: 0.03275\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1861/2000... Step: 1861... Loss: 0.05219... Val Loss: 0.03275\n",
      "Epoch: 1862/2000... Step: 1862... Loss: 0.05218... Val Loss: 0.03274\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1863/2000... Step: 1863... Loss: 0.05217... Val Loss: 0.03274\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1864/2000... Step: 1864... Loss: 0.05217... Val Loss: 0.03273\n",
      "Epoch: 1865/2000... Step: 1865... Loss: 0.05216... Val Loss: 0.03273\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1866/2000... Step: 1866... Loss: 0.05215... Val Loss: 0.03272\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1867/2000... Step: 1867... Loss: 0.05214... Val Loss: 0.03272\n",
      "Epoch: 1868/2000... Step: 1868... Loss: 0.05214... Val Loss: 0.03272\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1869/2000... Step: 1869... Loss: 0.05213... Val Loss: 0.03271\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1870/2000... Step: 1870... Loss: 0.05212... Val Loss: 0.03271\n",
      "Epoch: 1871/2000... Step: 1871... Loss: 0.05212... Val Loss: 0.03270\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1872/2000... Step: 1872... Loss: 0.05211... Val Loss: 0.03270\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1873/2000... Step: 1873... Loss: 0.05210... Val Loss: 0.03270\n",
      "Epoch: 1874/2000... Step: 1874... Loss: 0.05209... Val Loss: 0.03269\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1875/2000... Step: 1875... Loss: 0.05209... Val Loss: 0.03269\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1876/2000... Step: 1876... Loss: 0.05208... Val Loss: 0.03268\n",
      "Epoch: 1877/2000... Step: 1877... Loss: 0.05207... Val Loss: 0.03268\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1878/2000... Step: 1878... Loss: 0.05206... Val Loss: 0.03268\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1879/2000... Step: 1879... Loss: 0.05206... Val Loss: 0.03267\n",
      "Epoch: 1880/2000... Step: 1880... Loss: 0.05205... Val Loss: 0.03267\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1881/2000... Step: 1881... Loss: 0.05204... Val Loss: 0.03266\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1882/2000... Step: 1882... Loss: 0.05204... Val Loss: 0.03266\n",
      "Epoch: 1883/2000... Step: 1883... Loss: 0.05203... Val Loss: 0.03266\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1884/2000... Step: 1884... Loss: 0.05202... Val Loss: 0.03265\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1885/2000... Step: 1885... Loss: 0.05201... Val Loss: 0.03265\n",
      "Epoch: 1886/2000... Step: 1886... Loss: 0.05201... Val Loss: 0.03265\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1887/2000... Step: 1887... Loss: 0.05200... Val Loss: 0.03264\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1888/2000... Step: 1888... Loss: 0.05199... Val Loss: 0.03264\n",
      "Epoch: 1889/2000... Step: 1889... Loss: 0.05198... Val Loss: 0.03263\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1890/2000... Step: 1890... Loss: 0.05198... Val Loss: 0.03263\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1891/2000... Step: 1891... Loss: 0.05197... Val Loss: 0.03263\n",
      "Epoch: 1892/2000... Step: 1892... Loss: 0.05196... Val Loss: 0.03262\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1893/2000... Step: 1893... Loss: 0.05196... Val Loss: 0.03262\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1894/2000... Step: 1894... Loss: 0.05195... Val Loss: 0.03262\n",
      "Epoch: 1895/2000... Step: 1895... Loss: 0.05194... Val Loss: 0.03261\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1896/2000... Step: 1896... Loss: 0.05193... Val Loss: 0.03261\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1897/2000... Step: 1897... Loss: 0.05193... Val Loss: 0.03261\n",
      "Epoch: 1898/2000... Step: 1898... Loss: 0.05192... Val Loss: 0.03260\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1899/2000... Step: 1899... Loss: 0.05191... Val Loss: 0.03260\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1900/2000... Step: 1900... Loss: 0.05190... Val Loss: 0.03259\n",
      "Epoch: 1901/2000... Step: 1901... Loss: 0.05190... Val Loss: 0.03259\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1902/2000... Step: 1902... Loss: 0.05189... Val Loss: 0.03259\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1903/2000... Step: 1903... Loss: 0.05188... Val Loss: 0.03258\n",
      "Epoch: 1904/2000... Step: 1904... Loss: 0.05188... Val Loss: 0.03258\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1905/2000... Step: 1905... Loss: 0.05187... Val Loss: 0.03258\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1906/2000... Step: 1906... Loss: 0.05186... Val Loss: 0.03257\n",
      "Epoch: 1907/2000... Step: 1907... Loss: 0.05185... Val Loss: 0.03257\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1908/2000... Step: 1908... Loss: 0.05185... Val Loss: 0.03257\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1909/2000... Step: 1909... Loss: 0.05184... Val Loss: 0.03256\n",
      "Epoch: 1910/2000... Step: 1910... Loss: 0.05183... Val Loss: 0.03256\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1911/2000... Step: 1911... Loss: 0.05182... Val Loss: 0.03256\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1912/2000... Step: 1912... Loss: 0.05182... Val Loss: 0.03255\n",
      "Epoch: 1913/2000... Step: 1913... Loss: 0.05181... Val Loss: 0.03255\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1914/2000... Step: 1914... Loss: 0.05180... Val Loss: 0.03255\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1915/2000... Step: 1915... Loss: 0.05180... Val Loss: 0.03254\n",
      "Epoch: 1916/2000... Step: 1916... Loss: 0.05179... Val Loss: 0.03254\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1917/2000... Step: 1917... Loss: 0.05178... Val Loss: 0.03254\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1918/2000... Step: 1918... Loss: 0.05177... Val Loss: 0.03253\n",
      "Epoch: 1919/2000... Step: 1919... Loss: 0.05177... Val Loss: 0.03253\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1920/2000... Step: 1920... Loss: 0.05176... Val Loss: 0.03253\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1921/2000... Step: 1921... Loss: 0.05175... Val Loss: 0.03252\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1922/2000... Step: 1922... Loss: 0.05174... Val Loss: 0.03252\n",
      "Epoch: 1923/2000... Step: 1923... Loss: 0.05174... Val Loss: 0.03252\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1924/2000... Step: 1924... Loss: 0.05173... Val Loss: 0.03251\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1925/2000... Step: 1925... Loss: 0.05172... Val Loss: 0.03251\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1926/2000... Step: 1926... Loss: 0.05171... Val Loss: 0.03251\n",
      "Epoch: 1927/2000... Step: 1927... Loss: 0.05171... Val Loss: 0.03250\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1928/2000... Step: 1928... Loss: 0.05170... Val Loss: 0.03250\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1929/2000... Step: 1929... Loss: 0.05169... Val Loss: 0.03250\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1930/2000... Step: 1930... Loss: 0.05169... Val Loss: 0.03249\n",
      "Epoch: 1931/2000... Step: 1931... Loss: 0.05168... Val Loss: 0.03249\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1932/2000... Step: 1932... Loss: 0.05167... Val Loss: 0.03249\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1933/2000... Step: 1933... Loss: 0.05166... Val Loss: 0.03248\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1934/2000... Step: 1934... Loss: 0.05166... Val Loss: 0.03248\n",
      "Epoch: 1935/2000... Step: 1935... Loss: 0.05165... Val Loss: 0.03248\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1936/2000... Step: 1936... Loss: 0.05164... Val Loss: 0.03247\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1937/2000... Step: 1937... Loss: 0.05163... Val Loss: 0.03247\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1938/2000... Step: 1938... Loss: 0.05163... Val Loss: 0.03247\n",
      "Epoch: 1939/2000... Step: 1939... Loss: 0.05162... Val Loss: 0.03247\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1940/2000... Step: 1940... Loss: 0.05161... Val Loss: 0.03246\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1941/2000... Step: 1941... Loss: 0.05161... Val Loss: 0.03246\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1942/2000... Step: 1942... Loss: 0.05160... Val Loss: 0.03246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1943/2000... Step: 1943... Loss: 0.05159... Val Loss: 0.03245\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1944/2000... Step: 1944... Loss: 0.05158... Val Loss: 0.03245\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1945/2000... Step: 1945... Loss: 0.05158... Val Loss: 0.03245\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1946/2000... Step: 1946... Loss: 0.05157... Val Loss: 0.03244\n",
      "Epoch: 1947/2000... Step: 1947... Loss: 0.05156... Val Loss: 0.03244\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1948/2000... Step: 1948... Loss: 0.05155... Val Loss: 0.03244\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1949/2000... Step: 1949... Loss: 0.05155... Val Loss: 0.03244\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1950/2000... Step: 1950... Loss: 0.05154... Val Loss: 0.03243\n",
      "Epoch: 1951/2000... Step: 1951... Loss: 0.05153... Val Loss: 0.03243\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1952/2000... Step: 1952... Loss: 0.05152... Val Loss: 0.03243\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1953/2000... Step: 1953... Loss: 0.05152... Val Loss: 0.03242\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1954/2000... Step: 1954... Loss: 0.05151... Val Loss: 0.03242\n",
      "Epoch: 1955/2000... Step: 1955... Loss: 0.05150... Val Loss: 0.03242\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1956/2000... Step: 1956... Loss: 0.05150... Val Loss: 0.03242\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1957/2000... Step: 1957... Loss: 0.05149... Val Loss: 0.03241\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1958/2000... Step: 1958... Loss: 0.05148... Val Loss: 0.03241\n",
      "Epoch: 1959/2000... Step: 1959... Loss: 0.05147... Val Loss: 0.03241\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1960/2000... Step: 1960... Loss: 0.05147... Val Loss: 0.03241\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1961/2000... Step: 1961... Loss: 0.05146... Val Loss: 0.03240\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1962/2000... Step: 1962... Loss: 0.05145... Val Loss: 0.03240\n",
      "Epoch: 1963/2000... Step: 1963... Loss: 0.05144... Val Loss: 0.03240\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1964/2000... Step: 1964... Loss: 0.05144... Val Loss: 0.03239\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1965/2000... Step: 1965... Loss: 0.05143... Val Loss: 0.03239\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1966/2000... Step: 1966... Loss: 0.05142... Val Loss: 0.03239\n",
      "Epoch: 1967/2000... Step: 1967... Loss: 0.05141... Val Loss: 0.03239\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1968/2000... Step: 1968... Loss: 0.05141... Val Loss: 0.03238\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1969/2000... Step: 1969... Loss: 0.05140... Val Loss: 0.03238\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1970/2000... Step: 1970... Loss: 0.05139... Val Loss: 0.03238\n",
      "Epoch: 1971/2000... Step: 1971... Loss: 0.05139... Val Loss: 0.03238\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1972/2000... Step: 1972... Loss: 0.05138... Val Loss: 0.03237\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1973/2000... Step: 1973... Loss: 0.05137... Val Loss: 0.03237\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1974/2000... Step: 1974... Loss: 0.05136... Val Loss: 0.03237\n",
      "Epoch: 1975/2000... Step: 1975... Loss: 0.05136... Val Loss: 0.03237\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1976/2000... Step: 1976... Loss: 0.05135... Val Loss: 0.03236\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1977/2000... Step: 1977... Loss: 0.05134... Val Loss: 0.03236\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1978/2000... Step: 1978... Loss: 0.05133... Val Loss: 0.03236\n",
      "Epoch: 1979/2000... Step: 1979... Loss: 0.05133... Val Loss: 0.03236\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1980/2000... Step: 1980... Loss: 0.05132... Val Loss: 0.03235\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1981/2000... Step: 1981... Loss: 0.05131... Val Loss: 0.03235\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1982/2000... Step: 1982... Loss: 0.05130... Val Loss: 0.03235\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 1983/2000... Step: 1983... Loss: 0.05130... Val Loss: 0.03235\n",
      "Epoch: 1984/2000... Step: 1984... Loss: 0.05129... Val Loss: 0.03234\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1985/2000... Step: 1985... Loss: 0.05128... Val Loss: 0.03234\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1986/2000... Step: 1986... Loss: 0.05127... Val Loss: 0.03234\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1987/2000... Step: 1987... Loss: 0.05127... Val Loss: 0.03234\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 1988/2000... Step: 1988... Loss: 0.05126... Val Loss: 0.03233\n",
      "Epoch: 1989/2000... Step: 1989... Loss: 0.05125... Val Loss: 0.03233\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1990/2000... Step: 1990... Loss: 0.05124... Val Loss: 0.03233\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1991/2000... Step: 1991... Loss: 0.05124... Val Loss: 0.03233\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1992/2000... Step: 1992... Loss: 0.05123... Val Loss: 0.03232\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 1993/2000... Step: 1993... Loss: 0.05122... Val Loss: 0.03232\n",
      "Epoch: 1994/2000... Step: 1994... Loss: 0.05121... Val Loss: 0.03232\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 1995/2000... Step: 1995... Loss: 0.05121... Val Loss: 0.03232\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 1996/2000... Step: 1996... Loss: 0.05120... Val Loss: 0.03231\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 1997/2000... Step: 1997... Loss: 0.05119... Val Loss: 0.03231\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 1998/2000... Step: 1998... Loss: 0.05119... Val Loss: 0.03231\n",
      "Epoch: 1999/2000... Step: 1999... Loss: 0.05118... Val Loss: 0.03231\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 2000/2000... Step: 2000... Loss: 0.05117... Val Loss: 0.03231\n"
     ]
    }
   ],
   "source": [
    "train_losess, val_losses, roc_auc_train, roc_auc_val = train(net, training_generator, validation_generator, verbose=True,\n",
    "                                                             opt_func=opt, criterion_func=criterion, epochs=num_epochs,\n",
    "                                                             lr=0.01, check_early_stopping=True, check_auc_roc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after training on the Test set: 0.1283\n",
      "Accuracy After training on the Test set:\n",
      "0.96\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss after training on the Test set: {}\".format(infer(net, test_generator)))\n",
    "print(\"Accuracy After training on the Test set:\")\n",
    "y_pred_p = test_accuracy(net, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**: <br>\n",
    "We can see that the loss on the test set in this model is lower from the loss in the fisrt model (0.69 vs 0.07) <br>\n",
    "In addition we see that the accuracy is better in the current model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the plots:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss vs epochs part b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and the validation in this model are lower and going down very fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_vs_epochs(train_losess, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot auc vs epochs part b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model the Auc was very high faster than the provious model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_auc_vs_epochs(roc_auc_train, roc_auc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot roc cruve part b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The auc aimed at 1 much faster than the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test auc is: 0.99\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_cruve(y_test=y_test, y_pred_p=y_pred_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot decision boundary part b. <br>\n",
    "The diffrence between the models is that the first model tried to classify the data in their middle. <br>\n",
    "The second model try to classify all te blue point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 0.96\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_decision_boundary(x, x_test, y_test, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the excercise you will need to implement a regression model using neural networks. The model should predict the output of a trigonometric function of two variables. Your data set is based on a meshgrid. Your task is to create a list of points that would correspond to a grid and use it for the input of your neural network. Then, build your neural networks and find the architecture which gives you the best results.\n",
    "1. Plot the surface from the overall data and compare it to your predicted test sets.\n",
    "2. Which loss function and validation metric did you choose?\n",
    "3. Plot the loss and validation metrics vs epoch for the training and test sets.\n",
    "4. Build a new neural network and try overfitting your training set. Show that you managed to overfit. Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import all the packages that you will need during this part of assignment.\n",
    "\n",
    "Feel free to use another libraries if you want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_num)\n",
    "x = np.linspace(-5, 5, 30)\n",
    "y = np.linspace(-5, 5, 30)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "z = np.sin(xx) * np.cos(yy) + 0.1 * np.random.rand(xx.shape[0], xx.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = xx.flatten()\n",
    "yy = yy.flatten()\n",
    "targets = z.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate((xx.reshape(-1,1), yy.reshape(-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(inputs, targets, test_size=0.15, random_state=random_num)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, random_state=random_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generators for the data\n",
    "training_generator, validation_generator, test_generator = create_generators(x_train, y_train, x_val, y_val, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Model:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We choose the MSE Metric for the loss function*** as at a regression tasks we need to fit the model with the lowest difference between the continuous value y we predict and the true y. The MSE is substracting the y_pred from the y_true and make it squared - to emphasize big errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MlpNetwork(num_net_channels_part_c_1, nn.ReLU(), classifier=False)\n",
    "opt = torch.optim.Adam\n",
    "criterion = nn.MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before train on the Test set:\n",
      "0.2949\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss before train on the Test set:\")\n",
    "print(infer(net, test_generator, nn.MSELoss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2000... Step: 1... Loss: 0.26484... Val Loss: 0.46755\n",
      "Epoch: 2/2000... Step: 2... Loss: 0.51327... Val Loss: 0.26363\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 3/2000... Step: 3... Loss: 0.26608... Val Loss: 0.30823\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 4/2000... Step: 4... Loss: 0.30715... Val Loss: 0.27449\n",
      "Epoch: 5/2000... Step: 5... Loss: 0.27488... Val Loss: 0.25244\n",
      "Epoch: 6/2000... Step: 6... Loss: 0.25439... Val Loss: 0.24523\n",
      "Epoch: 7/2000... Step: 7... Loss: 0.24969... Val Loss: 0.24322\n",
      "Epoch: 8/2000... Step: 8... Loss: 0.25064... Val Loss: 0.24129\n",
      "Epoch: 9/2000... Step: 9... Loss: 0.24935... Val Loss: 0.23754\n",
      "Epoch: 10/2000... Step: 10... Loss: 0.24590... Val Loss: 0.23385\n",
      "Epoch: 11/2000... Step: 11... Loss: 0.24241... Val Loss: 0.23120\n",
      "Epoch: 12/2000... Step: 12... Loss: 0.23970... Val Loss: 0.22914\n",
      "Epoch: 13/2000... Step: 13... Loss: 0.23774... Val Loss: 0.22670\n",
      "Epoch: 14/2000... Step: 14... Loss: 0.23595... Val Loss: 0.22384\n",
      "Epoch: 15/2000... Step: 15... Loss: 0.23400... Val Loss: 0.22074\n",
      "Epoch: 16/2000... Step: 16... Loss: 0.23189... Val Loss: 0.21838\n",
      "Epoch: 17/2000... Step: 17... Loss: 0.22969... Val Loss: 0.21639\n",
      "Epoch: 18/2000... Step: 18... Loss: 0.22728... Val Loss: 0.21440\n",
      "Epoch: 19/2000... Step: 19... Loss: 0.22445... Val Loss: 0.21248\n",
      "Epoch: 20/2000... Step: 20... Loss: 0.22150... Val Loss: 0.20899\n",
      "Epoch: 21/2000... Step: 21... Loss: 0.21777... Val Loss: 0.20517\n",
      "Epoch: 22/2000... Step: 22... Loss: 0.21400... Val Loss: 0.20258\n",
      "Epoch: 23/2000... Step: 23... Loss: 0.21046... Val Loss: 0.20127\n",
      "Epoch: 24/2000... Step: 24... Loss: 0.20685... Val Loss: 0.20115\n",
      "Epoch: 25/2000... Step: 25... Loss: 0.20353... Val Loss: 0.20091\n",
      "Epoch: 26/2000... Step: 26... Loss: 0.20030... Val Loss: 0.19828\n",
      "Epoch: 27/2000... Step: 27... Loss: 0.19639... Val Loss: 0.19443\n",
      "Epoch: 28/2000... Step: 28... Loss: 0.19221... Val Loss: 0.19012\n",
      "Epoch: 29/2000... Step: 29... Loss: 0.18769... Val Loss: 0.18730\n",
      "Epoch: 30/2000... Step: 30... Loss: 0.18424... Val Loss: 0.18281\n",
      "Epoch: 31/2000... Step: 31... Loss: 0.17959... Val Loss: 0.18064\n",
      "Epoch: 32/2000... Step: 32... Loss: 0.17578... Val Loss: 0.18045\n",
      "Epoch: 33/2000... Step: 33... Loss: 0.17189... Val Loss: 0.17532\n",
      "Epoch: 34/2000... Step: 34... Loss: 0.16754... Val Loss: 0.17084\n",
      "Epoch: 35/2000... Step: 35... Loss: 0.16379... Val Loss: 0.16867\n",
      "Epoch: 36/2000... Step: 36... Loss: 0.15982... Val Loss: 0.16393\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 37/2000... Step: 37... Loss: 0.15588... Val Loss: 0.16968\n",
      "Epoch: 38/2000... Step: 38... Loss: 0.15360... Val Loss: 0.15358\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 39/2000... Step: 39... Loss: 0.15219... Val Loss: 0.15882\n",
      "Epoch: 40/2000... Step: 40... Loss: 0.14625... Val Loss: 0.15047\n",
      "Epoch: 41/2000... Step: 41... Loss: 0.14022... Val Loss: 0.14615\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 42/2000... Step: 42... Loss: 0.14027... Val Loss: 0.15236\n",
      "Epoch: 43/2000... Step: 43... Loss: 0.13750... Val Loss: 0.13796\n",
      "Epoch: 44/2000... Step: 44... Loss: 0.13164... Val Loss: 0.13494\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 45/2000... Step: 45... Loss: 0.12943... Val Loss: 0.14184\n",
      "Epoch: 46/2000... Step: 46... Loss: 0.12884... Val Loss: 0.12751\n",
      "Epoch: 47/2000... Step: 47... Loss: 0.12450... Val Loss: 0.12252\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 48/2000... Step: 48... Loss: 0.11957... Val Loss: 0.12292\n",
      "Epoch: 49/2000... Step: 49... Loss: 0.11855... Val Loss: 0.11852\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 50/2000... Step: 50... Loss: 0.11992... Val Loss: 0.12303\n",
      "Epoch: 51/2000... Step: 51... Loss: 0.11728... Val Loss: 0.11202\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 52/2000... Step: 52... Loss: 0.11320... Val Loss: 0.11293\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 53/2000... Step: 53... Loss: 0.10980... Val Loss: 0.11950\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 54/2000... Step: 54... Loss: 0.10962... Val Loss: 0.11329\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 55/2000... Step: 55... Loss: 0.11053... Val Loss: 0.12347\n",
      "Epoch: 56/2000... Step: 56... Loss: 0.11089... Val Loss: 0.11079\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 57/2000... Step: 57... Loss: 0.10902... Val Loss: 0.11490\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 58/2000... Step: 58... Loss: 0.10191... Val Loss: 0.11104\n",
      "Epoch: 59/2000... Step: 59... Loss: 0.09965... Val Loss: 0.10583\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 60/2000... Step: 60... Loss: 0.10214... Val Loss: 0.11353\n",
      "Epoch: 61/2000... Step: 61... Loss: 0.10003... Val Loss: 0.10351\n",
      "Epoch: 62/2000... Step: 62... Loss: 0.09590... Val Loss: 0.10158\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 63/2000... Step: 63... Loss: 0.09335... Val Loss: 0.10466\n",
      "Epoch: 64/2000... Step: 64... Loss: 0.09369... Val Loss: 0.09896\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 65/2000... Step: 65... Loss: 0.09461... Val Loss: 0.10183\n",
      "Epoch: 66/2000... Step: 66... Loss: 0.09191... Val Loss: 0.09401\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 67/2000... Step: 67... Loss: 0.08802... Val Loss: 0.09403\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 68/2000... Step: 68... Loss: 0.08735... Val Loss: 0.09657\n",
      "Epoch: 69/2000... Step: 69... Loss: 0.08729... Val Loss: 0.08923\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 70/2000... Step: 70... Loss: 0.08717... Val Loss: 0.09530\n",
      "Epoch: 71/2000... Step: 71... Loss: 0.08540... Val Loss: 0.08644\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 72/2000... Step: 72... Loss: 0.08254... Val Loss: 0.08847\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 73/2000... Step: 73... Loss: 0.08044... Val Loss: 0.08859\n",
      "Epoch: 74/2000... Step: 74... Loss: 0.07967... Val Loss: 0.08437\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 75/2000... Step: 75... Loss: 0.07964... Val Loss: 0.09311\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 76/2000... Step: 76... Loss: 0.08096... Val Loss: 0.08567\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 77/2000... Step: 77... Loss: 0.08305... Val Loss: 0.09610\n",
      "Epoch: 78/2000... Step: 78... Loss: 0.08261... Val Loss: 0.08325\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 79/2000... Step: 79... Loss: 0.07986... Val Loss: 0.08370\n",
      "Epoch: 80/2000... Step: 80... Loss: 0.07356... Val Loss: 0.08149\n",
      "Epoch: 81/2000... Step: 81... Loss: 0.07179... Val Loss: 0.07688\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 82/2000... Step: 82... Loss: 0.07396... Val Loss: 0.08416\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 83/2000... Step: 83... Loss: 0.07485... Val Loss: 0.07695\n",
      "Epoch: 84/2000... Step: 84... Loss: 0.07241... Val Loss: 0.07397\n",
      "Epoch: 85/2000... Step: 85... Loss: 0.06741... Val Loss: 0.07121\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 86/2000... Step: 86... Loss: 0.06674... Val Loss: 0.07138\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 87/2000... Step: 87... Loss: 0.06814... Val Loss: 0.07574\n",
      "Epoch: 88/2000... Step: 88... Loss: 0.06787... Val Loss: 0.06571\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 89/2000... Step: 89... Loss: 0.06550... Val Loss: 0.06753\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 90/2000... Step: 90... Loss: 0.06176... Val Loss: 0.06672\n",
      "Epoch: 91/2000... Step: 91... Loss: 0.06068... Val Loss: 0.06435\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 92/2000... Step: 92... Loss: 0.06123... Val Loss: 0.06930\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 93/2000... Step: 93... Loss: 0.06293... Val Loss: 0.07317\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 94/2000... Step: 94... Loss: 0.06779... Val Loss: 0.06932\n",
      "Epoch: 95/2000... Step: 95... Loss: 0.06273... Val Loss: 0.06078\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 96/2000... Step: 96... Loss: 0.05634... Val Loss: 0.06212\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 97/2000... Step: 97... Loss: 0.05485... Val Loss: 0.06333\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 98/2000... Step: 98... Loss: 0.05778... Val Loss: 0.06596\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 99/2000... Step: 99... Loss: 0.06286... Val Loss: 0.06706\n",
      "Epoch: 100/2000... Step: 100... Loss: 0.05721... Val Loss: 0.05300\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 101/2000... Step: 101... Loss: 0.05334... Val Loss: 0.05634\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 102/2000... Step: 102... Loss: 0.05145... Val Loss: 0.06122\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 103/2000... Step: 103... Loss: 0.05372... Val Loss: 0.05376\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 104/2000... Step: 104... Loss: 0.05520... Val Loss: 0.05390\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 105/2000... Step: 105... Loss: 0.04687... Val Loss: 0.05371\n",
      "Epoch: 106/2000... Step: 106... Loss: 0.04736... Val Loss: 0.04844\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 107/2000... Step: 107... Loss: 0.05017... Val Loss: 0.05492\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 108/2000... Step: 108... Loss: 0.04685... Val Loss: 0.05119\n",
      "Epoch: 109/2000... Step: 109... Loss: 0.04528... Val Loss: 0.04438\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 110/2000... Step: 110... Loss: 0.04574... Val Loss: 0.04868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 111/2000... Step: 111... Loss: 0.04168... Val Loss: 0.04877\n",
      "Epoch: 112/2000... Step: 112... Loss: 0.04197... Val Loss: 0.04280\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 113/2000... Step: 113... Loss: 0.04285... Val Loss: 0.04652\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 114/2000... Step: 114... Loss: 0.04069... Val Loss: 0.04617\n",
      "Epoch: 115/2000... Step: 115... Loss: 0.03934... Val Loss: 0.04038\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 116/2000... Step: 116... Loss: 0.03982... Val Loss: 0.04307\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 117/2000... Step: 117... Loss: 0.03744... Val Loss: 0.04124\n",
      "Epoch: 118/2000... Step: 118... Loss: 0.03633... Val Loss: 0.03785\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 119/2000... Step: 119... Loss: 0.03665... Val Loss: 0.04102\n",
      "Epoch: 120/2000... Step: 120... Loss: 0.03548... Val Loss: 0.03776\n",
      "Epoch: 121/2000... Step: 121... Loss: 0.03433... Val Loss: 0.03688\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 122/2000... Step: 122... Loss: 0.03444... Val Loss: 0.04055\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 123/2000... Step: 123... Loss: 0.03534... Val Loss: 0.03849\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 124/2000... Step: 124... Loss: 0.03652... Val Loss: 0.04295\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 125/2000... Step: 125... Loss: 0.04153... Val Loss: 0.05192\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 126/2000... Step: 126... Loss: 0.04549... Val Loss: 0.04272\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 127/2000... Step: 127... Loss: 0.04658... Val Loss: 0.03982\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 128/2000... Step: 128... Loss: 0.03463... Val Loss: 0.03760\n",
      "Epoch: 129/2000... Step: 129... Loss: 0.03342... Val Loss: 0.03625\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 130/2000... Step: 130... Loss: 0.03862... Val Loss: 0.04032\n",
      "Epoch: 131/2000... Step: 131... Loss: 0.03559... Val Loss: 0.03402\n",
      "Epoch: 132/2000... Step: 132... Loss: 0.03049... Val Loss: 0.03328\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 133/2000... Step: 133... Loss: 0.03217... Val Loss: 0.03903\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 134/2000... Step: 134... Loss: 0.03531... Val Loss: 0.03343\n",
      "Epoch: 135/2000... Step: 135... Loss: 0.03116... Val Loss: 0.03208\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 136/2000... Step: 136... Loss: 0.02910... Val Loss: 0.03339\n",
      "Epoch: 137/2000... Step: 137... Loss: 0.02943... Val Loss: 0.03060\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 138/2000... Step: 138... Loss: 0.02922... Val Loss: 0.03297\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 139/2000... Step: 139... Loss: 0.03012... Val Loss: 0.03244\n",
      "Epoch: 140/2000... Step: 140... Loss: 0.02861... Val Loss: 0.02804\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 141/2000... Step: 141... Loss: 0.02605... Val Loss: 0.02939\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 142/2000... Step: 142... Loss: 0.02675... Val Loss: 0.03181\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 143/2000... Step: 143... Loss: 0.02793... Val Loss: 0.02894\n",
      "Epoch: 144/2000... Step: 144... Loss: 0.02702... Val Loss: 0.02702\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 145/2000... Step: 145... Loss: 0.02544... Val Loss: 0.03063\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 146/2000... Step: 146... Loss: 0.02540... Val Loss: 0.02714\n",
      "Epoch: 147/2000... Step: 147... Loss: 0.02498... Val Loss: 0.02577\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 148/2000... Step: 148... Loss: 0.02553... Val Loss: 0.03105\n",
      "Epoch: 149/2000... Step: 149... Loss: 0.02519... Val Loss: 0.02523\n",
      "Epoch: 150/2000... Step: 150... Loss: 0.02354... Val Loss: 0.02461\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 151/2000... Step: 151... Loss: 0.02316... Val Loss: 0.02803\n",
      "Epoch: 152/2000... Step: 152... Loss: 0.02337... Val Loss: 0.02453\n",
      "Epoch: 153/2000... Step: 153... Loss: 0.02300... Val Loss: 0.02441\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 154/2000... Step: 154... Loss: 0.02320... Val Loss: 0.02667\n",
      "Epoch: 155/2000... Step: 155... Loss: 0.02356... Val Loss: 0.02433\n",
      "Epoch: 156/2000... Step: 156... Loss: 0.02297... Val Loss: 0.02324\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 157/2000... Step: 157... Loss: 0.02238... Val Loss: 0.02522\n",
      "Epoch: 158/2000... Step: 158... Loss: 0.02193... Val Loss: 0.02253\n",
      "Epoch: 159/2000... Step: 159... Loss: 0.02162... Val Loss: 0.02250\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 160/2000... Step: 160... Loss: 0.02119... Val Loss: 0.02339\n",
      "Epoch: 161/2000... Step: 161... Loss: 0.02102... Val Loss: 0.02239\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 162/2000... Step: 162... Loss: 0.02139... Val Loss: 0.02358\n",
      "Epoch: 163/2000... Step: 163... Loss: 0.02143... Val Loss: 0.02202\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 164/2000... Step: 164... Loss: 0.02141... Val Loss: 0.02356\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 165/2000... Step: 165... Loss: 0.02193... Val Loss: 0.02314\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 166/2000... Step: 166... Loss: 0.02310... Val Loss: 0.02546\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 167/2000... Step: 167... Loss: 0.02433... Val Loss: 0.02660\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 168/2000... Step: 168... Loss: 0.02595... Val Loss: 0.02921\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 169/2000... Step: 169... Loss: 0.02555... Val Loss: 0.02251\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 170/2000... Step: 170... Loss: 0.02359... Val Loss: 0.02402\n",
      "Epoch: 171/2000... Step: 171... Loss: 0.02063... Val Loss: 0.02144\n",
      "Epoch: 172/2000... Step: 172... Loss: 0.01957... Val Loss: 0.02033\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 173/2000... Step: 173... Loss: 0.02123... Val Loss: 0.02588\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 174/2000... Step: 174... Loss: 0.02210... Val Loss: 0.02304\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 175/2000... Step: 175... Loss: 0.02193... Val Loss: 0.02206\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 176/2000... Step: 176... Loss: 0.02118... Val Loss: 0.02203\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 177/2000... Step: 177... Loss: 0.01988... Val Loss: 0.02044\n",
      "Epoch: 178/2000... Step: 178... Loss: 0.01873... Val Loss: 0.01967\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 179/2000... Step: 179... Loss: 0.01922... Val Loss: 0.02168\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 180/2000... Step: 180... Loss: 0.02074... Val Loss: 0.02410\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 181/2000... Step: 181... Loss: 0.02189... Val Loss: 0.02178\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 182/2000... Step: 182... Loss: 0.02189... Val Loss: 0.02380\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 183/2000... Step: 183... Loss: 0.02077... Val Loss: 0.02021\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 184/2000... Step: 184... Loss: 0.01973... Val Loss: 0.01984\n",
      "Epoch: 185/2000... Step: 185... Loss: 0.01851... Val Loss: 0.01948\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 186/2000... Step: 186... Loss: 0.01785... Val Loss: 0.01973\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 187/2000... Step: 187... Loss: 0.01885... Val Loss: 0.02212\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 188/2000... Step: 188... Loss: 0.01973... Val Loss: 0.02002\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 189/2000... Step: 189... Loss: 0.01962... Val Loss: 0.02151\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 190/2000... Step: 190... Loss: 0.01916... Val Loss: 0.01994\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 191/2000... Step: 191... Loss: 0.01927... Val Loss: 0.01986\n",
      "Epoch: 192/2000... Step: 192... Loss: 0.01865... Val Loss: 0.01912\n",
      "Epoch: 193/2000... Step: 193... Loss: 0.01754... Val Loss: 0.01881\n",
      "Epoch: 194/2000... Step: 194... Loss: 0.01708... Val Loss: 0.01829\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 195/2000... Step: 195... Loss: 0.01749... Val Loss: 0.01892\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 196/2000... Step: 196... Loss: 0.01756... Val Loss: 0.01946\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 197/2000... Step: 197... Loss: 0.01752... Val Loss: 0.01878\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 198/2000... Step: 198... Loss: 0.01812... Val Loss: 0.02151\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 199/2000... Step: 199... Loss: 0.01855... Val Loss: 0.01859\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 200/2000... Step: 200... Loss: 0.01841... Val Loss: 0.01990\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 201/2000... Step: 201... Loss: 0.01798... Val Loss: 0.01975\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 202/2000... Step: 202... Loss: 0.01817... Val Loss: 0.01971\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 203/2000... Step: 203... Loss: 0.01845... Val Loss: 0.01943\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 204/2000... Step: 204... Loss: 0.01799... Val Loss: 0.01961\n",
      "Epoch: 205/2000... Step: 205... Loss: 0.01718... Val Loss: 0.01785\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 206/2000... Step: 206... Loss: 0.01742... Val Loss: 0.01936\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 207/2000... Step: 207... Loss: 0.01728... Val Loss: 0.01855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 208/2000... Step: 208... Loss: 0.01704... Val Loss: 0.01864\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 209/2000... Step: 209... Loss: 0.01676... Val Loss: 0.01869\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 210/2000... Step: 210... Loss: 0.01693... Val Loss: 0.01942\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 211/2000... Step: 211... Loss: 0.01758... Val Loss: 0.01999\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 212/2000... Step: 212... Loss: 0.01860... Val Loss: 0.02182\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 213/2000... Step: 213... Loss: 0.01982... Val Loss: 0.02378\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 214/2000... Step: 214... Loss: 0.02267... Val Loss: 0.02802\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 215/2000... Step: 215... Loss: 0.02487... Val Loss: 0.02592\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 216/2000... Step: 216... Loss: 0.02621... Val Loss: 0.02609\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 217/2000... Step: 217... Loss: 0.02235... Val Loss: 0.01827\n",
      "Epoch: 218/2000... Step: 218... Loss: 0.01726... Val Loss: 0.01644\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 219/2000... Step: 219... Loss: 0.01546... Val Loss: 0.02053\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 220/2000... Step: 220... Loss: 0.01739... Val Loss: 0.02066\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 221/2000... Step: 221... Loss: 0.01962... Val Loss: 0.02152\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 222/2000... Step: 222... Loss: 0.01990... Val Loss: 0.02124\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 223/2000... Step: 223... Loss: 0.01861... Val Loss: 0.01776\n",
      "Epoch: 224/2000... Step: 224... Loss: 0.01586... Val Loss: 0.01634\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 225/2000... Step: 225... Loss: 0.01494... Val Loss: 0.01834\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 226/2000... Step: 226... Loss: 0.01642... Val Loss: 0.01890\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 227/2000... Step: 227... Loss: 0.01752... Val Loss: 0.01758\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 228/2000... Step: 228... Loss: 0.01689... Val Loss: 0.01722\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 229/2000... Step: 229... Loss: 0.01520... Val Loss: 0.01642\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 230/2000... Step: 230... Loss: 0.01469... Val Loss: 0.01633\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 231/2000... Step: 231... Loss: 0.01507... Val Loss: 0.01710\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 232/2000... Step: 232... Loss: 0.01559... Val Loss: 0.01681\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 233/2000... Step: 233... Loss: 0.01590... Val Loss: 0.01666\n",
      "Epoch: 234/2000... Step: 234... Loss: 0.01532... Val Loss: 0.01558\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 235/2000... Step: 235... Loss: 0.01440... Val Loss: 0.01589\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 236/2000... Step: 236... Loss: 0.01391... Val Loss: 0.01572\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 237/2000... Step: 237... Loss: 0.01404... Val Loss: 0.01571\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 238/2000... Step: 238... Loss: 0.01446... Val Loss: 0.01676\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 239/2000... Step: 239... Loss: 0.01484... Val Loss: 0.01649\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 240/2000... Step: 240... Loss: 0.01526... Val Loss: 0.01716\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 241/2000... Step: 241... Loss: 0.01589... Val Loss: 0.01887\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 242/2000... Step: 242... Loss: 0.01686... Val Loss: 0.01846\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 243/2000... Step: 243... Loss: 0.01689... Val Loss: 0.01795\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 244/2000... Step: 244... Loss: 0.01669... Val Loss: 0.01856\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 245/2000... Step: 245... Loss: 0.01597... Val Loss: 0.01623\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 246/2000... Step: 246... Loss: 0.01527... Val Loss: 0.01613\n",
      "Epoch: 247/2000... Step: 247... Loss: 0.01420... Val Loss: 0.01533\n",
      "Epoch: 248/2000... Step: 248... Loss: 0.01333... Val Loss: 0.01469\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 249/2000... Step: 249... Loss: 0.01331... Val Loss: 0.01577\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 250/2000... Step: 250... Loss: 0.01401... Val Loss: 0.01601\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 251/2000... Step: 251... Loss: 0.01460... Val Loss: 0.01617\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 252/2000... Step: 252... Loss: 0.01458... Val Loss: 0.01580\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 253/2000... Step: 253... Loss: 0.01466... Val Loss: 0.01681\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 254/2000... Step: 254... Loss: 0.01482... Val Loss: 0.01632\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 255/2000... Step: 255... Loss: 0.01499... Val Loss: 0.01625\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 256/2000... Step: 256... Loss: 0.01480... Val Loss: 0.01660\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 257/2000... Step: 257... Loss: 0.01488... Val Loss: 0.01645\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 258/2000... Step: 258... Loss: 0.01483... Val Loss: 0.01601\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 259/2000... Step: 259... Loss: 0.01500... Val Loss: 0.01690\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 260/2000... Step: 260... Loss: 0.01478... Val Loss: 0.01540\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 261/2000... Step: 261... Loss: 0.01432... Val Loss: 0.01519\n",
      "Epoch: 262/2000... Step: 262... Loss: 0.01359... Val Loss: 0.01456\n",
      "Epoch: 263/2000... Step: 263... Loss: 0.01294... Val Loss: 0.01392\n",
      "Epoch: 264/2000... Step: 264... Loss: 0.01244... Val Loss: 0.01356\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 265/2000... Step: 265... Loss: 0.01216... Val Loss: 0.01393\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 266/2000... Step: 266... Loss: 0.01212... Val Loss: 0.01368\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 267/2000... Step: 267... Loss: 0.01226... Val Loss: 0.01403\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 268/2000... Step: 268... Loss: 0.01270... Val Loss: 0.01519\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 269/2000... Step: 269... Loss: 0.01340... Val Loss: 0.01591\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 270/2000... Step: 270... Loss: 0.01471... Val Loss: 0.01824\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 271/2000... Step: 271... Loss: 0.01672... Val Loss: 0.02255\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 272/2000... Step: 272... Loss: 0.02100... Val Loss: 0.02667\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 273/2000... Step: 273... Loss: 0.02459... Val Loss: 0.02805\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 274/2000... Step: 274... Loss: 0.02714... Val Loss: 0.02400\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 275/2000... Step: 275... Loss: 0.02094... Val Loss: 0.01544\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 276/2000... Step: 276... Loss: 0.01469... Val Loss: 0.01459\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 277/2000... Step: 277... Loss: 0.01353... Val Loss: 0.01998\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 278/2000... Step: 278... Loss: 0.01651... Val Loss: 0.01990\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 279/2000... Step: 279... Loss: 0.01849... Val Loss: 0.01673\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 280/2000... Step: 280... Loss: 0.01465... Val Loss: 0.01521\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 281/2000... Step: 281... Loss: 0.01206... Val Loss: 0.01665\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 282/2000... Step: 282... Loss: 0.01435... Val Loss: 0.01702\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 283/2000... Step: 283... Loss: 0.01519... Val Loss: 0.01510\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 284/2000... Step: 284... Loss: 0.01268... Val Loss: 0.01456\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 285/2000... Step: 285... Loss: 0.01178... Val Loss: 0.01538\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 286/2000... Step: 286... Loss: 0.01317... Val Loss: 0.01515\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 287/2000... Step: 287... Loss: 0.01298... Val Loss: 0.01457\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 288/2000... Step: 288... Loss: 0.01236... Val Loss: 0.01473\n",
      "Epoch: 289/2000... Step: 289... Loss: 0.01285... Val Loss: 0.01307\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 290/2000... Step: 290... Loss: 0.01210... Val Loss: 0.01334\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 291/2000... Step: 291... Loss: 0.01113... Val Loss: 0.01328\n",
      "Epoch: 292/2000... Step: 292... Loss: 0.01165... Val Loss: 0.01244\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 293/2000... Step: 293... Loss: 0.01227... Val Loss: 0.01445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 294/2000... Step: 294... Loss: 0.01244... Val Loss: 0.01411\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 295/2000... Step: 295... Loss: 0.01298... Val Loss: 0.01620\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 296/2000... Step: 296... Loss: 0.01454... Val Loss: 0.01652\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 297/2000... Step: 297... Loss: 0.01476... Val Loss: 0.01671\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 298/2000... Step: 298... Loss: 0.01453... Val Loss: 0.01353\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 299/2000... Step: 299... Loss: 0.01253... Val Loss: 0.01281\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 300/2000... Step: 300... Loss: 0.01089... Val Loss: 0.01321\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 301/2000... Step: 301... Loss: 0.01110... Val Loss: 0.01408\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 302/2000... Step: 302... Loss: 0.01235... Val Loss: 0.01400\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 303/2000... Step: 303... Loss: 0.01198... Val Loss: 0.01357\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 304/2000... Step: 304... Loss: 0.01101... Val Loss: 0.01252\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 305/2000... Step: 305... Loss: 0.01076... Val Loss: 0.01275\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 306/2000... Step: 306... Loss: 0.01100... Val Loss: 0.01358\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 307/2000... Step: 307... Loss: 0.01118... Val Loss: 0.01278\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 308/2000... Step: 308... Loss: 0.01121... Val Loss: 0.01246\n",
      "Epoch: 309/2000... Step: 309... Loss: 0.01082... Val Loss: 0.01221\n",
      "Epoch: 310/2000... Step: 310... Loss: 0.01023... Val Loss: 0.01169\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 311/2000... Step: 311... Loss: 0.01011... Val Loss: 0.01223\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 312/2000... Step: 312... Loss: 0.01055... Val Loss: 0.01284\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 313/2000... Step: 313... Loss: 0.01084... Val Loss: 0.01259\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 314/2000... Step: 314... Loss: 0.01080... Val Loss: 0.01283\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 315/2000... Step: 315... Loss: 0.01104... Val Loss: 0.01316\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 316/2000... Step: 316... Loss: 0.01134... Val Loss: 0.01382\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 317/2000... Step: 317... Loss: 0.01167... Val Loss: 0.01320\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 318/2000... Step: 318... Loss: 0.01143... Val Loss: 0.01353\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 319/2000... Step: 319... Loss: 0.01143... Val Loss: 0.01334\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 320/2000... Step: 320... Loss: 0.01115... Val Loss: 0.01236\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 321/2000... Step: 321... Loss: 0.01075... Val Loss: 0.01215\n",
      "Epoch: 322/2000... Step: 322... Loss: 0.00999... Val Loss: 0.01142\n",
      "Epoch: 323/2000... Step: 323... Loss: 0.00954... Val Loss: 0.01135\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 324/2000... Step: 324... Loss: 0.00955... Val Loss: 0.01197\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 325/2000... Step: 325... Loss: 0.00978... Val Loss: 0.01172\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 326/2000... Step: 326... Loss: 0.00999... Val Loss: 0.01198\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 327/2000... Step: 327... Loss: 0.01014... Val Loss: 0.01230\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 328/2000... Step: 328... Loss: 0.01026... Val Loss: 0.01225\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 329/2000... Step: 329... Loss: 0.01031... Val Loss: 0.01230\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 330/2000... Step: 330... Loss: 0.01025... Val Loss: 0.01232\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 331/2000... Step: 331... Loss: 0.01009... Val Loss: 0.01178\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 332/2000... Step: 332... Loss: 0.00996... Val Loss: 0.01165\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 333/2000... Step: 333... Loss: 0.00980... Val Loss: 0.01156\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 334/2000... Step: 334... Loss: 0.00970... Val Loss: 0.01140\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 335/2000... Step: 335... Loss: 0.00960... Val Loss: 0.01154\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 336/2000... Step: 336... Loss: 0.00966... Val Loss: 0.01163\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 337/2000... Step: 337... Loss: 0.00984... Val Loss: 0.01237\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 338/2000... Step: 338... Loss: 0.01037... Val Loss: 0.01314\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 339/2000... Step: 339... Loss: 0.01122... Val Loss: 0.01554\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 340/2000... Step: 340... Loss: 0.01326... Val Loss: 0.01812\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 341/2000... Step: 341... Loss: 0.01609... Val Loss: 0.02329\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 342/2000... Step: 342... Loss: 0.02110... Val Loss: 0.02656\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 343/2000... Step: 343... Loss: 0.02350... Val Loss: 0.02332\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 344/2000... Step: 344... Loss: 0.02156... Val Loss: 0.01648\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 345/2000... Step: 345... Loss: 0.01346... Val Loss: 0.01194\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 346/2000... Step: 346... Loss: 0.00945... Val Loss: 0.01408\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 347/2000... Step: 347... Loss: 0.01260... Val Loss: 0.01864\n",
      "EarlyStopping counter: 25 out of 30\n",
      "Epoch: 348/2000... Step: 348... Loss: 0.01560... Val Loss: 0.01555\n",
      "Epoch: 349/2000... Step: 349... Loss: 0.01318... Val Loss: 0.01116\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 350/2000... Step: 350... Loss: 0.00911... Val Loss: 0.01330\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 351/2000... Step: 351... Loss: 0.01091... Val Loss: 0.01620\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 352/2000... Step: 352... Loss: 0.01371... Val Loss: 0.01307\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 353/2000... Step: 353... Loss: 0.01112... Val Loss: 0.01123\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 354/2000... Step: 354... Loss: 0.00900... Val Loss: 0.01277\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 355/2000... Step: 355... Loss: 0.01018... Val Loss: 0.01282\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 356/2000... Step: 356... Loss: 0.01084... Val Loss: 0.01225\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 357/2000... Step: 357... Loss: 0.01027... Val Loss: 0.01118\n",
      "Epoch: 358/2000... Step: 358... Loss: 0.00936... Val Loss: 0.01082\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 359/2000... Step: 359... Loss: 0.00877... Val Loss: 0.01133\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 360/2000... Step: 360... Loss: 0.00931... Val Loss: 0.01177\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 361/2000... Step: 361... Loss: 0.01033... Val Loss: 0.01326\n",
      "Epoch: 362/2000... Step: 362... Loss: 0.01065... Val Loss: 0.01064\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 363/2000... Step: 363... Loss: 0.00905... Val Loss: 0.01093\n",
      "Epoch: 364/2000... Step: 364... Loss: 0.00854... Val Loss: 0.00983\n",
      "Epoch: 365/2000... Step: 365... Loss: 0.00820... Val Loss: 0.00968\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 366/2000... Step: 366... Loss: 0.00786... Val Loss: 0.01015\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 367/2000... Step: 367... Loss: 0.00795... Val Loss: 0.00999\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 368/2000... Step: 368... Loss: 0.00798... Val Loss: 0.01031\n",
      "Epoch: 369/2000... Step: 369... Loss: 0.00771... Val Loss: 0.00951\n",
      "Epoch: 370/2000... Step: 370... Loss: 0.00748... Val Loss: 0.00899\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 371/2000... Step: 371... Loss: 0.00764... Val Loss: 0.00931\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 372/2000... Step: 372... Loss: 0.00748... Val Loss: 0.00934\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 373/2000... Step: 373... Loss: 0.00751... Val Loss: 0.00957\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 374/2000... Step: 374... Loss: 0.00755... Val Loss: 0.00909\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 375/2000... Step: 375... Loss: 0.00772... Val Loss: 0.01043\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 376/2000... Step: 376... Loss: 0.00816... Val Loss: 0.01003\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 377/2000... Step: 377... Loss: 0.00909... Val Loss: 0.01517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 378/2000... Step: 378... Loss: 0.01234... Val Loss: 0.01779\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 379/2000... Step: 379... Loss: 0.01722... Val Loss: 0.02853\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 380/2000... Step: 380... Loss: 0.02451... Val Loss: 0.01804\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 381/2000... Step: 381... Loss: 0.01663... Val Loss: 0.01165\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 382/2000... Step: 382... Loss: 0.00972... Val Loss: 0.01482\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 383/2000... Step: 383... Loss: 0.01213... Val Loss: 0.01561\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 384/2000... Step: 384... Loss: 0.01248... Val Loss: 0.01270\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 385/2000... Step: 385... Loss: 0.01075... Val Loss: 0.01388\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 386/2000... Step: 386... Loss: 0.01244... Val Loss: 0.01307\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 387/2000... Step: 387... Loss: 0.00928... Val Loss: 0.01194\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 388/2000... Step: 388... Loss: 0.00977... Val Loss: 0.01157\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 389/2000... Step: 389... Loss: 0.01147... Val Loss: 0.01260\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 390/2000... Step: 390... Loss: 0.00863... Val Loss: 0.01118\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 391/2000... Step: 391... Loss: 0.00866... Val Loss: 0.00956\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 392/2000... Step: 392... Loss: 0.00913... Val Loss: 0.01186\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 393/2000... Step: 393... Loss: 0.00812... Val Loss: 0.01190\n",
      "Epoch: 394/2000... Step: 394... Loss: 0.00827... Val Loss: 0.00847\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 395/2000... Step: 395... Loss: 0.00787... Val Loss: 0.00964\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 396/2000... Step: 396... Loss: 0.00700... Val Loss: 0.01223\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 397/2000... Step: 397... Loss: 0.00832... Val Loss: 0.00894\n",
      "Epoch: 398/2000... Step: 398... Loss: 0.00711... Val Loss: 0.00786\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 399/2000... Step: 399... Loss: 0.00646... Val Loss: 0.01013\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 400/2000... Step: 400... Loss: 0.00732... Val Loss: 0.00997\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 401/2000... Step: 401... Loss: 0.00749... Val Loss: 0.00793\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 402/2000... Step: 402... Loss: 0.00730... Val Loss: 0.00844\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 403/2000... Step: 403... Loss: 0.00615... Val Loss: 0.00901\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 404/2000... Step: 404... Loss: 0.00615... Val Loss: 0.00828\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 405/2000... Step: 405... Loss: 0.00711... Val Loss: 0.00878\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 406/2000... Step: 406... Loss: 0.00658... Val Loss: 0.00889\n",
      "Epoch: 407/2000... Step: 407... Loss: 0.00701... Val Loss: 0.00778\n",
      "Epoch: 408/2000... Step: 408... Loss: 0.00621... Val Loss: 0.00670\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 409/2000... Step: 409... Loss: 0.00569... Val Loss: 0.00796\n",
      "Epoch: 410/2000... Step: 410... Loss: 0.00573... Val Loss: 0.00651\n",
      "Epoch: 411/2000... Step: 411... Loss: 0.00542... Val Loss: 0.00618\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 412/2000... Step: 412... Loss: 0.00552... Val Loss: 0.00765\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 413/2000... Step: 413... Loss: 0.00549... Val Loss: 0.00662\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 414/2000... Step: 414... Loss: 0.00564... Val Loss: 0.00759\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 415/2000... Step: 415... Loss: 0.00661... Val Loss: 0.00873\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 416/2000... Step: 416... Loss: 0.00752... Val Loss: 0.01210\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 417/2000... Step: 417... Loss: 0.00925... Val Loss: 0.01269\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 418/2000... Step: 418... Loss: 0.01119... Val Loss: 0.01440\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 419/2000... Step: 419... Loss: 0.01211... Val Loss: 0.00973\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 420/2000... Step: 420... Loss: 0.00841... Val Loss: 0.00815\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 421/2000... Step: 421... Loss: 0.00651... Val Loss: 0.00823\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 422/2000... Step: 422... Loss: 0.00629... Val Loss: 0.00891\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 423/2000... Step: 423... Loss: 0.00697... Val Loss: 0.00949\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 424/2000... Step: 424... Loss: 0.00742... Val Loss: 0.00812\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 425/2000... Step: 425... Loss: 0.00719... Val Loss: 0.00796\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 426/2000... Step: 426... Loss: 0.00606... Val Loss: 0.00678\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 427/2000... Step: 427... Loss: 0.00508... Val Loss: 0.00658\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 428/2000... Step: 428... Loss: 0.00594... Val Loss: 0.00791\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 429/2000... Step: 429... Loss: 0.00675... Val Loss: 0.00635\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 430/2000... Step: 430... Loss: 0.00528... Val Loss: 0.00635\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 431/2000... Step: 431... Loss: 0.00477... Val Loss: 0.00711\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 432/2000... Step: 432... Loss: 0.00540... Val Loss: 0.00646\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 433/2000... Step: 433... Loss: 0.00547... Val Loss: 0.00620\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 434/2000... Step: 434... Loss: 0.00491... Val Loss: 0.00623\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 435/2000... Step: 435... Loss: 0.00479... Val Loss: 0.00682\n",
      "Epoch: 436/2000... Step: 436... Loss: 0.00500... Val Loss: 0.00602\n",
      "Epoch: 437/2000... Step: 437... Loss: 0.00484... Val Loss: 0.00559\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 438/2000... Step: 438... Loss: 0.00441... Val Loss: 0.00605\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 439/2000... Step: 439... Loss: 0.00460... Val Loss: 0.00586\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 440/2000... Step: 440... Loss: 0.00493... Val Loss: 0.00632\n",
      "Epoch: 441/2000... Step: 441... Loss: 0.00478... Val Loss: 0.00557\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 442/2000... Step: 442... Loss: 0.00449... Val Loss: 0.00573\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 443/2000... Step: 443... Loss: 0.00453... Val Loss: 0.00562\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 444/2000... Step: 444... Loss: 0.00451... Val Loss: 0.00622\n",
      "Epoch: 445/2000... Step: 445... Loss: 0.00461... Val Loss: 0.00539\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 446/2000... Step: 446... Loss: 0.00458... Val Loss: 0.00600\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 447/2000... Step: 447... Loss: 0.00475... Val Loss: 0.00603\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 448/2000... Step: 448... Loss: 0.00526... Val Loss: 0.00784\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 449/2000... Step: 449... Loss: 0.00642... Val Loss: 0.00827\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 450/2000... Step: 450... Loss: 0.00712... Val Loss: 0.01055\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 451/2000... Step: 451... Loss: 0.00830... Val Loss: 0.00874\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 452/2000... Step: 452... Loss: 0.00786... Val Loss: 0.00830\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 453/2000... Step: 453... Loss: 0.00651... Val Loss: 0.00636\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 454/2000... Step: 454... Loss: 0.00533... Val Loss: 0.00624\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 455/2000... Step: 455... Loss: 0.00490... Val Loss: 0.00611\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 456/2000... Step: 456... Loss: 0.00459... Val Loss: 0.00629\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 457/2000... Step: 457... Loss: 0.00507... Val Loss: 0.00746\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 458/2000... Step: 458... Loss: 0.00587... Val Loss: 0.00624\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 459/2000... Step: 459... Loss: 0.00538... Val Loss: 0.00574\n",
      "Epoch: 460/2000... Step: 460... Loss: 0.00411... Val Loss: 0.00527\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 461/2000... Step: 461... Loss: 0.00390... Val Loss: 0.00541\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 462/2000... Step: 462... Loss: 0.00464... Val Loss: 0.00618\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 463/2000... Step: 463... Loss: 0.00473... Val Loss: 0.00542\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 464/2000... Step: 464... Loss: 0.00423... Val Loss: 0.00564\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 465/2000... Step: 465... Loss: 0.00410... Val Loss: 0.00536\n",
      "Epoch: 466/2000... Step: 466... Loss: 0.00411... Val Loss: 0.00510\n",
      "Epoch: 467/2000... Step: 467... Loss: 0.00385... Val Loss: 0.00498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 468/2000... Step: 468... Loss: 0.00369... Val Loss: 0.00501\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 469/2000... Step: 469... Loss: 0.00398... Val Loss: 0.00605\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 470/2000... Step: 470... Loss: 0.00443... Val Loss: 0.00565\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 471/2000... Step: 471... Loss: 0.00464... Val Loss: 0.00628\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 472/2000... Step: 472... Loss: 0.00480... Val Loss: 0.00582\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 473/2000... Step: 473... Loss: 0.00470... Val Loss: 0.00649\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 474/2000... Step: 474... Loss: 0.00497... Val Loss: 0.00606\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 475/2000... Step: 475... Loss: 0.00511... Val Loss: 0.00691\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 476/2000... Step: 476... Loss: 0.00531... Val Loss: 0.00599\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 477/2000... Step: 477... Loss: 0.00511... Val Loss: 0.00649\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 478/2000... Step: 478... Loss: 0.00485... Val Loss: 0.00540\n",
      "Epoch: 479/2000... Step: 479... Loss: 0.00421... Val Loss: 0.00489\n",
      "Epoch: 480/2000... Step: 480... Loss: 0.00362... Val Loss: 0.00426\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 481/2000... Step: 481... Loss: 0.00325... Val Loss: 0.00438\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 482/2000... Step: 482... Loss: 0.00325... Val Loss: 0.00479\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 483/2000... Step: 483... Loss: 0.00349... Val Loss: 0.00485\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 484/2000... Step: 484... Loss: 0.00371... Val Loss: 0.00528\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 485/2000... Step: 485... Loss: 0.00396... Val Loss: 0.00504\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 486/2000... Step: 486... Loss: 0.00410... Val Loss: 0.00566\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 487/2000... Step: 487... Loss: 0.00425... Val Loss: 0.00520\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 488/2000... Step: 488... Loss: 0.00431... Val Loss: 0.00593\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 489/2000... Step: 489... Loss: 0.00459... Val Loss: 0.00606\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 490/2000... Step: 490... Loss: 0.00499... Val Loss: 0.00755\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 491/2000... Step: 491... Loss: 0.00603... Val Loss: 0.00805\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 492/2000... Step: 492... Loss: 0.00696... Val Loss: 0.01056\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 493/2000... Step: 493... Loss: 0.00831... Val Loss: 0.00862\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 494/2000... Step: 494... Loss: 0.00741... Val Loss: 0.00779\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 495/2000... Step: 495... Loss: 0.00620... Val Loss: 0.00615\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 496/2000... Step: 496... Loss: 0.00532... Val Loss: 0.00647\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 497/2000... Step: 497... Loss: 0.00468... Val Loss: 0.00467\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 498/2000... Step: 498... Loss: 0.00357... Val Loss: 0.00473\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 499/2000... Step: 499... Loss: 0.00349... Val Loss: 0.00621\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 500/2000... Step: 500... Loss: 0.00460... Val Loss: 0.00569\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 501/2000... Step: 501... Loss: 0.00484... Val Loss: 0.00550\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 502/2000... Step: 502... Loss: 0.00387... Val Loss: 0.00475\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 503/2000... Step: 503... Loss: 0.00329... Val Loss: 0.00462\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 504/2000... Step: 504... Loss: 0.00350... Val Loss: 0.00528\n",
      "EarlyStopping counter: 25 out of 30\n",
      "Epoch: 505/2000... Step: 505... Loss: 0.00351... Val Loss: 0.00466\n",
      "EarlyStopping counter: 26 out of 30\n",
      "Epoch: 506/2000... Step: 506... Loss: 0.00336... Val Loss: 0.00516\n",
      "EarlyStopping counter: 27 out of 30\n",
      "Epoch: 507/2000... Step: 507... Loss: 0.00363... Val Loss: 0.00536\n",
      "EarlyStopping counter: 28 out of 30\n",
      "Epoch: 508/2000... Step: 508... Loss: 0.00381... Val Loss: 0.00501\n",
      "Epoch: 509/2000... Step: 509... Loss: 0.00348... Val Loss: 0.00407\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 510/2000... Step: 510... Loss: 0.00296... Val Loss: 0.00418\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 511/2000... Step: 511... Loss: 0.00277... Val Loss: 0.00427\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 512/2000... Step: 512... Loss: 0.00293... Val Loss: 0.00442\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 513/2000... Step: 513... Loss: 0.00320... Val Loss: 0.00493\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 514/2000... Step: 514... Loss: 0.00354... Val Loss: 0.00521\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 515/2000... Step: 515... Loss: 0.00397... Val Loss: 0.00659\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 516/2000... Step: 516... Loss: 0.00503... Val Loss: 0.00741\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 517/2000... Step: 517... Loss: 0.00626... Val Loss: 0.01013\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 518/2000... Step: 518... Loss: 0.00833... Val Loss: 0.01002\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 519/2000... Step: 519... Loss: 0.00886... Val Loss: 0.01005\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 520/2000... Step: 520... Loss: 0.00826... Val Loss: 0.00615\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 521/2000... Step: 521... Loss: 0.00518... Val Loss: 0.00440\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 522/2000... Step: 522... Loss: 0.00312... Val Loss: 0.00479\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 523/2000... Step: 523... Loss: 0.00359... Val Loss: 0.00623\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 524/2000... Step: 524... Loss: 0.00499... Val Loss: 0.00680\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 525/2000... Step: 525... Loss: 0.00545... Val Loss: 0.00543\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 526/2000... Step: 526... Loss: 0.00450... Val Loss: 0.00472\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 527/2000... Step: 527... Loss: 0.00324... Val Loss: 0.00419\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 528/2000... Step: 528... Loss: 0.00290... Val Loss: 0.00484\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 529/2000... Step: 529... Loss: 0.00373... Val Loss: 0.00581\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 530/2000... Step: 530... Loss: 0.00443... Val Loss: 0.00495\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 531/2000... Step: 531... Loss: 0.00382... Val Loss: 0.00432\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 532/2000... Step: 532... Loss: 0.00285... Val Loss: 0.00406\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 533/2000... Step: 533... Loss: 0.00263... Val Loss: 0.00418\n",
      "EarlyStopping counter: 25 out of 30\n",
      "Epoch: 534/2000... Step: 534... Loss: 0.00312... Val Loss: 0.00478\n",
      "EarlyStopping counter: 26 out of 30\n",
      "Epoch: 535/2000... Step: 535... Loss: 0.00339... Val Loss: 0.00439\n",
      "EarlyStopping counter: 27 out of 30\n",
      "Epoch: 536/2000... Step: 536... Loss: 0.00323... Val Loss: 0.00457\n",
      "EarlyStopping counter: 28 out of 30\n",
      "Epoch: 537/2000... Step: 537... Loss: 0.00314... Val Loss: 0.00456\n",
      "EarlyStopping counter: 29 out of 30\n",
      "Epoch: 538/2000... Step: 538... Loss: 0.00323... Val Loss: 0.00479\n",
      "EarlyStopping counter: 30 out of 30\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "train_losess, val_losses = train(net, training_generator, validation_generator, verbose=True,\n",
    "                                                             opt_func=opt, criterion_func=criterion, epochs=num_epochs,\n",
    "                                                             lr=0.01, check_early_stopping=True, check_auc_roc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after train on the Test set:\n",
      "0.0041\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss after train on the Test set:\")\n",
    "print(infer(net, test_generator, nn.MSELoss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the plots:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the mse loss and compare the train and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_vs_epochs(train_losess, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a new neural network and try overfitting your training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_num)\n",
    "x = np.linspace(-5, 5, 30)\n",
    "y = np.linspace(-5, 5, 30)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "z = np.sin(xx) * np.cos(yy) + 0.1 * np.random.rand(xx.shape[0], xx.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = xx.flatten()\n",
    "yy = yy.flatten()\n",
    "targets = z.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate((xx.reshape(-1,1), yy.reshape(-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(inputs, targets, test_size=0.15, random_state=random_num)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, random_state=random_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generators for the data\n",
    "training_generator, validation_generator, test_generator = create_generators(x_train, y_train, x_val, y_val, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MlpNetwork(num_net_channels_part_c_2, nn.ReLU(), classifier=False)\n",
    "opt = torch.optim.Adam\n",
    "criterion = nn.MSELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2000... Step: 1... Loss: 0.26069... Val Loss: 0.48543\n",
      "Epoch: 2/2000... Step: 2... Loss: 0.46874... Val Loss: 0.37832\n",
      "Epoch: 3/2000... Step: 3... Loss: 0.42838... Val Loss: 0.23399\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 4/2000... Step: 4... Loss: 0.25682... Val Loss: 0.35048\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 5/2000... Step: 5... Loss: 0.34755... Val Loss: 0.23783\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 6/2000... Step: 6... Loss: 0.25191... Val Loss: 0.23402\n",
      "Epoch: 7/2000... Step: 7... Loss: 0.25144... Val Loss: 0.23378\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 8/2000... Step: 8... Loss: 0.25129... Val Loss: 0.23421\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 9/2000... Step: 9... Loss: 0.25092... Val Loss: 0.23388\n",
      "Epoch: 10/2000... Step: 10... Loss: 0.24954... Val Loss: 0.23291\n",
      "Epoch: 11/2000... Step: 11... Loss: 0.24776... Val Loss: 0.23031\n",
      "Epoch: 12/2000... Step: 12... Loss: 0.24554... Val Loss: 0.22720\n",
      "Epoch: 13/2000... Step: 13... Loss: 0.24284... Val Loss: 0.22392\n",
      "Epoch: 14/2000... Step: 14... Loss: 0.23918... Val Loss: 0.22182\n",
      "Epoch: 15/2000... Step: 15... Loss: 0.23510... Val Loss: 0.21493\n",
      "Epoch: 16/2000... Step: 16... Loss: 0.22950... Val Loss: 0.21098\n",
      "Epoch: 17/2000... Step: 17... Loss: 0.22208... Val Loss: 0.20472\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 18/2000... Step: 18... Loss: 0.21558... Val Loss: 0.21439\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 19/2000... Step: 19... Loss: 0.21718... Val Loss: 0.21181\n",
      "Epoch: 20/2000... Step: 20... Loss: 0.23418... Val Loss: 0.19627\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 21/2000... Step: 21... Loss: 0.20137... Val Loss: 0.20632\n",
      "Epoch: 22/2000... Step: 22... Loss: 0.20679... Val Loss: 0.17382\n",
      "Epoch: 23/2000... Step: 23... Loss: 0.18997... Val Loss: 0.17220\n",
      "Epoch: 24/2000... Step: 24... Loss: 0.19143... Val Loss: 0.17198\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 25/2000... Step: 25... Loss: 0.17726... Val Loss: 0.18308\n",
      "Epoch: 26/2000... Step: 26... Loss: 0.17959... Val Loss: 0.15491\n",
      "Epoch: 27/2000... Step: 27... Loss: 0.16484... Val Loss: 0.14764\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 28/2000... Step: 28... Loss: 0.16382... Val Loss: 0.15697\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 29/2000... Step: 29... Loss: 0.15757... Val Loss: 0.15154\n",
      "Epoch: 30/2000... Step: 30... Loss: 0.14970... Val Loss: 0.14580\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 31/2000... Step: 31... Loss: 0.15516... Val Loss: 0.14685\n",
      "Epoch: 32/2000... Step: 32... Loss: 0.14165... Val Loss: 0.14572\n",
      "Epoch: 33/2000... Step: 33... Loss: 0.13797... Val Loss: 0.14079\n",
      "Epoch: 34/2000... Step: 34... Loss: 0.13987... Val Loss: 0.13866\n",
      "Epoch: 35/2000... Step: 35... Loss: 0.13003... Val Loss: 0.12232\n",
      "Epoch: 36/2000... Step: 36... Loss: 0.12136... Val Loss: 0.11827\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 37/2000... Step: 37... Loss: 0.12348... Val Loss: 0.12918\n",
      "Epoch: 38/2000... Step: 38... Loss: 0.12214... Val Loss: 0.11456\n",
      "Epoch: 39/2000... Step: 39... Loss: 0.11199... Val Loss: 0.10648\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 40/2000... Step: 40... Loss: 0.10463... Val Loss: 0.11835\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 41/2000... Step: 41... Loss: 0.11113... Val Loss: 0.11557\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 42/2000... Step: 42... Loss: 0.11782... Val Loss: 0.13194\n",
      "Epoch: 43/2000... Step: 43... Loss: 0.11932... Val Loss: 0.09752\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 44/2000... Step: 44... Loss: 0.09437... Val Loss: 0.09993\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 45/2000... Step: 45... Loss: 0.09657... Val Loss: 0.11438\n",
      "Epoch: 46/2000... Step: 46... Loss: 0.10444... Val Loss: 0.08523\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 47/2000... Step: 47... Loss: 0.08506... Val Loss: 0.09072\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 48/2000... Step: 48... Loss: 0.09142... Val Loss: 0.09505\n",
      "Epoch: 49/2000... Step: 49... Loss: 0.08544... Val Loss: 0.08456\n",
      "Epoch: 50/2000... Step: 50... Loss: 0.07698... Val Loss: 0.07824\n",
      "Epoch: 51/2000... Step: 51... Loss: 0.08216... Val Loss: 0.07572\n",
      "Epoch: 52/2000... Step: 52... Loss: 0.07202... Val Loss: 0.07436\n",
      "Epoch: 53/2000... Step: 53... Loss: 0.06899... Val Loss: 0.07179\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 54/2000... Step: 54... Loss: 0.07028... Val Loss: 0.07250\n",
      "Epoch: 55/2000... Step: 55... Loss: 0.06419... Val Loss: 0.05751\n",
      "Epoch: 56/2000... Step: 56... Loss: 0.05645... Val Loss: 0.05690\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 57/2000... Step: 57... Loss: 0.05408... Val Loss: 0.07171\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 58/2000... Step: 58... Loss: 0.06027... Val Loss: 0.06732\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 59/2000... Step: 59... Loss: 0.06875... Val Loss: 0.08362\n",
      "Epoch: 60/2000... Step: 60... Loss: 0.07418... Val Loss: 0.05086\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 61/2000... Step: 61... Loss: 0.05245... Val Loss: 0.05192\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 62/2000... Step: 62... Loss: 0.04634... Val Loss: 0.06422\n",
      "Epoch: 63/2000... Step: 63... Loss: 0.05465... Val Loss: 0.03963\n",
      "Epoch: 64/2000... Step: 64... Loss: 0.04365... Val Loss: 0.03821\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 65/2000... Step: 65... Loss: 0.04346... Val Loss: 0.04797\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 66/2000... Step: 66... Loss: 0.04259... Val Loss: 0.04318\n",
      "Epoch: 67/2000... Step: 67... Loss: 0.03707... Val Loss: 0.03781\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 68/2000... Step: 68... Loss: 0.03751... Val Loss: 0.03962\n",
      "Epoch: 69/2000... Step: 69... Loss: 0.04101... Val Loss: 0.03561\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 70/2000... Step: 70... Loss: 0.03533... Val Loss: 0.04054\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 71/2000... Step: 71... Loss: 0.03562... Val Loss: 0.03970\n",
      "Epoch: 72/2000... Step: 72... Loss: 0.03419... Val Loss: 0.02973\n",
      "Epoch: 73/2000... Step: 73... Loss: 0.02937... Val Loss: 0.02865\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 74/2000... Step: 74... Loss: 0.02980... Val Loss: 0.03291\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 75/2000... Step: 75... Loss: 0.02951... Val Loss: 0.03026\n",
      "Epoch: 76/2000... Step: 76... Loss: 0.02821... Val Loss: 0.02747\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 77/2000... Step: 77... Loss: 0.02476... Val Loss: 0.03005\n",
      "Epoch: 78/2000... Step: 78... Loss: 0.02555... Val Loss: 0.02713\n",
      "Epoch: 79/2000... Step: 79... Loss: 0.02555... Val Loss: 0.02629\n",
      "Epoch: 80/2000... Step: 80... Loss: 0.02351... Val Loss: 0.02243\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 81/2000... Step: 81... Loss: 0.02029... Val Loss: 0.02287\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 82/2000... Step: 82... Loss: 0.02185... Val Loss: 0.02371\n",
      "Epoch: 83/2000... Step: 83... Loss: 0.02066... Val Loss: 0.02171\n",
      "Epoch: 84/2000... Step: 84... Loss: 0.02163... Val Loss: 0.01983\n",
      "Epoch: 85/2000... Step: 85... Loss: 0.01848... Val Loss: 0.01862\n",
      "Epoch: 86/2000... Step: 86... Loss: 0.01823... Val Loss: 0.01825\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 87/2000... Step: 87... Loss: 0.01723... Val Loss: 0.02038\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 88/2000... Step: 88... Loss: 0.01831... Val Loss: 0.01862\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 89/2000... Step: 89... Loss: 0.01774... Val Loss: 0.02136\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 90/2000... Step: 90... Loss: 0.01901... Val Loss: 0.01932\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 91/2000... Step: 91... Loss: 0.01880... Val Loss: 0.02103\n",
      "Epoch: 92/2000... Step: 92... Loss: 0.01807... Val Loss: 0.01820\n",
      "Epoch: 93/2000... Step: 93... Loss: 0.01639... Val Loss: 0.01790\n",
      "Epoch: 94/2000... Step: 94... Loss: 0.01551... Val Loss: 0.01688\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 95/2000... Step: 95... Loss: 0.01454... Val Loss: 0.01719\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 96/2000... Step: 96... Loss: 0.01527... Val Loss: 0.01742\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 97/2000... Step: 97... Loss: 0.01527... Val Loss: 0.01726\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 98/2000... Step: 98... Loss: 0.01582... Val Loss: 0.01749\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 99/2000... Step: 99... Loss: 0.01515... Val Loss: 0.01731\n",
      "Epoch: 100/2000... Step: 100... Loss: 0.01488... Val Loss: 0.01572\n",
      "Epoch: 101/2000... Step: 101... Loss: 0.01391... Val Loss: 0.01477\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 102/2000... Step: 102... Loss: 0.01350... Val Loss: 0.01493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 103/2000... Step: 103... Loss: 0.01267... Val Loss: 0.01479\n",
      "Epoch: 104/2000... Step: 104... Loss: 0.01231... Val Loss: 0.01376\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 105/2000... Step: 105... Loss: 0.01192... Val Loss: 0.01385\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 106/2000... Step: 106... Loss: 0.01185... Val Loss: 0.01384\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 107/2000... Step: 107... Loss: 0.01175... Val Loss: 0.01424\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 108/2000... Step: 108... Loss: 0.01226... Val Loss: 0.01607\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 109/2000... Step: 109... Loss: 0.01403... Val Loss: 0.02184\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 110/2000... Step: 110... Loss: 0.02015... Val Loss: 0.03331\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 111/2000... Step: 111... Loss: 0.03150... Val Loss: 0.04512\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 112/2000... Step: 112... Loss: 0.04289... Val Loss: 0.02207\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 113/2000... Step: 113... Loss: 0.02103... Val Loss: 0.01801\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 114/2000... Step: 114... Loss: 0.01603... Val Loss: 0.03190\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 115/2000... Step: 115... Loss: 0.02897... Val Loss: 0.01422\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 116/2000... Step: 116... Loss: 0.01331... Val Loss: 0.02159\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 117/2000... Step: 117... Loss: 0.02173... Val Loss: 0.01958\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 118/2000... Step: 118... Loss: 0.01813... Val Loss: 0.01771\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 119/2000... Step: 119... Loss: 0.01558... Val Loss: 0.02307\n",
      "Epoch: 120/2000... Step: 120... Loss: 0.02105... Val Loss: 0.01233\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 121/2000... Step: 121... Loss: 0.01200... Val Loss: 0.01691\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 122/2000... Step: 122... Loss: 0.01731... Val Loss: 0.01360\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 123/2000... Step: 123... Loss: 0.01219... Val Loss: 0.01674\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 124/2000... Step: 124... Loss: 0.01398... Val Loss: 0.01528\n",
      "Epoch: 125/2000... Step: 125... Loss: 0.01414... Val Loss: 0.01046\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 126/2000... Step: 126... Loss: 0.01079... Val Loss: 0.01510\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 127/2000... Step: 127... Loss: 0.01464... Val Loss: 0.01293\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 128/2000... Step: 128... Loss: 0.01080... Val Loss: 0.01432\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 129/2000... Step: 129... Loss: 0.01139... Val Loss: 0.01261\n",
      "Epoch: 130/2000... Step: 130... Loss: 0.01190... Val Loss: 0.00959\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 131/2000... Step: 131... Loss: 0.00911... Val Loss: 0.01254\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 132/2000... Step: 132... Loss: 0.01133... Val Loss: 0.01148\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 133/2000... Step: 133... Loss: 0.00960... Val Loss: 0.01046\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 134/2000... Step: 134... Loss: 0.00874... Val Loss: 0.01122\n",
      "Epoch: 135/2000... Step: 135... Loss: 0.00992... Val Loss: 0.00894\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 136/2000... Step: 136... Loss: 0.00798... Val Loss: 0.01016\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 137/2000... Step: 137... Loss: 0.00872... Val Loss: 0.01059\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 138/2000... Step: 138... Loss: 0.00878... Val Loss: 0.00931\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 139/2000... Step: 139... Loss: 0.00735... Val Loss: 0.00955\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 140/2000... Step: 140... Loss: 0.00789... Val Loss: 0.00992\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 141/2000... Step: 141... Loss: 0.00783... Val Loss: 0.00902\n",
      "Epoch: 142/2000... Step: 142... Loss: 0.00673... Val Loss: 0.00888\n",
      "Epoch: 143/2000... Step: 143... Loss: 0.00726... Val Loss: 0.00856\n",
      "Epoch: 144/2000... Step: 144... Loss: 0.00728... Val Loss: 0.00773\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 145/2000... Step: 145... Loss: 0.00619... Val Loss: 0.00829\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 146/2000... Step: 146... Loss: 0.00646... Val Loss: 0.00828\n",
      "Epoch: 147/2000... Step: 147... Loss: 0.00658... Val Loss: 0.00738\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 148/2000... Step: 148... Loss: 0.00594... Val Loss: 0.00742\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 149/2000... Step: 149... Loss: 0.00562... Val Loss: 0.00791\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 150/2000... Step: 150... Loss: 0.00593... Val Loss: 0.00745\n",
      "Epoch: 151/2000... Step: 151... Loss: 0.00569... Val Loss: 0.00705\n",
      "Epoch: 152/2000... Step: 152... Loss: 0.00521... Val Loss: 0.00657\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 153/2000... Step: 153... Loss: 0.00510... Val Loss: 0.00695\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 154/2000... Step: 154... Loss: 0.00511... Val Loss: 0.00728\n",
      "Epoch: 155/2000... Step: 155... Loss: 0.00513... Val Loss: 0.00651\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 156/2000... Step: 156... Loss: 0.00498... Val Loss: 0.00666\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 157/2000... Step: 157... Loss: 0.00447... Val Loss: 0.00650\n",
      "Epoch: 158/2000... Step: 158... Loss: 0.00439... Val Loss: 0.00593\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 159/2000... Step: 159... Loss: 0.00454... Val Loss: 0.00651\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 160/2000... Step: 160... Loss: 0.00459... Val Loss: 0.00614\n",
      "Epoch: 161/2000... Step: 161... Loss: 0.00438... Val Loss: 0.00591\n",
      "Epoch: 162/2000... Step: 162... Loss: 0.00418... Val Loss: 0.00561\n",
      "Epoch: 163/2000... Step: 163... Loss: 0.00396... Val Loss: 0.00523\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 164/2000... Step: 164... Loss: 0.00374... Val Loss: 0.00536\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 165/2000... Step: 165... Loss: 0.00362... Val Loss: 0.00529\n",
      "Epoch: 166/2000... Step: 166... Loss: 0.00358... Val Loss: 0.00516\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 167/2000... Step: 167... Loss: 0.00363... Val Loss: 0.00539\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 168/2000... Step: 168... Loss: 0.00369... Val Loss: 0.00564\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 169/2000... Step: 169... Loss: 0.00382... Val Loss: 0.00569\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 170/2000... Step: 170... Loss: 0.00413... Val Loss: 0.00670\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 171/2000... Step: 171... Loss: 0.00501... Val Loss: 0.00799\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 172/2000... Step: 172... Loss: 0.00675... Val Loss: 0.01150\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 173/2000... Step: 173... Loss: 0.00946... Val Loss: 0.01405\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 174/2000... Step: 174... Loss: 0.01264... Val Loss: 0.01597\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 175/2000... Step: 175... Loss: 0.01438... Val Loss: 0.01257\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 176/2000... Step: 176... Loss: 0.01033... Val Loss: 0.00643\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 177/2000... Step: 177... Loss: 0.00475... Val Loss: 0.00561\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 178/2000... Step: 178... Loss: 0.00401... Val Loss: 0.00936\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 179/2000... Step: 179... Loss: 0.00745... Val Loss: 0.00830\n",
      "Epoch: 180/2000... Step: 180... Loss: 0.00745... Val Loss: 0.00493\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 181/2000... Step: 181... Loss: 0.00344... Val Loss: 0.00609\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 182/2000... Step: 182... Loss: 0.00453... Val Loss: 0.00781\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 183/2000... Step: 183... Loss: 0.00722... Val Loss: 0.00603\n",
      "Epoch: 184/2000... Step: 184... Loss: 0.00486... Val Loss: 0.00432\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 185/2000... Step: 185... Loss: 0.00289... Val Loss: 0.00690\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 186/2000... Step: 186... Loss: 0.00520... Val Loss: 0.00707\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 187/2000... Step: 187... Loss: 0.00526... Val Loss: 0.00441\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 188/2000... Step: 188... Loss: 0.00311... Val Loss: 0.00511\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 189/2000... Step: 189... Loss: 0.00330... Val Loss: 0.00666\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 190/2000... Step: 190... Loss: 0.00467... Val Loss: 0.00520\n",
      "Epoch: 191/2000... Step: 191... Loss: 0.00410... Val Loss: 0.00393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 192/2000... Step: 192... Loss: 0.00243... Val Loss: 0.00604\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 193/2000... Step: 193... Loss: 0.00371... Val Loss: 0.00658\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 194/2000... Step: 194... Loss: 0.00541... Val Loss: 0.00768\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 195/2000... Step: 195... Loss: 0.00546... Val Loss: 0.00450\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 196/2000... Step: 196... Loss: 0.00249... Val Loss: 0.00559\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 197/2000... Step: 197... Loss: 0.00328... Val Loss: 0.00644\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 198/2000... Step: 198... Loss: 0.00420... Val Loss: 0.00533\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 199/2000... Step: 199... Loss: 0.00369... Val Loss: 0.00463\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 200/2000... Step: 200... Loss: 0.00254... Val Loss: 0.00473\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 201/2000... Step: 201... Loss: 0.00259... Val Loss: 0.00545\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 202/2000... Step: 202... Loss: 0.00379... Val Loss: 0.00441\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 203/2000... Step: 203... Loss: 0.00251... Val Loss: 0.00449\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 204/2000... Step: 204... Loss: 0.00234... Val Loss: 0.00436\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 205/2000... Step: 205... Loss: 0.00248... Val Loss: 0.00513\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 206/2000... Step: 206... Loss: 0.00334... Val Loss: 0.00525\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 207/2000... Step: 207... Loss: 0.00291... Val Loss: 0.00574\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 208/2000... Step: 208... Loss: 0.00458... Val Loss: 0.00746\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 209/2000... Step: 209... Loss: 0.00492... Val Loss: 0.00520\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 210/2000... Step: 210... Loss: 0.00428... Val Loss: 0.00447\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 211/2000... Step: 211... Loss: 0.00205... Val Loss: 0.00502\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 212/2000... Step: 212... Loss: 0.00267... Val Loss: 0.00537\n",
      "Epoch: 213/2000... Step: 213... Loss: 0.00431... Val Loss: 0.00390\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 214/2000... Step: 214... Loss: 0.00210... Val Loss: 0.00464\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 215/2000... Step: 215... Loss: 0.00242... Val Loss: 0.00535\n",
      "Epoch: 216/2000... Step: 216... Loss: 0.00360... Val Loss: 0.00365\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 217/2000... Step: 217... Loss: 0.00200... Val Loss: 0.00443\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 218/2000... Step: 218... Loss: 0.00250... Val Loss: 0.00482\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 219/2000... Step: 219... Loss: 0.00288... Val Loss: 0.00371\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 220/2000... Step: 220... Loss: 0.00199... Val Loss: 0.00453\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 221/2000... Step: 221... Loss: 0.00244... Val Loss: 0.00459\n",
      "Epoch: 222/2000... Step: 222... Loss: 0.00261... Val Loss: 0.00314\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 223/2000... Step: 223... Loss: 0.00165... Val Loss: 0.00437\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 224/2000... Step: 224... Loss: 0.00236... Val Loss: 0.00438\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 225/2000... Step: 225... Loss: 0.00213... Val Loss: 0.00317\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 226/2000... Step: 226... Loss: 0.00157... Val Loss: 0.00413\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 227/2000... Step: 227... Loss: 0.00245... Val Loss: 0.00372\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 228/2000... Step: 228... Loss: 0.00166... Val Loss: 0.00372\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 229/2000... Step: 229... Loss: 0.00162... Val Loss: 0.00354\n",
      "Epoch: 230/2000... Step: 230... Loss: 0.00195... Val Loss: 0.00299\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 231/2000... Step: 231... Loss: 0.00127... Val Loss: 0.00380\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 232/2000... Step: 232... Loss: 0.00161... Val Loss: 0.00339\n",
      "Epoch: 233/2000... Step: 233... Loss: 0.00147... Val Loss: 0.00281\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 234/2000... Step: 234... Loss: 0.00128... Val Loss: 0.00336\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 235/2000... Step: 235... Loss: 0.00145... Val Loss: 0.00339\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 236/2000... Step: 236... Loss: 0.00134... Val Loss: 0.00290\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 237/2000... Step: 237... Loss: 0.00136... Val Loss: 0.00313\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 238/2000... Step: 238... Loss: 0.00139... Val Loss: 0.00348\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 239/2000... Step: 239... Loss: 0.00148... Val Loss: 0.00318\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 240/2000... Step: 240... Loss: 0.00152... Val Loss: 0.00326\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 241/2000... Step: 241... Loss: 0.00181... Val Loss: 0.00414\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 242/2000... Step: 242... Loss: 0.00224... Val Loss: 0.00458\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 243/2000... Step: 243... Loss: 0.00294... Val Loss: 0.00595\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 244/2000... Step: 244... Loss: 0.00418... Val Loss: 0.00805\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 245/2000... Step: 245... Loss: 0.00615... Val Loss: 0.00911\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 246/2000... Step: 246... Loss: 0.00749... Val Loss: 0.01057\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 247/2000... Step: 247... Loss: 0.00894... Val Loss: 0.00737\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 248/2000... Step: 248... Loss: 0.00549... Val Loss: 0.00393\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 249/2000... Step: 249... Loss: 0.00221... Val Loss: 0.00297\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 250/2000... Step: 250... Loss: 0.00137... Val Loss: 0.00458\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 251/2000... Step: 251... Loss: 0.00285... Val Loss: 0.00526\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 252/2000... Step: 252... Loss: 0.00382... Val Loss: 0.00437\n",
      "Epoch: 253/2000... Step: 253... Loss: 0.00249... Val Loss: 0.00258\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 254/2000... Step: 254... Loss: 0.00126... Val Loss: 0.00280\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 255/2000... Step: 255... Loss: 0.00149... Val Loss: 0.00434\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 256/2000... Step: 256... Loss: 0.00235... Val Loss: 0.00432\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 257/2000... Step: 257... Loss: 0.00267... Val Loss: 0.00345\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 258/2000... Step: 258... Loss: 0.00191... Val Loss: 0.00284\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 259/2000... Step: 259... Loss: 0.00117... Val Loss: 0.00296\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 260/2000... Step: 260... Loss: 0.00122... Val Loss: 0.00359\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 261/2000... Step: 261... Loss: 0.00180... Val Loss: 0.00366\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 262/2000... Step: 262... Loss: 0.00188... Val Loss: 0.00309\n",
      "Epoch: 263/2000... Step: 263... Loss: 0.00144... Val Loss: 0.00257\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 264/2000... Step: 264... Loss: 0.00098... Val Loss: 0.00285\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 265/2000... Step: 265... Loss: 0.00105... Val Loss: 0.00310\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 266/2000... Step: 266... Loss: 0.00139... Val Loss: 0.00308\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 267/2000... Step: 267... Loss: 0.00149... Val Loss: 0.00306\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 268/2000... Step: 268... Loss: 0.00129... Val Loss: 0.00283\n",
      "Epoch: 269/2000... Step: 269... Loss: 0.00104... Val Loss: 0.00256\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 270/2000... Step: 270... Loss: 0.00089... Val Loss: 0.00257\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 271/2000... Step: 271... Loss: 0.00094... Val Loss: 0.00285\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 272/2000... Step: 272... Loss: 0.00109... Val Loss: 0.00296\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 273/2000... Step: 273... Loss: 0.00120... Val Loss: 0.00285\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 274/2000... Step: 274... Loss: 0.00117... Val Loss: 0.00274\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 275/2000... Step: 275... Loss: 0.00102... Val Loss: 0.00258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 276/2000... Step: 276... Loss: 0.00089... Val Loss: 0.00251\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 277/2000... Step: 277... Loss: 0.00082... Val Loss: 0.00257\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 278/2000... Step: 278... Loss: 0.00083... Val Loss: 0.00259\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 279/2000... Step: 279... Loss: 0.00088... Val Loss: 0.00267\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 280/2000... Step: 280... Loss: 0.00094... Val Loss: 0.00281\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 281/2000... Step: 281... Loss: 0.00100... Val Loss: 0.00275\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 282/2000... Step: 282... Loss: 0.00105... Val Loss: 0.00289\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 283/2000... Step: 283... Loss: 0.00109... Val Loss: 0.00285\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 284/2000... Step: 284... Loss: 0.00108... Val Loss: 0.00281\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 285/2000... Step: 285... Loss: 0.00105... Val Loss: 0.00272\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 286/2000... Step: 286... Loss: 0.00101... Val Loss: 0.00269\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 287/2000... Step: 287... Loss: 0.00097... Val Loss: 0.00263\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 288/2000... Step: 288... Loss: 0.00093... Val Loss: 0.00257\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 289/2000... Step: 289... Loss: 0.00088... Val Loss: 0.00252\n",
      "Epoch: 290/2000... Step: 290... Loss: 0.00083... Val Loss: 0.00246\n",
      "Epoch: 291/2000... Step: 291... Loss: 0.00079... Val Loss: 0.00244\n",
      "Epoch: 292/2000... Step: 292... Loss: 0.00076... Val Loss: 0.00240\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 293/2000... Step: 293... Loss: 0.00074... Val Loss: 0.00240\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 294/2000... Step: 294... Loss: 0.00073... Val Loss: 0.00240\n",
      "Epoch: 295/2000... Step: 295... Loss: 0.00072... Val Loss: 0.00237\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 296/2000... Step: 296... Loss: 0.00071... Val Loss: 0.00238\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 297/2000... Step: 297... Loss: 0.00071... Val Loss: 0.00242\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 298/2000... Step: 298... Loss: 0.00071... Val Loss: 0.00242\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 299/2000... Step: 299... Loss: 0.00073... Val Loss: 0.00244\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 300/2000... Step: 300... Loss: 0.00077... Val Loss: 0.00257\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 301/2000... Step: 301... Loss: 0.00086... Val Loss: 0.00272\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 302/2000... Step: 302... Loss: 0.00104... Val Loss: 0.00315\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 303/2000... Step: 303... Loss: 0.00145... Val Loss: 0.00392\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 304/2000... Step: 304... Loss: 0.00228... Val Loss: 0.00576\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 305/2000... Step: 305... Loss: 0.00394... Val Loss: 0.00797\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 306/2000... Step: 306... Loss: 0.00647... Val Loss: 0.01284\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 307/2000... Step: 307... Loss: 0.01063... Val Loss: 0.01376\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 308/2000... Step: 308... Loss: 0.01183... Val Loss: 0.01228\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 309/2000... Step: 309... Loss: 0.01019... Val Loss: 0.00449\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 310/2000... Step: 310... Loss: 0.00283... Val Loss: 0.00423\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 311/2000... Step: 311... Loss: 0.00243... Val Loss: 0.00798\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 312/2000... Step: 312... Loss: 0.00682... Val Loss: 0.00618\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 313/2000... Step: 313... Loss: 0.00453... Val Loss: 0.00276\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 314/2000... Step: 314... Loss: 0.00126... Val Loss: 0.00414\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 315/2000... Step: 315... Loss: 0.00301... Val Loss: 0.00623\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 316/2000... Step: 316... Loss: 0.00455... Val Loss: 0.00375\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 317/2000... Step: 317... Loss: 0.00248... Val Loss: 0.00261\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 318/2000... Step: 318... Loss: 0.00118... Val Loss: 0.00485\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 319/2000... Step: 319... Loss: 0.00307... Val Loss: 0.00499\n",
      "EarlyStopping counter: 25 out of 30\n",
      "Epoch: 320/2000... Step: 320... Loss: 0.00356... Val Loss: 0.00292\n",
      "EarlyStopping counter: 26 out of 30\n",
      "Epoch: 321/2000... Step: 321... Loss: 0.00136... Val Loss: 0.00344\n",
      "EarlyStopping counter: 27 out of 30\n",
      "Epoch: 322/2000... Step: 322... Loss: 0.00167... Val Loss: 0.00445\n",
      "EarlyStopping counter: 28 out of 30\n",
      "Epoch: 323/2000... Step: 323... Loss: 0.00298... Val Loss: 0.00309\n",
      "EarlyStopping counter: 29 out of 30\n",
      "Epoch: 324/2000... Step: 324... Loss: 0.00175... Val Loss: 0.00286\n",
      "EarlyStopping counter: 30 out of 30\n",
      "Epoch: 325/2000... Step: 325... Loss: 0.00115... Val Loss: 0.00314\n",
      "EarlyStopping counter: 31 out of 30\n",
      "Epoch: 326/2000... Step: 326... Loss: 0.00189... Val Loss: 0.00294\n",
      "EarlyStopping counter: 32 out of 30\n",
      "Epoch: 327/2000... Step: 327... Loss: 0.00184... Val Loss: 0.00322\n",
      "Epoch: 328/2000... Step: 328... Loss: 0.00123... Val Loss: 0.00219\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 329/2000... Step: 329... Loss: 0.00102... Val Loss: 0.00242\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 330/2000... Step: 330... Loss: 0.00140... Val Loss: 0.00369\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 331/2000... Step: 331... Loss: 0.00149... Val Loss: 0.00227\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 332/2000... Step: 332... Loss: 0.00098... Val Loss: 0.00220\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 333/2000... Step: 333... Loss: 0.00088... Val Loss: 0.00317\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 334/2000... Step: 334... Loss: 0.00123... Val Loss: 0.00243\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 335/2000... Step: 335... Loss: 0.00116... Val Loss: 0.00225\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 336/2000... Step: 336... Loss: 0.00074... Val Loss: 0.00259\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 337/2000... Step: 337... Loss: 0.00085... Val Loss: 0.00229\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 338/2000... Step: 338... Loss: 0.00102... Val Loss: 0.00249\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 339/2000... Step: 339... Loss: 0.00086... Val Loss: 0.00239\n",
      "Epoch: 340/2000... Step: 340... Loss: 0.00076... Val Loss: 0.00206\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 341/2000... Step: 341... Loss: 0.00078... Val Loss: 0.00246\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 342/2000... Step: 342... Loss: 0.00079... Val Loss: 0.00244\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 343/2000... Step: 343... Loss: 0.00084... Val Loss: 0.00207\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 344/2000... Step: 344... Loss: 0.00078... Val Loss: 0.00225\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 345/2000... Step: 345... Loss: 0.00066... Val Loss: 0.00237\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 346/2000... Step: 346... Loss: 0.00070... Val Loss: 0.00219\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 347/2000... Step: 347... Loss: 0.00079... Val Loss: 0.00229\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 348/2000... Step: 348... Loss: 0.00071... Val Loss: 0.00229\n",
      "Epoch: 349/2000... Step: 349... Loss: 0.00065... Val Loss: 0.00200\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 350/2000... Step: 350... Loss: 0.00066... Val Loss: 0.00218\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 351/2000... Step: 351... Loss: 0.00065... Val Loss: 0.00223\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 352/2000... Step: 352... Loss: 0.00066... Val Loss: 0.00205\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 353/2000... Step: 353... Loss: 0.00070... Val Loss: 0.00236\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 354/2000... Step: 354... Loss: 0.00068... Val Loss: 0.00213\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 355/2000... Step: 355... Loss: 0.00064... Val Loss: 0.00213\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 356/2000... Step: 356... Loss: 0.00068... Val Loss: 0.00240\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 357/2000... Step: 357... Loss: 0.00076... Val Loss: 0.00229\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 358/2000... Step: 358... Loss: 0.00086... Val Loss: 0.00247\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 359/2000... Step: 359... Loss: 0.00094... Val Loss: 0.00268\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 360/2000... Step: 360... Loss: 0.00109... Val Loss: 0.00259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 361/2000... Step: 361... Loss: 0.00115... Val Loss: 0.00273\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 362/2000... Step: 362... Loss: 0.00123... Val Loss: 0.00270\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 363/2000... Step: 363... Loss: 0.00118... Val Loss: 0.00251\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 364/2000... Step: 364... Loss: 0.00114... Val Loss: 0.00245\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 365/2000... Step: 365... Loss: 0.00090... Val Loss: 0.00236\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 366/2000... Step: 366... Loss: 0.00072... Val Loss: 0.00210\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 367/2000... Step: 367... Loss: 0.00061... Val Loss: 0.00216\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 368/2000... Step: 368... Loss: 0.00062... Val Loss: 0.00226\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 369/2000... Step: 369... Loss: 0.00070... Val Loss: 0.00213\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 370/2000... Step: 370... Loss: 0.00074... Val Loss: 0.00241\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 371/2000... Step: 371... Loss: 0.00075... Val Loss: 0.00214\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 372/2000... Step: 372... Loss: 0.00071... Val Loss: 0.00220\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 373/2000... Step: 373... Loss: 0.00073... Val Loss: 0.00252\n",
      "EarlyStopping counter: 25 out of 30\n",
      "Epoch: 374/2000... Step: 374... Loss: 0.00080... Val Loss: 0.00209\n",
      "EarlyStopping counter: 26 out of 30\n",
      "Epoch: 375/2000... Step: 375... Loss: 0.00083... Val Loss: 0.00245\n",
      "EarlyStopping counter: 27 out of 30\n",
      "Epoch: 376/2000... Step: 376... Loss: 0.00077... Val Loss: 0.00210\n",
      "EarlyStopping counter: 28 out of 30\n",
      "Epoch: 377/2000... Step: 377... Loss: 0.00068... Val Loss: 0.00204\n",
      "EarlyStopping counter: 29 out of 30\n",
      "Epoch: 378/2000... Step: 378... Loss: 0.00063... Val Loss: 0.00216\n",
      "EarlyStopping counter: 30 out of 30\n",
      "Epoch: 379/2000... Step: 379... Loss: 0.00059... Val Loss: 0.00205\n",
      "EarlyStopping counter: 31 out of 30\n",
      "Epoch: 380/2000... Step: 380... Loss: 0.00057... Val Loss: 0.00207\n",
      "EarlyStopping counter: 32 out of 30\n",
      "Epoch: 381/2000... Step: 381... Loss: 0.00059... Val Loss: 0.00216\n",
      "EarlyStopping counter: 33 out of 30\n",
      "Epoch: 382/2000... Step: 382... Loss: 0.00061... Val Loss: 0.00203\n",
      "EarlyStopping counter: 34 out of 30\n",
      "Epoch: 383/2000... Step: 383... Loss: 0.00060... Val Loss: 0.00202\n",
      "EarlyStopping counter: 35 out of 30\n",
      "Epoch: 384/2000... Step: 384... Loss: 0.00053... Val Loss: 0.00205\n",
      "EarlyStopping counter: 36 out of 30\n",
      "Epoch: 385/2000... Step: 385... Loss: 0.00051... Val Loss: 0.00200\n",
      "EarlyStopping counter: 37 out of 30\n",
      "Epoch: 386/2000... Step: 386... Loss: 0.00053... Val Loss: 0.00206\n",
      "EarlyStopping counter: 38 out of 30\n",
      "Epoch: 387/2000... Step: 387... Loss: 0.00055... Val Loss: 0.00216\n",
      "Epoch: 388/2000... Step: 388... Loss: 0.00058... Val Loss: 0.00197\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 389/2000... Step: 389... Loss: 0.00059... Val Loss: 0.00229\n",
      "Epoch: 390/2000... Step: 390... Loss: 0.00059... Val Loss: 0.00195\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 391/2000... Step: 391... Loss: 0.00059... Val Loss: 0.00218\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 392/2000... Step: 392... Loss: 0.00058... Val Loss: 0.00205\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 393/2000... Step: 393... Loss: 0.00060... Val Loss: 0.00216\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 394/2000... Step: 394... Loss: 0.00067... Val Loss: 0.00233\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 395/2000... Step: 395... Loss: 0.00083... Val Loss: 0.00269\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 396/2000... Step: 396... Loss: 0.00120... Val Loss: 0.00322\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 397/2000... Step: 397... Loss: 0.00199... Val Loss: 0.00511\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 398/2000... Step: 398... Loss: 0.00345... Val Loss: 0.00657\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 399/2000... Step: 399... Loss: 0.00583... Val Loss: 0.01011\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 400/2000... Step: 400... Loss: 0.00820... Val Loss: 0.01005\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 401/2000... Step: 401... Loss: 0.00840... Val Loss: 0.00922\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 402/2000... Step: 402... Loss: 0.00741... Val Loss: 0.00377\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 403/2000... Step: 403... Loss: 0.00247... Val Loss: 0.00357\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 404/2000... Step: 404... Loss: 0.00167... Val Loss: 0.00524\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 405/2000... Step: 405... Loss: 0.00444... Val Loss: 0.00545\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 406/2000... Step: 406... Loss: 0.00391... Val Loss: 0.00314\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 407/2000... Step: 407... Loss: 0.00169... Val Loss: 0.00211\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 408/2000... Step: 408... Loss: 0.00115... Val Loss: 0.00466\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 409/2000... Step: 409... Loss: 0.00278... Val Loss: 0.00463\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 410/2000... Step: 410... Loss: 0.00348... Val Loss: 0.00315\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 411/2000... Step: 411... Loss: 0.00173... Val Loss: 0.00289\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 412/2000... Step: 412... Loss: 0.00097... Val Loss: 0.00334\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 413/2000... Step: 413... Loss: 0.00199... Val Loss: 0.00394\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 414/2000... Step: 414... Loss: 0.00246... Val Loss: 0.00338\n",
      "Epoch: 415/2000... Step: 415... Loss: 0.00152... Val Loss: 0.00186\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 416/2000... Step: 416... Loss: 0.00082... Val Loss: 0.00271\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 417/2000... Step: 417... Loss: 0.00117... Val Loss: 0.00344\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 418/2000... Step: 418... Loss: 0.00179... Val Loss: 0.00248\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 419/2000... Step: 419... Loss: 0.00153... Val Loss: 0.00238\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 420/2000... Step: 420... Loss: 0.00079... Val Loss: 0.00242\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 421/2000... Step: 421... Loss: 0.00073... Val Loss: 0.00242\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 422/2000... Step: 422... Loss: 0.00123... Val Loss: 0.00313\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 423/2000... Step: 423... Loss: 0.00133... Val Loss: 0.00235\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 424/2000... Step: 424... Loss: 0.00089... Val Loss: 0.00214\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 425/2000... Step: 425... Loss: 0.00066... Val Loss: 0.00263\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 426/2000... Step: 426... Loss: 0.00079... Val Loss: 0.00222\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 427/2000... Step: 427... Loss: 0.00096... Val Loss: 0.00247\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 428/2000... Step: 428... Loss: 0.00091... Val Loss: 0.00234\n",
      "Epoch: 429/2000... Step: 429... Loss: 0.00071... Val Loss: 0.00181\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 430/2000... Step: 430... Loss: 0.00056... Val Loss: 0.00225\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 431/2000... Step: 431... Loss: 0.00065... Val Loss: 0.00227\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 432/2000... Step: 432... Loss: 0.00083... Val Loss: 0.00228\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 433/2000... Step: 433... Loss: 0.00083... Val Loss: 0.00210\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 434/2000... Step: 434... Loss: 0.00064... Val Loss: 0.00214\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 435/2000... Step: 435... Loss: 0.00053... Val Loss: 0.00196\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 436/2000... Step: 436... Loss: 0.00056... Val Loss: 0.00210\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 437/2000... Step: 437... Loss: 0.00060... Val Loss: 0.00230\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 438/2000... Step: 438... Loss: 0.00063... Val Loss: 0.00194\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 439/2000... Step: 439... Loss: 0.00066... Val Loss: 0.00240\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 440/2000... Step: 440... Loss: 0.00064... Val Loss: 0.00186\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 441/2000... Step: 441... Loss: 0.00056... Val Loss: 0.00206\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 442/2000... Step: 442... Loss: 0.00046... Val Loss: 0.00204\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 443/2000... Step: 443... Loss: 0.00047... Val Loss: 0.00188\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 444/2000... Step: 444... Loss: 0.00052... Val Loss: 0.00226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 445/2000... Step: 445... Loss: 0.00055... Val Loss: 0.00197\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 446/2000... Step: 446... Loss: 0.00056... Val Loss: 0.00208\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 447/2000... Step: 447... Loss: 0.00058... Val Loss: 0.00212\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 448/2000... Step: 448... Loss: 0.00057... Val Loss: 0.00197\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 449/2000... Step: 449... Loss: 0.00055... Val Loss: 0.00204\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 450/2000... Step: 450... Loss: 0.00053... Val Loss: 0.00199\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 451/2000... Step: 451... Loss: 0.00055... Val Loss: 0.00214\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 452/2000... Step: 452... Loss: 0.00063... Val Loss: 0.00207\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 453/2000... Step: 453... Loss: 0.00070... Val Loss: 0.00246\n",
      "EarlyStopping counter: 25 out of 30\n",
      "Epoch: 454/2000... Step: 454... Loss: 0.00086... Val Loss: 0.00233\n",
      "EarlyStopping counter: 26 out of 30\n",
      "Epoch: 455/2000... Step: 455... Loss: 0.00118... Val Loss: 0.00387\n",
      "EarlyStopping counter: 27 out of 30\n",
      "Epoch: 456/2000... Step: 456... Loss: 0.00182... Val Loss: 0.00313\n",
      "EarlyStopping counter: 28 out of 30\n",
      "Epoch: 457/2000... Step: 457... Loss: 0.00209... Val Loss: 0.00450\n",
      "EarlyStopping counter: 29 out of 30\n",
      "Epoch: 458/2000... Step: 458... Loss: 0.00219... Val Loss: 0.00245\n",
      "EarlyStopping counter: 30 out of 30\n",
      "Epoch: 459/2000... Step: 459... Loss: 0.00144... Val Loss: 0.00245\n",
      "EarlyStopping counter: 31 out of 30\n",
      "Epoch: 460/2000... Step: 460... Loss: 0.00075... Val Loss: 0.00202\n",
      "EarlyStopping counter: 32 out of 30\n",
      "Epoch: 461/2000... Step: 461... Loss: 0.00050... Val Loss: 0.00205\n",
      "EarlyStopping counter: 33 out of 30\n",
      "Epoch: 462/2000... Step: 462... Loss: 0.00097... Val Loss: 0.00339\n",
      "EarlyStopping counter: 34 out of 30\n",
      "Epoch: 463/2000... Step: 463... Loss: 0.00140... Val Loss: 0.00228\n",
      "EarlyStopping counter: 35 out of 30\n",
      "Epoch: 464/2000... Step: 464... Loss: 0.00107... Val Loss: 0.00221\n",
      "EarlyStopping counter: 36 out of 30\n",
      "Epoch: 465/2000... Step: 465... Loss: 0.00061... Val Loss: 0.00245\n",
      "EarlyStopping counter: 37 out of 30\n",
      "Epoch: 466/2000... Step: 466... Loss: 0.00062... Val Loss: 0.00211\n",
      "EarlyStopping counter: 38 out of 30\n",
      "Epoch: 467/2000... Step: 467... Loss: 0.00094... Val Loss: 0.00265\n",
      "EarlyStopping counter: 39 out of 30\n",
      "Epoch: 468/2000... Step: 468... Loss: 0.00090... Val Loss: 0.00196\n",
      "EarlyStopping counter: 40 out of 30\n",
      "Epoch: 469/2000... Step: 469... Loss: 0.00057... Val Loss: 0.00183\n",
      "EarlyStopping counter: 41 out of 30\n",
      "Epoch: 470/2000... Step: 470... Loss: 0.00054... Val Loss: 0.00254\n",
      "EarlyStopping counter: 42 out of 30\n",
      "Epoch: 471/2000... Step: 471... Loss: 0.00078... Val Loss: 0.00207\n",
      "EarlyStopping counter: 43 out of 30\n",
      "Epoch: 472/2000... Step: 472... Loss: 0.00073... Val Loss: 0.00211\n",
      "EarlyStopping counter: 44 out of 30\n",
      "Epoch: 473/2000... Step: 473... Loss: 0.00056... Val Loss: 0.00222\n",
      "EarlyStopping counter: 45 out of 30\n",
      "Epoch: 474/2000... Step: 474... Loss: 0.00048... Val Loss: 0.00198\n",
      "EarlyStopping counter: 46 out of 30\n",
      "Epoch: 475/2000... Step: 475... Loss: 0.00062... Val Loss: 0.00241\n",
      "EarlyStopping counter: 47 out of 30\n",
      "Epoch: 476/2000... Step: 476... Loss: 0.00070... Val Loss: 0.00215\n",
      "EarlyStopping counter: 48 out of 30\n",
      "Epoch: 477/2000... Step: 477... Loss: 0.00064... Val Loss: 0.00210\n",
      "EarlyStopping counter: 49 out of 30\n",
      "Epoch: 478/2000... Step: 478... Loss: 0.00062... Val Loss: 0.00240\n",
      "EarlyStopping counter: 50 out of 30\n",
      "Epoch: 479/2000... Step: 479... Loss: 0.00079... Val Loss: 0.00270\n",
      "EarlyStopping counter: 51 out of 30\n",
      "Epoch: 480/2000... Step: 480... Loss: 0.00115... Val Loss: 0.00303\n",
      "EarlyStopping counter: 52 out of 30\n",
      "Epoch: 481/2000... Step: 481... Loss: 0.00175... Val Loss: 0.00469\n",
      "EarlyStopping counter: 53 out of 30\n",
      "Epoch: 482/2000... Step: 482... Loss: 0.00283... Val Loss: 0.00570\n",
      "EarlyStopping counter: 54 out of 30\n",
      "Epoch: 483/2000... Step: 483... Loss: 0.00461... Val Loss: 0.01009\n",
      "EarlyStopping counter: 55 out of 30\n",
      "Epoch: 484/2000... Step: 484... Loss: 0.00802... Val Loss: 0.01121\n",
      "EarlyStopping counter: 56 out of 30\n",
      "Epoch: 485/2000... Step: 485... Loss: 0.00991... Val Loss: 0.01209\n",
      "EarlyStopping counter: 57 out of 30\n",
      "Epoch: 486/2000... Step: 486... Loss: 0.01028... Val Loss: 0.00525\n",
      "EarlyStopping counter: 58 out of 30\n",
      "Epoch: 487/2000... Step: 487... Loss: 0.00378... Val Loss: 0.00234\n",
      "EarlyStopping counter: 59 out of 30\n",
      "Epoch: 488/2000... Step: 488... Loss: 0.00111... Val Loss: 0.00541\n",
      "EarlyStopping counter: 60 out of 30\n",
      "Epoch: 489/2000... Step: 489... Loss: 0.00421... Val Loss: 0.00617\n",
      "EarlyStopping counter: 61 out of 30\n",
      "Epoch: 490/2000... Step: 490... Loss: 0.00465... Val Loss: 0.00283\n",
      "EarlyStopping counter: 62 out of 30\n",
      "Epoch: 491/2000... Step: 491... Loss: 0.00194... Val Loss: 0.00282\n",
      "EarlyStopping counter: 63 out of 30\n",
      "Epoch: 492/2000... Step: 492... Loss: 0.00125... Val Loss: 0.00451\n",
      "EarlyStopping counter: 64 out of 30\n",
      "Epoch: 493/2000... Step: 493... Loss: 0.00330... Val Loss: 0.00482\n",
      "EarlyStopping counter: 65 out of 30\n",
      "Epoch: 494/2000... Step: 494... Loss: 0.00364... Val Loss: 0.00361\n",
      "EarlyStopping counter: 66 out of 30\n",
      "Epoch: 495/2000... Step: 495... Loss: 0.00156... Val Loss: 0.00263\n",
      "EarlyStopping counter: 67 out of 30\n",
      "Epoch: 496/2000... Step: 496... Loss: 0.00152... Val Loss: 0.00428\n",
      "EarlyStopping counter: 68 out of 30\n",
      "Epoch: 497/2000... Step: 497... Loss: 0.00269... Val Loss: 0.00360\n",
      "EarlyStopping counter: 69 out of 30\n",
      "Epoch: 498/2000... Step: 498... Loss: 0.00183... Val Loss: 0.00218\n",
      "EarlyStopping counter: 70 out of 30\n",
      "Epoch: 499/2000... Step: 499... Loss: 0.00100... Val Loss: 0.00321\n",
      "EarlyStopping counter: 71 out of 30\n",
      "Epoch: 500/2000... Step: 500... Loss: 0.00166... Val Loss: 0.00330\n",
      "EarlyStopping counter: 72 out of 30\n",
      "Epoch: 501/2000... Step: 501... Loss: 0.00184... Val Loss: 0.00246\n",
      "EarlyStopping counter: 73 out of 30\n",
      "Epoch: 502/2000... Step: 502... Loss: 0.00100... Val Loss: 0.00230\n",
      "EarlyStopping counter: 74 out of 30\n",
      "Epoch: 503/2000... Step: 503... Loss: 0.00084... Val Loss: 0.00295\n",
      "EarlyStopping counter: 75 out of 30\n",
      "Epoch: 504/2000... Step: 504... Loss: 0.00157... Val Loss: 0.00339\n",
      "EarlyStopping counter: 76 out of 30\n",
      "Epoch: 505/2000... Step: 505... Loss: 0.00156... Val Loss: 0.00204\n",
      "EarlyStopping counter: 77 out of 30\n",
      "Epoch: 506/2000... Step: 506... Loss: 0.00078... Val Loss: 0.00217\n",
      "EarlyStopping counter: 78 out of 30\n",
      "Epoch: 507/2000... Step: 507... Loss: 0.00066... Val Loss: 0.00285\n",
      "EarlyStopping counter: 79 out of 30\n",
      "Epoch: 508/2000... Step: 508... Loss: 0.00114... Val Loss: 0.00254\n",
      "EarlyStopping counter: 80 out of 30\n",
      "Epoch: 509/2000... Step: 509... Loss: 0.00115... Val Loss: 0.00251\n",
      "EarlyStopping counter: 81 out of 30\n",
      "Epoch: 510/2000... Step: 510... Loss: 0.00071... Val Loss: 0.00210\n",
      "EarlyStopping counter: 82 out of 30\n",
      "Epoch: 511/2000... Step: 511... Loss: 0.00063... Val Loss: 0.00236\n",
      "EarlyStopping counter: 83 out of 30\n",
      "Epoch: 512/2000... Step: 512... Loss: 0.00082... Val Loss: 0.00248\n",
      "EarlyStopping counter: 84 out of 30\n",
      "Epoch: 513/2000... Step: 513... Loss: 0.00081... Val Loss: 0.00185\n",
      "EarlyStopping counter: 85 out of 30\n",
      "Epoch: 514/2000... Step: 514... Loss: 0.00062... Val Loss: 0.00224\n",
      "EarlyStopping counter: 86 out of 30\n",
      "Epoch: 515/2000... Step: 515... Loss: 0.00059... Val Loss: 0.00226\n",
      "EarlyStopping counter: 87 out of 30\n",
      "Epoch: 516/2000... Step: 516... Loss: 0.00068... Val Loss: 0.00202\n",
      "EarlyStopping counter: 88 out of 30\n",
      "Epoch: 517/2000... Step: 517... Loss: 0.00071... Val Loss: 0.00254\n",
      "EarlyStopping counter: 89 out of 30\n",
      "Epoch: 518/2000... Step: 518... Loss: 0.00062... Val Loss: 0.00193\n",
      "EarlyStopping counter: 90 out of 30\n",
      "Epoch: 519/2000... Step: 519... Loss: 0.00052... Val Loss: 0.00211\n",
      "EarlyStopping counter: 91 out of 30\n",
      "Epoch: 520/2000... Step: 520... Loss: 0.00051... Val Loss: 0.00236\n",
      "EarlyStopping counter: 92 out of 30\n",
      "Epoch: 521/2000... Step: 521... Loss: 0.00058... Val Loss: 0.00184\n",
      "EarlyStopping counter: 93 out of 30\n",
      "Epoch: 522/2000... Step: 522... Loss: 0.00060... Val Loss: 0.00225\n",
      "EarlyStopping counter: 94 out of 30\n",
      "Epoch: 523/2000... Step: 523... Loss: 0.00051... Val Loss: 0.00193\n",
      "EarlyStopping counter: 95 out of 30\n",
      "Epoch: 524/2000... Step: 524... Loss: 0.00046... Val Loss: 0.00185\n",
      "EarlyStopping counter: 96 out of 30\n",
      "Epoch: 525/2000... Step: 525... Loss: 0.00050... Val Loss: 0.00232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 97 out of 30\n",
      "Epoch: 526/2000... Step: 526... Loss: 0.00053... Val Loss: 0.00185\n",
      "EarlyStopping counter: 98 out of 30\n",
      "Epoch: 527/2000... Step: 527... Loss: 0.00046... Val Loss: 0.00195\n",
      "EarlyStopping counter: 99 out of 30\n",
      "Epoch: 528/2000... Step: 528... Loss: 0.00040... Val Loss: 0.00209\n",
      "EarlyStopping counter: 100 out of 30\n",
      "Epoch: 529/2000... Step: 529... Loss: 0.00044... Val Loss: 0.00185\n",
      "EarlyStopping counter: 101 out of 30\n",
      "Epoch: 530/2000... Step: 530... Loss: 0.00051... Val Loss: 0.00218\n",
      "EarlyStopping counter: 102 out of 30\n",
      "Epoch: 531/2000... Step: 531... Loss: 0.00049... Val Loss: 0.00191\n",
      "EarlyStopping counter: 103 out of 30\n",
      "Epoch: 532/2000... Step: 532... Loss: 0.00044... Val Loss: 0.00194\n",
      "EarlyStopping counter: 104 out of 30\n",
      "Epoch: 533/2000... Step: 533... Loss: 0.00043... Val Loss: 0.00218\n",
      "EarlyStopping counter: 105 out of 30\n",
      "Epoch: 534/2000... Step: 534... Loss: 0.00044... Val Loss: 0.00187\n",
      "EarlyStopping counter: 106 out of 30\n",
      "Epoch: 535/2000... Step: 535... Loss: 0.00044... Val Loss: 0.00206\n",
      "EarlyStopping counter: 107 out of 30\n",
      "Epoch: 536/2000... Step: 536... Loss: 0.00042... Val Loss: 0.00198\n",
      "EarlyStopping counter: 108 out of 30\n",
      "Epoch: 537/2000... Step: 537... Loss: 0.00042... Val Loss: 0.00186\n",
      "EarlyStopping counter: 109 out of 30\n",
      "Epoch: 538/2000... Step: 538... Loss: 0.00041... Val Loss: 0.00206\n",
      "EarlyStopping counter: 110 out of 30\n",
      "Epoch: 539/2000... Step: 539... Loss: 0.00039... Val Loss: 0.00194\n",
      "EarlyStopping counter: 111 out of 30\n",
      "Epoch: 540/2000... Step: 540... Loss: 0.00039... Val Loss: 0.00201\n",
      "EarlyStopping counter: 112 out of 30\n",
      "Epoch: 541/2000... Step: 541... Loss: 0.00040... Val Loss: 0.00207\n",
      "EarlyStopping counter: 113 out of 30\n",
      "Epoch: 542/2000... Step: 542... Loss: 0.00042... Val Loss: 0.00196\n",
      "EarlyStopping counter: 114 out of 30\n",
      "Epoch: 543/2000... Step: 543... Loss: 0.00043... Val Loss: 0.00202\n",
      "EarlyStopping counter: 115 out of 30\n",
      "Epoch: 544/2000... Step: 544... Loss: 0.00042... Val Loss: 0.00200\n",
      "EarlyStopping counter: 116 out of 30\n",
      "Epoch: 545/2000... Step: 545... Loss: 0.00042... Val Loss: 0.00200\n",
      "EarlyStopping counter: 117 out of 30\n",
      "Epoch: 546/2000... Step: 546... Loss: 0.00045... Val Loss: 0.00214\n",
      "EarlyStopping counter: 118 out of 30\n",
      "Epoch: 547/2000... Step: 547... Loss: 0.00049... Val Loss: 0.00205\n",
      "EarlyStopping counter: 119 out of 30\n",
      "Epoch: 548/2000... Step: 548... Loss: 0.00055... Val Loss: 0.00222\n",
      "EarlyStopping counter: 120 out of 30\n",
      "Epoch: 549/2000... Step: 549... Loss: 0.00060... Val Loss: 0.00222\n",
      "EarlyStopping counter: 121 out of 30\n",
      "Epoch: 550/2000... Step: 550... Loss: 0.00072... Val Loss: 0.00241\n",
      "EarlyStopping counter: 122 out of 30\n",
      "Epoch: 551/2000... Step: 551... Loss: 0.00086... Val Loss: 0.00258\n",
      "EarlyStopping counter: 123 out of 30\n",
      "Epoch: 552/2000... Step: 552... Loss: 0.00110... Val Loss: 0.00293\n",
      "EarlyStopping counter: 124 out of 30\n",
      "Epoch: 553/2000... Step: 553... Loss: 0.00136... Val Loss: 0.00311\n",
      "EarlyStopping counter: 125 out of 30\n",
      "Epoch: 554/2000... Step: 554... Loss: 0.00166... Val Loss: 0.00389\n",
      "EarlyStopping counter: 126 out of 30\n",
      "Epoch: 555/2000... Step: 555... Loss: 0.00205... Val Loss: 0.00351\n",
      "EarlyStopping counter: 127 out of 30\n",
      "Epoch: 556/2000... Step: 556... Loss: 0.00234... Val Loss: 0.00431\n",
      "EarlyStopping counter: 128 out of 30\n",
      "Epoch: 557/2000... Step: 557... Loss: 0.00229... Val Loss: 0.00307\n",
      "EarlyStopping counter: 129 out of 30\n",
      "Epoch: 558/2000... Step: 558... Loss: 0.00190... Val Loss: 0.00299\n",
      "EarlyStopping counter: 130 out of 30\n",
      "Epoch: 559/2000... Step: 559... Loss: 0.00120... Val Loss: 0.00230\n",
      "EarlyStopping counter: 131 out of 30\n",
      "Epoch: 560/2000... Step: 560... Loss: 0.00066... Val Loss: 0.00189\n",
      "EarlyStopping counter: 132 out of 30\n",
      "Epoch: 561/2000... Step: 561... Loss: 0.00055... Val Loss: 0.00232\n",
      "EarlyStopping counter: 133 out of 30\n",
      "Epoch: 562/2000... Step: 562... Loss: 0.00065... Val Loss: 0.00247\n",
      "EarlyStopping counter: 134 out of 30\n",
      "Epoch: 563/2000... Step: 563... Loss: 0.00095... Val Loss: 0.00290\n",
      "EarlyStopping counter: 135 out of 30\n",
      "Epoch: 564/2000... Step: 564... Loss: 0.00128... Val Loss: 0.00281\n",
      "EarlyStopping counter: 136 out of 30\n",
      "Epoch: 565/2000... Step: 565... Loss: 0.00124... Val Loss: 0.00261\n",
      "EarlyStopping counter: 137 out of 30\n",
      "Epoch: 566/2000... Step: 566... Loss: 0.00097... Val Loss: 0.00219\n",
      "EarlyStopping counter: 138 out of 30\n",
      "Epoch: 567/2000... Step: 567... Loss: 0.00062... Val Loss: 0.00208\n",
      "EarlyStopping counter: 139 out of 30\n",
      "Epoch: 568/2000... Step: 568... Loss: 0.00045... Val Loss: 0.00195\n",
      "EarlyStopping counter: 140 out of 30\n",
      "Epoch: 569/2000... Step: 569... Loss: 0.00046... Val Loss: 0.00224\n",
      "EarlyStopping counter: 141 out of 30\n",
      "Epoch: 570/2000... Step: 570... Loss: 0.00059... Val Loss: 0.00230\n",
      "EarlyStopping counter: 142 out of 30\n",
      "Epoch: 571/2000... Step: 571... Loss: 0.00066... Val Loss: 0.00218\n",
      "EarlyStopping counter: 143 out of 30\n",
      "Epoch: 572/2000... Step: 572... Loss: 0.00070... Val Loss: 0.00258\n",
      "EarlyStopping counter: 144 out of 30\n",
      "Epoch: 573/2000... Step: 573... Loss: 0.00075... Val Loss: 0.00217\n",
      "EarlyStopping counter: 145 out of 30\n",
      "Epoch: 574/2000... Step: 574... Loss: 0.00078... Val Loss: 0.00259\n",
      "EarlyStopping counter: 146 out of 30\n",
      "Epoch: 575/2000... Step: 575... Loss: 0.00073... Val Loss: 0.00196\n",
      "EarlyStopping counter: 147 out of 30\n",
      "Epoch: 576/2000... Step: 576... Loss: 0.00063... Val Loss: 0.00227\n",
      "EarlyStopping counter: 148 out of 30\n",
      "Epoch: 577/2000... Step: 577... Loss: 0.00052... Val Loss: 0.00204\n",
      "EarlyStopping counter: 149 out of 30\n",
      "Epoch: 578/2000... Step: 578... Loss: 0.00046... Val Loss: 0.00204\n",
      "EarlyStopping counter: 150 out of 30\n",
      "Epoch: 579/2000... Step: 579... Loss: 0.00045... Val Loss: 0.00211\n",
      "EarlyStopping counter: 151 out of 30\n",
      "Epoch: 580/2000... Step: 580... Loss: 0.00044... Val Loss: 0.00197\n",
      "EarlyStopping counter: 152 out of 30\n",
      "Epoch: 581/2000... Step: 581... Loss: 0.00042... Val Loss: 0.00201\n",
      "EarlyStopping counter: 153 out of 30\n",
      "Epoch: 582/2000... Step: 582... Loss: 0.00042... Val Loss: 0.00208\n",
      "EarlyStopping counter: 154 out of 30\n",
      "Epoch: 583/2000... Step: 583... Loss: 0.00046... Val Loss: 0.00201\n",
      "EarlyStopping counter: 155 out of 30\n",
      "Epoch: 584/2000... Step: 584... Loss: 0.00053... Val Loss: 0.00233\n",
      "EarlyStopping counter: 156 out of 30\n",
      "Epoch: 585/2000... Step: 585... Loss: 0.00059... Val Loss: 0.00213\n",
      "EarlyStopping counter: 157 out of 30\n",
      "Epoch: 586/2000... Step: 586... Loss: 0.00064... Val Loss: 0.00241\n",
      "EarlyStopping counter: 158 out of 30\n",
      "Epoch: 587/2000... Step: 587... Loss: 0.00063... Val Loss: 0.00207\n",
      "EarlyStopping counter: 159 out of 30\n",
      "Epoch: 588/2000... Step: 588... Loss: 0.00061... Val Loss: 0.00219\n",
      "EarlyStopping counter: 160 out of 30\n",
      "Epoch: 589/2000... Step: 589... Loss: 0.00061... Val Loss: 0.00216\n",
      "EarlyStopping counter: 161 out of 30\n",
      "Epoch: 590/2000... Step: 590... Loss: 0.00064... Val Loss: 0.00218\n",
      "EarlyStopping counter: 162 out of 30\n",
      "Epoch: 591/2000... Step: 591... Loss: 0.00070... Val Loss: 0.00231\n",
      "EarlyStopping counter: 163 out of 30\n",
      "Epoch: 592/2000... Step: 592... Loss: 0.00071... Val Loss: 0.00239\n",
      "EarlyStopping counter: 164 out of 30\n",
      "Epoch: 593/2000... Step: 593... Loss: 0.00076... Val Loss: 0.00238\n",
      "EarlyStopping counter: 165 out of 30\n",
      "Epoch: 594/2000... Step: 594... Loss: 0.00074... Val Loss: 0.00250\n",
      "EarlyStopping counter: 166 out of 30\n",
      "Epoch: 595/2000... Step: 595... Loss: 0.00080... Val Loss: 0.00247\n",
      "EarlyStopping counter: 167 out of 30\n",
      "Epoch: 596/2000... Step: 596... Loss: 0.00084... Val Loss: 0.00251\n",
      "EarlyStopping counter: 168 out of 30\n",
      "Epoch: 597/2000... Step: 597... Loss: 0.00084... Val Loss: 0.00247\n",
      "EarlyStopping counter: 169 out of 30\n",
      "Epoch: 598/2000... Step: 598... Loss: 0.00082... Val Loss: 0.00233\n",
      "EarlyStopping counter: 170 out of 30\n",
      "Epoch: 599/2000... Step: 599... Loss: 0.00079... Val Loss: 0.00261\n",
      "EarlyStopping counter: 171 out of 30\n",
      "Epoch: 600/2000... Step: 600... Loss: 0.00086... Val Loss: 0.00219\n",
      "EarlyStopping counter: 172 out of 30\n",
      "Epoch: 601/2000... Step: 601... Loss: 0.00102... Val Loss: 0.00304\n",
      "EarlyStopping counter: 173 out of 30\n",
      "Epoch: 602/2000... Step: 602... Loss: 0.00127... Val Loss: 0.00234\n",
      "EarlyStopping counter: 174 out of 30\n",
      "Epoch: 603/2000... Step: 603... Loss: 0.00155... Val Loss: 0.00356\n",
      "EarlyStopping counter: 175 out of 30\n",
      "Epoch: 604/2000... Step: 604... Loss: 0.00170... Val Loss: 0.00261\n",
      "EarlyStopping counter: 176 out of 30\n",
      "Epoch: 605/2000... Step: 605... Loss: 0.00178... Val Loss: 0.00374\n",
      "EarlyStopping counter: 177 out of 30\n",
      "Epoch: 606/2000... Step: 606... Loss: 0.00164... Val Loss: 0.00277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 178 out of 30\n",
      "Epoch: 607/2000... Step: 607... Loss: 0.00146... Val Loss: 0.00327\n",
      "EarlyStopping counter: 179 out of 30\n",
      "Epoch: 608/2000... Step: 608... Loss: 0.00130... Val Loss: 0.00276\n",
      "EarlyStopping counter: 180 out of 30\n",
      "Epoch: 609/2000... Step: 609... Loss: 0.00122... Val Loss: 0.00283\n",
      "EarlyStopping counter: 181 out of 30\n",
      "Epoch: 610/2000... Step: 610... Loss: 0.00096... Val Loss: 0.00246\n",
      "EarlyStopping counter: 182 out of 30\n",
      "Epoch: 611/2000... Step: 611... Loss: 0.00084... Val Loss: 0.00235\n",
      "EarlyStopping counter: 183 out of 30\n",
      "Epoch: 612/2000... Step: 612... Loss: 0.00067... Val Loss: 0.00230\n",
      "EarlyStopping counter: 184 out of 30\n",
      "Epoch: 613/2000... Step: 613... Loss: 0.00064... Val Loss: 0.00218\n",
      "EarlyStopping counter: 185 out of 30\n",
      "Epoch: 614/2000... Step: 614... Loss: 0.00066... Val Loss: 0.00263\n",
      "EarlyStopping counter: 186 out of 30\n",
      "Epoch: 615/2000... Step: 615... Loss: 0.00071... Val Loss: 0.00213\n",
      "EarlyStopping counter: 187 out of 30\n",
      "Epoch: 616/2000... Step: 616... Loss: 0.00072... Val Loss: 0.00274\n",
      "EarlyStopping counter: 188 out of 30\n",
      "Epoch: 617/2000... Step: 617... Loss: 0.00069... Val Loss: 0.00203\n",
      "EarlyStopping counter: 189 out of 30\n",
      "Epoch: 618/2000... Step: 618... Loss: 0.00073... Val Loss: 0.00264\n",
      "EarlyStopping counter: 190 out of 30\n",
      "Epoch: 619/2000... Step: 619... Loss: 0.00081... Val Loss: 0.00225\n",
      "EarlyStopping counter: 191 out of 30\n",
      "Epoch: 620/2000... Step: 620... Loss: 0.00098... Val Loss: 0.00259\n",
      "EarlyStopping counter: 192 out of 30\n",
      "Epoch: 621/2000... Step: 621... Loss: 0.00115... Val Loss: 0.00308\n",
      "EarlyStopping counter: 193 out of 30\n",
      "Epoch: 622/2000... Step: 622... Loss: 0.00135... Val Loss: 0.00281\n",
      "EarlyStopping counter: 194 out of 30\n",
      "Epoch: 623/2000... Step: 623... Loss: 0.00150... Val Loss: 0.00378\n",
      "EarlyStopping counter: 195 out of 30\n",
      "Epoch: 624/2000... Step: 624... Loss: 0.00165... Val Loss: 0.00309\n",
      "EarlyStopping counter: 196 out of 30\n",
      "Epoch: 625/2000... Step: 625... Loss: 0.00167... Val Loss: 0.00353\n",
      "EarlyStopping counter: 197 out of 30\n",
      "Epoch: 626/2000... Step: 626... Loss: 0.00151... Val Loss: 0.00297\n",
      "EarlyStopping counter: 198 out of 30\n",
      "Epoch: 627/2000... Step: 627... Loss: 0.00125... Val Loss: 0.00250\n",
      "EarlyStopping counter: 199 out of 30\n",
      "Epoch: 628/2000... Step: 628... Loss: 0.00104... Val Loss: 0.00326\n",
      "EarlyStopping counter: 200 out of 30\n",
      "Epoch: 629/2000... Step: 629... Loss: 0.00107... Val Loss: 0.00219\n",
      "EarlyStopping counter: 201 out of 30\n",
      "Epoch: 630/2000... Step: 630... Loss: 0.00121... Val Loss: 0.00354\n",
      "EarlyStopping counter: 202 out of 30\n",
      "Epoch: 631/2000... Step: 631... Loss: 0.00109... Val Loss: 0.00191\n",
      "EarlyStopping counter: 203 out of 30\n",
      "Epoch: 632/2000... Step: 632... Loss: 0.00072... Val Loss: 0.00218\n",
      "EarlyStopping counter: 204 out of 30\n",
      "Epoch: 633/2000... Step: 633... Loss: 0.00044... Val Loss: 0.00228\n",
      "EarlyStopping counter: 205 out of 30\n",
      "Epoch: 634/2000... Step: 634... Loss: 0.00044... Val Loss: 0.00184\n",
      "EarlyStopping counter: 206 out of 30\n",
      "Epoch: 635/2000... Step: 635... Loss: 0.00058... Val Loss: 0.00284\n",
      "EarlyStopping counter: 207 out of 30\n",
      "Epoch: 636/2000... Step: 636... Loss: 0.00062... Val Loss: 0.00192\n",
      "EarlyStopping counter: 208 out of 30\n",
      "Epoch: 637/2000... Step: 637... Loss: 0.00053... Val Loss: 0.00242\n",
      "EarlyStopping counter: 209 out of 30\n",
      "Epoch: 638/2000... Step: 638... Loss: 0.00050... Val Loss: 0.00238\n",
      "EarlyStopping counter: 210 out of 30\n",
      "Epoch: 639/2000... Step: 639... Loss: 0.00058... Val Loss: 0.00217\n",
      "EarlyStopping counter: 211 out of 30\n",
      "Epoch: 640/2000... Step: 640... Loss: 0.00068... Val Loss: 0.00259\n",
      "EarlyStopping counter: 212 out of 30\n",
      "Epoch: 641/2000... Step: 641... Loss: 0.00072... Val Loss: 0.00245\n",
      "EarlyStopping counter: 213 out of 30\n",
      "Epoch: 642/2000... Step: 642... Loss: 0.00082... Val Loss: 0.00246\n",
      "EarlyStopping counter: 214 out of 30\n",
      "Epoch: 643/2000... Step: 643... Loss: 0.00105... Val Loss: 0.00340\n",
      "EarlyStopping counter: 215 out of 30\n",
      "Epoch: 644/2000... Step: 644... Loss: 0.00140... Val Loss: 0.00296\n",
      "EarlyStopping counter: 216 out of 30\n",
      "Epoch: 645/2000... Step: 645... Loss: 0.00185... Val Loss: 0.00466\n",
      "EarlyStopping counter: 217 out of 30\n",
      "Epoch: 646/2000... Step: 646... Loss: 0.00237... Val Loss: 0.00439\n",
      "EarlyStopping counter: 218 out of 30\n",
      "Epoch: 647/2000... Step: 647... Loss: 0.00302... Val Loss: 0.00598\n",
      "EarlyStopping counter: 219 out of 30\n",
      "Epoch: 648/2000... Step: 648... Loss: 0.00376... Val Loss: 0.00577\n",
      "EarlyStopping counter: 220 out of 30\n",
      "Epoch: 649/2000... Step: 649... Loss: 0.00392... Val Loss: 0.00541\n",
      "EarlyStopping counter: 221 out of 30\n",
      "Epoch: 650/2000... Step: 650... Loss: 0.00353... Val Loss: 0.00343\n",
      "EarlyStopping counter: 222 out of 30\n",
      "Epoch: 651/2000... Step: 651... Loss: 0.00179... Val Loss: 0.00217\n",
      "EarlyStopping counter: 223 out of 30\n",
      "Epoch: 652/2000... Step: 652... Loss: 0.00063... Val Loss: 0.00208\n",
      "EarlyStopping counter: 224 out of 30\n",
      "Epoch: 653/2000... Step: 653... Loss: 0.00064... Val Loss: 0.00305\n",
      "EarlyStopping counter: 225 out of 30\n",
      "Epoch: 654/2000... Step: 654... Loss: 0.00140... Val Loss: 0.00375\n",
      "EarlyStopping counter: 226 out of 30\n",
      "Epoch: 655/2000... Step: 655... Loss: 0.00203... Val Loss: 0.00337\n",
      "EarlyStopping counter: 227 out of 30\n",
      "Epoch: 656/2000... Step: 656... Loss: 0.00198... Val Loss: 0.00331\n",
      "EarlyStopping counter: 228 out of 30\n",
      "Epoch: 657/2000... Step: 657... Loss: 0.00162... Val Loss: 0.00235\n",
      "EarlyStopping counter: 229 out of 30\n",
      "Epoch: 658/2000... Step: 658... Loss: 0.00110... Val Loss: 0.00218\n",
      "EarlyStopping counter: 230 out of 30\n",
      "Epoch: 659/2000... Step: 659... Loss: 0.00072... Val Loss: 0.00243\n",
      "EarlyStopping counter: 231 out of 30\n",
      "Epoch: 660/2000... Step: 660... Loss: 0.00062... Val Loss: 0.00214\n",
      "EarlyStopping counter: 232 out of 30\n",
      "Epoch: 661/2000... Step: 661... Loss: 0.00086... Val Loss: 0.00314\n",
      "EarlyStopping counter: 233 out of 30\n",
      "Epoch: 662/2000... Step: 662... Loss: 0.00115... Val Loss: 0.00232\n",
      "EarlyStopping counter: 234 out of 30\n",
      "Epoch: 663/2000... Step: 663... Loss: 0.00120... Val Loss: 0.00281\n",
      "EarlyStopping counter: 235 out of 30\n",
      "Epoch: 664/2000... Step: 664... Loss: 0.00087... Val Loss: 0.00212\n",
      "EarlyStopping counter: 236 out of 30\n",
      "Epoch: 665/2000... Step: 665... Loss: 0.00050... Val Loss: 0.00190\n",
      "EarlyStopping counter: 237 out of 30\n",
      "Epoch: 666/2000... Step: 666... Loss: 0.00044... Val Loss: 0.00276\n",
      "EarlyStopping counter: 238 out of 30\n",
      "Epoch: 667/2000... Step: 667... Loss: 0.00063... Val Loss: 0.00198\n",
      "EarlyStopping counter: 239 out of 30\n",
      "Epoch: 668/2000... Step: 668... Loss: 0.00079... Val Loss: 0.00296\n",
      "EarlyStopping counter: 240 out of 30\n",
      "Epoch: 669/2000... Step: 669... Loss: 0.00077... Val Loss: 0.00205\n",
      "EarlyStopping counter: 241 out of 30\n",
      "Epoch: 670/2000... Step: 670... Loss: 0.00066... Val Loss: 0.00249\n",
      "EarlyStopping counter: 242 out of 30\n",
      "Epoch: 671/2000... Step: 671... Loss: 0.00058... Val Loss: 0.00222\n",
      "EarlyStopping counter: 243 out of 30\n",
      "Epoch: 672/2000... Step: 672... Loss: 0.00051... Val Loss: 0.00200\n",
      "EarlyStopping counter: 244 out of 30\n",
      "Epoch: 673/2000... Step: 673... Loss: 0.00042... Val Loss: 0.00219\n",
      "EarlyStopping counter: 245 out of 30\n",
      "Epoch: 674/2000... Step: 674... Loss: 0.00035... Val Loss: 0.00194\n",
      "EarlyStopping counter: 246 out of 30\n",
      "Epoch: 675/2000... Step: 675... Loss: 0.00039... Val Loss: 0.00228\n",
      "EarlyStopping counter: 247 out of 30\n",
      "Epoch: 676/2000... Step: 676... Loss: 0.00050... Val Loss: 0.00214\n",
      "EarlyStopping counter: 248 out of 30\n",
      "Epoch: 677/2000... Step: 677... Loss: 0.00055... Val Loss: 0.00235\n",
      "EarlyStopping counter: 249 out of 30\n",
      "Epoch: 678/2000... Step: 678... Loss: 0.00051... Val Loss: 0.00193\n",
      "EarlyStopping counter: 250 out of 30\n",
      "Epoch: 679/2000... Step: 679... Loss: 0.00043... Val Loss: 0.00227\n",
      "EarlyStopping counter: 251 out of 30\n",
      "Epoch: 680/2000... Step: 680... Loss: 0.00040... Val Loss: 0.00188\n",
      "EarlyStopping counter: 252 out of 30\n",
      "Epoch: 681/2000... Step: 681... Loss: 0.00042... Val Loss: 0.00217\n",
      "EarlyStopping counter: 253 out of 30\n",
      "Epoch: 682/2000... Step: 682... Loss: 0.00042... Val Loss: 0.00205\n",
      "EarlyStopping counter: 254 out of 30\n",
      "Epoch: 683/2000... Step: 683... Loss: 0.00041... Val Loss: 0.00199\n",
      "EarlyStopping counter: 255 out of 30\n",
      "Epoch: 684/2000... Step: 684... Loss: 0.00039... Val Loss: 0.00231\n",
      "EarlyStopping counter: 256 out of 30\n",
      "Epoch: 685/2000... Step: 685... Loss: 0.00041... Val Loss: 0.00189\n",
      "EarlyStopping counter: 257 out of 30\n",
      "Epoch: 686/2000... Step: 686... Loss: 0.00044... Val Loss: 0.00244\n",
      "EarlyStopping counter: 258 out of 30\n",
      "Epoch: 687/2000... Step: 687... Loss: 0.00045... Val Loss: 0.00189\n",
      "EarlyStopping counter: 259 out of 30\n",
      "Epoch: 688/2000... Step: 688... Loss: 0.00044... Val Loss: 0.00225\n",
      "EarlyStopping counter: 260 out of 30\n",
      "Epoch: 689/2000... Step: 689... Loss: 0.00042... Val Loss: 0.00204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 261 out of 30\n",
      "Epoch: 690/2000... Step: 690... Loss: 0.00043... Val Loss: 0.00217\n",
      "EarlyStopping counter: 262 out of 30\n",
      "Epoch: 691/2000... Step: 691... Loss: 0.00048... Val Loss: 0.00231\n",
      "EarlyStopping counter: 263 out of 30\n",
      "Epoch: 692/2000... Step: 692... Loss: 0.00052... Val Loss: 0.00222\n",
      "EarlyStopping counter: 264 out of 30\n",
      "Epoch: 693/2000... Step: 693... Loss: 0.00062... Val Loss: 0.00266\n",
      "EarlyStopping counter: 265 out of 30\n",
      "Epoch: 694/2000... Step: 694... Loss: 0.00076... Val Loss: 0.00252\n",
      "EarlyStopping counter: 266 out of 30\n",
      "Epoch: 695/2000... Step: 695... Loss: 0.00100... Val Loss: 0.00334\n",
      "EarlyStopping counter: 267 out of 30\n",
      "Epoch: 696/2000... Step: 696... Loss: 0.00136... Val Loss: 0.00331\n",
      "EarlyStopping counter: 268 out of 30\n",
      "Epoch: 697/2000... Step: 697... Loss: 0.00181... Val Loss: 0.00443\n",
      "EarlyStopping counter: 269 out of 30\n",
      "Epoch: 698/2000... Step: 698... Loss: 0.00247... Val Loss: 0.00462\n",
      "EarlyStopping counter: 270 out of 30\n",
      "Epoch: 699/2000... Step: 699... Loss: 0.00311... Val Loss: 0.00574\n",
      "EarlyStopping counter: 271 out of 30\n",
      "Epoch: 700/2000... Step: 700... Loss: 0.00399... Val Loss: 0.00570\n",
      "EarlyStopping counter: 272 out of 30\n",
      "Epoch: 701/2000... Step: 701... Loss: 0.00420... Val Loss: 0.00530\n",
      "EarlyStopping counter: 273 out of 30\n",
      "Epoch: 702/2000... Step: 702... Loss: 0.00377... Val Loss: 0.00396\n",
      "EarlyStopping counter: 274 out of 30\n",
      "Epoch: 703/2000... Step: 703... Loss: 0.00213... Val Loss: 0.00206\n",
      "EarlyStopping counter: 275 out of 30\n",
      "Epoch: 704/2000... Step: 704... Loss: 0.00082... Val Loss: 0.00274\n",
      "EarlyStopping counter: 276 out of 30\n",
      "Epoch: 705/2000... Step: 705... Loss: 0.00082... Val Loss: 0.00328\n",
      "EarlyStopping counter: 277 out of 30\n",
      "Epoch: 706/2000... Step: 706... Loss: 0.00181... Val Loss: 0.00412\n",
      "EarlyStopping counter: 278 out of 30\n",
      "Epoch: 707/2000... Step: 707... Loss: 0.00237... Val Loss: 0.00397\n",
      "EarlyStopping counter: 279 out of 30\n",
      "Epoch: 708/2000... Step: 708... Loss: 0.00216... Val Loss: 0.00311\n",
      "EarlyStopping counter: 280 out of 30\n",
      "Epoch: 709/2000... Step: 709... Loss: 0.00145... Val Loss: 0.00246\n",
      "EarlyStopping counter: 281 out of 30\n",
      "Epoch: 710/2000... Step: 710... Loss: 0.00071... Val Loss: 0.00227\n",
      "EarlyStopping counter: 282 out of 30\n",
      "Epoch: 711/2000... Step: 711... Loss: 0.00059... Val Loss: 0.00243\n",
      "EarlyStopping counter: 283 out of 30\n",
      "Epoch: 712/2000... Step: 712... Loss: 0.00097... Val Loss: 0.00320\n",
      "EarlyStopping counter: 284 out of 30\n",
      "Epoch: 713/2000... Step: 713... Loss: 0.00138... Val Loss: 0.00305\n",
      "EarlyStopping counter: 285 out of 30\n",
      "Epoch: 714/2000... Step: 714... Loss: 0.00134... Val Loss: 0.00248\n",
      "EarlyStopping counter: 286 out of 30\n",
      "Epoch: 715/2000... Step: 715... Loss: 0.00091... Val Loss: 0.00252\n",
      "EarlyStopping counter: 287 out of 30\n",
      "Epoch: 716/2000... Step: 716... Loss: 0.00058... Val Loss: 0.00185\n",
      "EarlyStopping counter: 288 out of 30\n",
      "Epoch: 717/2000... Step: 717... Loss: 0.00063... Val Loss: 0.00296\n",
      "EarlyStopping counter: 289 out of 30\n",
      "Epoch: 718/2000... Step: 718... Loss: 0.00092... Val Loss: 0.00225\n",
      "EarlyStopping counter: 290 out of 30\n",
      "Epoch: 719/2000... Step: 719... Loss: 0.00098... Val Loss: 0.00283\n",
      "EarlyStopping counter: 291 out of 30\n",
      "Epoch: 720/2000... Step: 720... Loss: 0.00077... Val Loss: 0.00213\n",
      "EarlyStopping counter: 292 out of 30\n",
      "Epoch: 721/2000... Step: 721... Loss: 0.00045... Val Loss: 0.00196\n",
      "EarlyStopping counter: 293 out of 30\n",
      "Epoch: 722/2000... Step: 722... Loss: 0.00043... Val Loss: 0.00261\n",
      "EarlyStopping counter: 294 out of 30\n",
      "Epoch: 723/2000... Step: 723... Loss: 0.00059... Val Loss: 0.00197\n",
      "EarlyStopping counter: 295 out of 30\n",
      "Epoch: 724/2000... Step: 724... Loss: 0.00063... Val Loss: 0.00249\n",
      "EarlyStopping counter: 296 out of 30\n",
      "Epoch: 725/2000... Step: 725... Loss: 0.00057... Val Loss: 0.00202\n",
      "EarlyStopping counter: 297 out of 30\n",
      "Epoch: 726/2000... Step: 726... Loss: 0.00044... Val Loss: 0.00201\n",
      "EarlyStopping counter: 298 out of 30\n",
      "Epoch: 727/2000... Step: 727... Loss: 0.00044... Val Loss: 0.00223\n",
      "EarlyStopping counter: 299 out of 30\n",
      "Epoch: 728/2000... Step: 728... Loss: 0.00045... Val Loss: 0.00189\n",
      "EarlyStopping counter: 300 out of 30\n",
      "Epoch: 729/2000... Step: 729... Loss: 0.00040... Val Loss: 0.00228\n",
      "EarlyStopping counter: 301 out of 30\n",
      "Epoch: 730/2000... Step: 730... Loss: 0.00038... Val Loss: 0.00205\n",
      "EarlyStopping counter: 302 out of 30\n",
      "Epoch: 731/2000... Step: 731... Loss: 0.00042... Val Loss: 0.00213\n",
      "EarlyStopping counter: 303 out of 30\n",
      "Epoch: 732/2000... Step: 732... Loss: 0.00050... Val Loss: 0.00218\n",
      "EarlyStopping counter: 304 out of 30\n",
      "Epoch: 733/2000... Step: 733... Loss: 0.00046... Val Loss: 0.00202\n",
      "EarlyStopping counter: 305 out of 30\n",
      "Epoch: 734/2000... Step: 734... Loss: 0.00035... Val Loss: 0.00196\n",
      "EarlyStopping counter: 306 out of 30\n",
      "Epoch: 735/2000... Step: 735... Loss: 0.00030... Val Loss: 0.00209\n",
      "EarlyStopping counter: 307 out of 30\n",
      "Epoch: 736/2000... Step: 736... Loss: 0.00033... Val Loss: 0.00200\n",
      "EarlyStopping counter: 308 out of 30\n",
      "Epoch: 737/2000... Step: 737... Loss: 0.00038... Val Loss: 0.00203\n",
      "EarlyStopping counter: 309 out of 30\n",
      "Epoch: 738/2000... Step: 738... Loss: 0.00037... Val Loss: 0.00208\n",
      "EarlyStopping counter: 310 out of 30\n",
      "Epoch: 739/2000... Step: 739... Loss: 0.00034... Val Loss: 0.00192\n",
      "EarlyStopping counter: 311 out of 30\n",
      "Epoch: 740/2000... Step: 740... Loss: 0.00033... Val Loss: 0.00224\n",
      "EarlyStopping counter: 312 out of 30\n",
      "Epoch: 741/2000... Step: 741... Loss: 0.00034... Val Loss: 0.00182\n",
      "EarlyStopping counter: 313 out of 30\n",
      "Epoch: 742/2000... Step: 742... Loss: 0.00034... Val Loss: 0.00224\n",
      "EarlyStopping counter: 314 out of 30\n",
      "Epoch: 743/2000... Step: 743... Loss: 0.00034... Val Loss: 0.00189\n",
      "EarlyStopping counter: 315 out of 30\n",
      "Epoch: 744/2000... Step: 744... Loss: 0.00033... Val Loss: 0.00217\n",
      "EarlyStopping counter: 316 out of 30\n",
      "Epoch: 745/2000... Step: 745... Loss: 0.00035... Val Loss: 0.00204\n",
      "EarlyStopping counter: 317 out of 30\n",
      "Epoch: 746/2000... Step: 746... Loss: 0.00038... Val Loss: 0.00220\n",
      "EarlyStopping counter: 318 out of 30\n",
      "Epoch: 747/2000... Step: 747... Loss: 0.00041... Val Loss: 0.00209\n",
      "EarlyStopping counter: 319 out of 30\n",
      "Epoch: 748/2000... Step: 748... Loss: 0.00046... Val Loss: 0.00246\n",
      "EarlyStopping counter: 320 out of 30\n",
      "Epoch: 749/2000... Step: 749... Loss: 0.00058... Val Loss: 0.00228\n",
      "EarlyStopping counter: 321 out of 30\n",
      "Epoch: 750/2000... Step: 750... Loss: 0.00081... Val Loss: 0.00318\n",
      "EarlyStopping counter: 322 out of 30\n",
      "Epoch: 751/2000... Step: 751... Loss: 0.00129... Val Loss: 0.00325\n",
      "EarlyStopping counter: 323 out of 30\n",
      "Epoch: 752/2000... Step: 752... Loss: 0.00214... Val Loss: 0.00557\n",
      "EarlyStopping counter: 324 out of 30\n",
      "Epoch: 753/2000... Step: 753... Loss: 0.00382... Val Loss: 0.00651\n",
      "EarlyStopping counter: 325 out of 30\n",
      "Epoch: 754/2000... Step: 754... Loss: 0.00600... Val Loss: 0.01047\n",
      "EarlyStopping counter: 326 out of 30\n",
      "Epoch: 755/2000... Step: 755... Loss: 0.00832... Val Loss: 0.00832\n",
      "EarlyStopping counter: 327 out of 30\n",
      "Epoch: 756/2000... Step: 756... Loss: 0.00702... Val Loss: 0.00581\n",
      "EarlyStopping counter: 328 out of 30\n",
      "Epoch: 757/2000... Step: 757... Loss: 0.00430... Val Loss: 0.00361\n",
      "EarlyStopping counter: 329 out of 30\n",
      "Epoch: 758/2000... Step: 758... Loss: 0.00166... Val Loss: 0.00310\n",
      "EarlyStopping counter: 330 out of 30\n",
      "Epoch: 759/2000... Step: 759... Loss: 0.00199... Val Loss: 0.00563\n",
      "EarlyStopping counter: 331 out of 30\n",
      "Epoch: 760/2000... Step: 760... Loss: 0.00418... Val Loss: 0.00629\n",
      "EarlyStopping counter: 332 out of 30\n",
      "Epoch: 761/2000... Step: 761... Loss: 0.00393... Val Loss: 0.00289\n",
      "EarlyStopping counter: 333 out of 30\n",
      "Epoch: 762/2000... Step: 762... Loss: 0.00165... Val Loss: 0.00254\n",
      "EarlyStopping counter: 334 out of 30\n",
      "Epoch: 763/2000... Step: 763... Loss: 0.00104... Val Loss: 0.00422\n",
      "EarlyStopping counter: 335 out of 30\n",
      "Epoch: 764/2000... Step: 764... Loss: 0.00237... Val Loss: 0.00429\n",
      "EarlyStopping counter: 336 out of 30\n",
      "Epoch: 765/2000... Step: 765... Loss: 0.00291... Val Loss: 0.00353\n",
      "EarlyStopping counter: 337 out of 30\n",
      "Epoch: 766/2000... Step: 766... Loss: 0.00164... Val Loss: 0.00213\n",
      "EarlyStopping counter: 338 out of 30\n",
      "Epoch: 767/2000... Step: 767... Loss: 0.00069... Val Loss: 0.00326\n",
      "EarlyStopping counter: 339 out of 30\n",
      "Epoch: 768/2000... Step: 768... Loss: 0.00168... Val Loss: 0.00412\n",
      "EarlyStopping counter: 340 out of 30\n",
      "Epoch: 769/2000... Step: 769... Loss: 0.00220... Val Loss: 0.00309\n",
      "EarlyStopping counter: 341 out of 30\n",
      "Epoch: 770/2000... Step: 770... Loss: 0.00129... Val Loss: 0.00196\n",
      "EarlyStopping counter: 342 out of 30\n",
      "Epoch: 771/2000... Step: 771... Loss: 0.00100... Val Loss: 0.00369\n",
      "EarlyStopping counter: 343 out of 30\n",
      "Epoch: 772/2000... Step: 772... Loss: 0.00150... Val Loss: 0.00269\n",
      "EarlyStopping counter: 344 out of 30\n",
      "Epoch: 773/2000... Step: 773... Loss: 0.00141... Val Loss: 0.00226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 345 out of 30\n",
      "Epoch: 774/2000... Step: 774... Loss: 0.00083... Val Loss: 0.00282\n",
      "EarlyStopping counter: 346 out of 30\n",
      "Epoch: 775/2000... Step: 775... Loss: 0.00090... Val Loss: 0.00234\n",
      "EarlyStopping counter: 347 out of 30\n",
      "Epoch: 776/2000... Step: 776... Loss: 0.00120... Val Loss: 0.00297\n",
      "EarlyStopping counter: 348 out of 30\n",
      "Epoch: 777/2000... Step: 777... Loss: 0.00086... Val Loss: 0.00230\n",
      "EarlyStopping counter: 349 out of 30\n",
      "Epoch: 778/2000... Step: 778... Loss: 0.00056... Val Loss: 0.00209\n",
      "EarlyStopping counter: 350 out of 30\n",
      "Epoch: 779/2000... Step: 779... Loss: 0.00076... Val Loss: 0.00314\n",
      "EarlyStopping counter: 351 out of 30\n",
      "Epoch: 780/2000... Step: 780... Loss: 0.00110... Val Loss: 0.00197\n",
      "EarlyStopping counter: 352 out of 30\n",
      "Epoch: 781/2000... Step: 781... Loss: 0.00062... Val Loss: 0.00214\n",
      "EarlyStopping counter: 353 out of 30\n",
      "Epoch: 782/2000... Step: 782... Loss: 0.00045... Val Loss: 0.00251\n",
      "EarlyStopping counter: 354 out of 30\n",
      "Epoch: 783/2000... Step: 783... Loss: 0.00073... Val Loss: 0.00217\n",
      "EarlyStopping counter: 355 out of 30\n",
      "Epoch: 784/2000... Step: 784... Loss: 0.00075... Val Loss: 0.00233\n",
      "EarlyStopping counter: 356 out of 30\n",
      "Epoch: 785/2000... Step: 785... Loss: 0.00052... Val Loss: 0.00211\n",
      "EarlyStopping counter: 357 out of 30\n",
      "Epoch: 786/2000... Step: 786... Loss: 0.00039... Val Loss: 0.00220\n",
      "EarlyStopping counter: 358 out of 30\n",
      "Epoch: 787/2000... Step: 787... Loss: 0.00055... Val Loss: 0.00237\n",
      "EarlyStopping counter: 359 out of 30\n",
      "Epoch: 788/2000... Step: 788... Loss: 0.00056... Val Loss: 0.00212\n",
      "EarlyStopping counter: 360 out of 30\n",
      "Epoch: 789/2000... Step: 789... Loss: 0.00040... Val Loss: 0.00202\n",
      "EarlyStopping counter: 361 out of 30\n",
      "Epoch: 790/2000... Step: 790... Loss: 0.00040... Val Loss: 0.00249\n",
      "EarlyStopping counter: 362 out of 30\n",
      "Epoch: 791/2000... Step: 791... Loss: 0.00053... Val Loss: 0.00205\n",
      "EarlyStopping counter: 363 out of 30\n",
      "Epoch: 792/2000... Step: 792... Loss: 0.00045... Val Loss: 0.00210\n",
      "EarlyStopping counter: 364 out of 30\n",
      "Epoch: 793/2000... Step: 793... Loss: 0.00036... Val Loss: 0.00223\n",
      "EarlyStopping counter: 365 out of 30\n",
      "Epoch: 794/2000... Step: 794... Loss: 0.00040... Val Loss: 0.00202\n",
      "EarlyStopping counter: 366 out of 30\n",
      "Epoch: 795/2000... Step: 795... Loss: 0.00048... Val Loss: 0.00215\n",
      "EarlyStopping counter: 367 out of 30\n",
      "Epoch: 796/2000... Step: 796... Loss: 0.00041... Val Loss: 0.00213\n",
      "EarlyStopping counter: 368 out of 30\n",
      "Epoch: 797/2000... Step: 797... Loss: 0.00036... Val Loss: 0.00195\n",
      "EarlyStopping counter: 369 out of 30\n",
      "Epoch: 798/2000... Step: 798... Loss: 0.00039... Val Loss: 0.00237\n",
      "EarlyStopping counter: 370 out of 30\n",
      "Epoch: 799/2000... Step: 799... Loss: 0.00044... Val Loss: 0.00187\n",
      "EarlyStopping counter: 371 out of 30\n",
      "Epoch: 800/2000... Step: 800... Loss: 0.00040... Val Loss: 0.00237\n",
      "EarlyStopping counter: 372 out of 30\n",
      "Epoch: 801/2000... Step: 801... Loss: 0.00041... Val Loss: 0.00193\n",
      "EarlyStopping counter: 373 out of 30\n",
      "Epoch: 802/2000... Step: 802... Loss: 0.00046... Val Loss: 0.00238\n",
      "EarlyStopping counter: 374 out of 30\n",
      "Epoch: 803/2000... Step: 803... Loss: 0.00051... Val Loss: 0.00196\n",
      "EarlyStopping counter: 375 out of 30\n",
      "Epoch: 804/2000... Step: 804... Loss: 0.00049... Val Loss: 0.00253\n",
      "EarlyStopping counter: 376 out of 30\n",
      "Epoch: 805/2000... Step: 805... Loss: 0.00051... Val Loss: 0.00191\n",
      "EarlyStopping counter: 377 out of 30\n",
      "Epoch: 806/2000... Step: 806... Loss: 0.00055... Val Loss: 0.00267\n",
      "EarlyStopping counter: 378 out of 30\n",
      "Epoch: 807/2000... Step: 807... Loss: 0.00060... Val Loss: 0.00199\n",
      "EarlyStopping counter: 379 out of 30\n",
      "Epoch: 808/2000... Step: 808... Loss: 0.00061... Val Loss: 0.00248\n",
      "EarlyStopping counter: 380 out of 30\n",
      "Epoch: 809/2000... Step: 809... Loss: 0.00059... Val Loss: 0.00203\n",
      "EarlyStopping counter: 381 out of 30\n",
      "Epoch: 810/2000... Step: 810... Loss: 0.00054... Val Loss: 0.00227\n",
      "EarlyStopping counter: 382 out of 30\n",
      "Epoch: 811/2000... Step: 811... Loss: 0.00046... Val Loss: 0.00199\n",
      "EarlyStopping counter: 383 out of 30\n",
      "Epoch: 812/2000... Step: 812... Loss: 0.00034... Val Loss: 0.00203\n",
      "EarlyStopping counter: 384 out of 30\n",
      "Epoch: 813/2000... Step: 813... Loss: 0.00029... Val Loss: 0.00213\n",
      "EarlyStopping counter: 385 out of 30\n",
      "Epoch: 814/2000... Step: 814... Loss: 0.00031... Val Loss: 0.00195\n",
      "EarlyStopping counter: 386 out of 30\n",
      "Epoch: 815/2000... Step: 815... Loss: 0.00036... Val Loss: 0.00229\n",
      "EarlyStopping counter: 387 out of 30\n",
      "Epoch: 816/2000... Step: 816... Loss: 0.00039... Val Loss: 0.00189\n",
      "EarlyStopping counter: 388 out of 30\n",
      "Epoch: 817/2000... Step: 817... Loss: 0.00038... Val Loss: 0.00241\n",
      "EarlyStopping counter: 389 out of 30\n",
      "Epoch: 818/2000... Step: 818... Loss: 0.00037... Val Loss: 0.00183\n",
      "EarlyStopping counter: 390 out of 30\n",
      "Epoch: 819/2000... Step: 819... Loss: 0.00033... Val Loss: 0.00218\n",
      "EarlyStopping counter: 391 out of 30\n",
      "Epoch: 820/2000... Step: 820... Loss: 0.00030... Val Loss: 0.00195\n",
      "EarlyStopping counter: 392 out of 30\n",
      "Epoch: 821/2000... Step: 821... Loss: 0.00030... Val Loss: 0.00205\n",
      "EarlyStopping counter: 393 out of 30\n",
      "Epoch: 822/2000... Step: 822... Loss: 0.00031... Val Loss: 0.00209\n",
      "EarlyStopping counter: 394 out of 30\n",
      "Epoch: 823/2000... Step: 823... Loss: 0.00032... Val Loss: 0.00200\n",
      "EarlyStopping counter: 395 out of 30\n",
      "Epoch: 824/2000... Step: 824... Loss: 0.00032... Val Loss: 0.00215\n",
      "EarlyStopping counter: 396 out of 30\n",
      "Epoch: 825/2000... Step: 825... Loss: 0.00031... Val Loss: 0.00198\n",
      "EarlyStopping counter: 397 out of 30\n",
      "Epoch: 826/2000... Step: 826... Loss: 0.00032... Val Loss: 0.00217\n",
      "EarlyStopping counter: 398 out of 30\n",
      "Epoch: 827/2000... Step: 827... Loss: 0.00035... Val Loss: 0.00200\n",
      "EarlyStopping counter: 399 out of 30\n",
      "Epoch: 828/2000... Step: 828... Loss: 0.00039... Val Loss: 0.00226\n",
      "EarlyStopping counter: 400 out of 30\n",
      "Epoch: 829/2000... Step: 829... Loss: 0.00043... Val Loss: 0.00213\n",
      "EarlyStopping counter: 401 out of 30\n",
      "Epoch: 830/2000... Step: 830... Loss: 0.00050... Val Loss: 0.00248\n",
      "EarlyStopping counter: 402 out of 30\n",
      "Epoch: 831/2000... Step: 831... Loss: 0.00061... Val Loss: 0.00245\n",
      "EarlyStopping counter: 403 out of 30\n",
      "Epoch: 832/2000... Step: 832... Loss: 0.00079... Val Loss: 0.00280\n",
      "EarlyStopping counter: 404 out of 30\n",
      "Epoch: 833/2000... Step: 833... Loss: 0.00105... Val Loss: 0.00322\n",
      "EarlyStopping counter: 405 out of 30\n",
      "Epoch: 834/2000... Step: 834... Loss: 0.00147... Val Loss: 0.00345\n",
      "EarlyStopping counter: 406 out of 30\n",
      "Epoch: 835/2000... Step: 835... Loss: 0.00203... Val Loss: 0.00483\n",
      "EarlyStopping counter: 407 out of 30\n",
      "Epoch: 836/2000... Step: 836... Loss: 0.00292... Val Loss: 0.00478\n",
      "EarlyStopping counter: 408 out of 30\n",
      "Epoch: 837/2000... Step: 837... Loss: 0.00370... Val Loss: 0.00639\n",
      "EarlyStopping counter: 409 out of 30\n",
      "Epoch: 838/2000... Step: 838... Loss: 0.00416... Val Loss: 0.00480\n",
      "EarlyStopping counter: 410 out of 30\n",
      "Epoch: 839/2000... Step: 839... Loss: 0.00352... Val Loss: 0.00451\n",
      "EarlyStopping counter: 411 out of 30\n",
      "Epoch: 840/2000... Step: 840... Loss: 0.00225... Val Loss: 0.00227\n",
      "EarlyStopping counter: 412 out of 30\n",
      "Epoch: 841/2000... Step: 841... Loss: 0.00106... Val Loss: 0.00280\n",
      "EarlyStopping counter: 413 out of 30\n",
      "Epoch: 842/2000... Step: 842... Loss: 0.00089... Val Loss: 0.00298\n",
      "EarlyStopping counter: 414 out of 30\n",
      "Epoch: 843/2000... Step: 843... Loss: 0.00172... Val Loss: 0.00437\n",
      "EarlyStopping counter: 415 out of 30\n",
      "Epoch: 844/2000... Step: 844... Loss: 0.00230... Val Loss: 0.00391\n",
      "EarlyStopping counter: 416 out of 30\n",
      "Epoch: 845/2000... Step: 845... Loss: 0.00216... Val Loss: 0.00287\n",
      "EarlyStopping counter: 417 out of 30\n",
      "Epoch: 846/2000... Step: 846... Loss: 0.00117... Val Loss: 0.00248\n",
      "EarlyStopping counter: 418 out of 30\n",
      "Epoch: 847/2000... Step: 847... Loss: 0.00048... Val Loss: 0.00206\n",
      "EarlyStopping counter: 419 out of 30\n",
      "Epoch: 848/2000... Step: 848... Loss: 0.00061... Val Loss: 0.00318\n",
      "EarlyStopping counter: 420 out of 30\n",
      "Epoch: 849/2000... Step: 849... Loss: 0.00119... Val Loss: 0.00304\n",
      "EarlyStopping counter: 421 out of 30\n",
      "Epoch: 850/2000... Step: 850... Loss: 0.00150... Val Loss: 0.00284\n",
      "EarlyStopping counter: 422 out of 30\n",
      "Epoch: 851/2000... Step: 851... Loss: 0.00119... Val Loss: 0.00245\n",
      "EarlyStopping counter: 423 out of 30\n",
      "Epoch: 852/2000... Step: 852... Loss: 0.00080... Val Loss: 0.00208\n",
      "EarlyStopping counter: 424 out of 30\n",
      "Epoch: 853/2000... Step: 853... Loss: 0.00051... Val Loss: 0.00239\n",
      "EarlyStopping counter: 425 out of 30\n",
      "Epoch: 854/2000... Step: 854... Loss: 0.00047... Val Loss: 0.00237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 426 out of 30\n",
      "Epoch: 855/2000... Step: 855... Loss: 0.00064... Val Loss: 0.00242\n",
      "EarlyStopping counter: 427 out of 30\n",
      "Epoch: 856/2000... Step: 856... Loss: 0.00083... Val Loss: 0.00270\n",
      "EarlyStopping counter: 428 out of 30\n",
      "Epoch: 857/2000... Step: 857... Loss: 0.00084... Val Loss: 0.00234\n",
      "EarlyStopping counter: 429 out of 30\n",
      "Epoch: 858/2000... Step: 858... Loss: 0.00063... Val Loss: 0.00218\n",
      "EarlyStopping counter: 430 out of 30\n",
      "Epoch: 859/2000... Step: 859... Loss: 0.00039... Val Loss: 0.00226\n",
      "EarlyStopping counter: 431 out of 30\n",
      "Epoch: 860/2000... Step: 860... Loss: 0.00033... Val Loss: 0.00203\n",
      "EarlyStopping counter: 432 out of 30\n",
      "Epoch: 861/2000... Step: 861... Loss: 0.00041... Val Loss: 0.00231\n",
      "EarlyStopping counter: 433 out of 30\n",
      "Epoch: 862/2000... Step: 862... Loss: 0.00049... Val Loss: 0.00224\n",
      "EarlyStopping counter: 434 out of 30\n",
      "Epoch: 863/2000... Step: 863... Loss: 0.00054... Val Loss: 0.00230\n",
      "EarlyStopping counter: 435 out of 30\n",
      "Epoch: 864/2000... Step: 864... Loss: 0.00057... Val Loss: 0.00232\n",
      "EarlyStopping counter: 436 out of 30\n",
      "Epoch: 865/2000... Step: 865... Loss: 0.00052... Val Loss: 0.00209\n",
      "EarlyStopping counter: 437 out of 30\n",
      "Epoch: 866/2000... Step: 866... Loss: 0.00040... Val Loss: 0.00217\n",
      "EarlyStopping counter: 438 out of 30\n",
      "Epoch: 867/2000... Step: 867... Loss: 0.00030... Val Loss: 0.00196\n",
      "EarlyStopping counter: 439 out of 30\n",
      "Epoch: 868/2000... Step: 868... Loss: 0.00030... Val Loss: 0.00212\n",
      "EarlyStopping counter: 440 out of 30\n",
      "Epoch: 869/2000... Step: 869... Loss: 0.00036... Val Loss: 0.00201\n",
      "EarlyStopping counter: 441 out of 30\n",
      "Epoch: 870/2000... Step: 870... Loss: 0.00040... Val Loss: 0.00236\n",
      "EarlyStopping counter: 442 out of 30\n",
      "Epoch: 871/2000... Step: 871... Loss: 0.00044... Val Loss: 0.00199\n",
      "EarlyStopping counter: 443 out of 30\n",
      "Epoch: 872/2000... Step: 872... Loss: 0.00048... Val Loss: 0.00253\n",
      "EarlyStopping counter: 444 out of 30\n",
      "Epoch: 873/2000... Step: 873... Loss: 0.00051... Val Loss: 0.00195\n",
      "EarlyStopping counter: 445 out of 30\n",
      "Epoch: 874/2000... Step: 874... Loss: 0.00050... Val Loss: 0.00255\n",
      "EarlyStopping counter: 446 out of 30\n",
      "Epoch: 875/2000... Step: 875... Loss: 0.00048... Val Loss: 0.00189\n",
      "EarlyStopping counter: 447 out of 30\n",
      "Epoch: 876/2000... Step: 876... Loss: 0.00049... Val Loss: 0.00269\n",
      "EarlyStopping counter: 448 out of 30\n",
      "Epoch: 877/2000... Step: 877... Loss: 0.00054... Val Loss: 0.00197\n",
      "EarlyStopping counter: 449 out of 30\n",
      "Epoch: 878/2000... Step: 878... Loss: 0.00065... Val Loss: 0.00314\n",
      "EarlyStopping counter: 450 out of 30\n",
      "Epoch: 879/2000... Step: 879... Loss: 0.00086... Val Loss: 0.00237\n",
      "EarlyStopping counter: 451 out of 30\n",
      "Epoch: 880/2000... Step: 880... Loss: 0.00121... Val Loss: 0.00428\n",
      "EarlyStopping counter: 452 out of 30\n",
      "Epoch: 881/2000... Step: 881... Loss: 0.00169... Val Loss: 0.00287\n",
      "EarlyStopping counter: 453 out of 30\n",
      "Epoch: 882/2000... Step: 882... Loss: 0.00177... Val Loss: 0.00426\n",
      "EarlyStopping counter: 454 out of 30\n",
      "Epoch: 883/2000... Step: 883... Loss: 0.00171... Val Loss: 0.00236\n",
      "EarlyStopping counter: 455 out of 30\n",
      "Epoch: 884/2000... Step: 884... Loss: 0.00113... Val Loss: 0.00240\n",
      "EarlyStopping counter: 456 out of 30\n",
      "Epoch: 885/2000... Step: 885... Loss: 0.00053... Val Loss: 0.00216\n",
      "EarlyStopping counter: 457 out of 30\n",
      "Epoch: 886/2000... Step: 886... Loss: 0.00034... Val Loss: 0.00193\n",
      "EarlyStopping counter: 458 out of 30\n",
      "Epoch: 887/2000... Step: 887... Loss: 0.00064... Val Loss: 0.00317\n",
      "EarlyStopping counter: 459 out of 30\n",
      "Epoch: 888/2000... Step: 888... Loss: 0.00097... Val Loss: 0.00209\n",
      "EarlyStopping counter: 460 out of 30\n",
      "Epoch: 889/2000... Step: 889... Loss: 0.00074... Val Loss: 0.00235\n",
      "EarlyStopping counter: 461 out of 30\n",
      "Epoch: 890/2000... Step: 890... Loss: 0.00043... Val Loss: 0.00220\n",
      "EarlyStopping counter: 462 out of 30\n",
      "Epoch: 891/2000... Step: 891... Loss: 0.00040... Val Loss: 0.00195\n",
      "EarlyStopping counter: 463 out of 30\n",
      "Epoch: 892/2000... Step: 892... Loss: 0.00058... Val Loss: 0.00274\n",
      "EarlyStopping counter: 464 out of 30\n",
      "Epoch: 893/2000... Step: 893... Loss: 0.00067... Val Loss: 0.00192\n",
      "EarlyStopping counter: 465 out of 30\n",
      "Epoch: 894/2000... Step: 894... Loss: 0.00049... Val Loss: 0.00209\n",
      "EarlyStopping counter: 466 out of 30\n",
      "Epoch: 895/2000... Step: 895... Loss: 0.00037... Val Loss: 0.00232\n",
      "EarlyStopping counter: 467 out of 30\n",
      "Epoch: 896/2000... Step: 896... Loss: 0.00042... Val Loss: 0.00195\n",
      "EarlyStopping counter: 468 out of 30\n",
      "Epoch: 897/2000... Step: 897... Loss: 0.00049... Val Loss: 0.00239\n",
      "EarlyStopping counter: 469 out of 30\n",
      "Epoch: 898/2000... Step: 898... Loss: 0.00046... Val Loss: 0.00198\n",
      "EarlyStopping counter: 470 out of 30\n",
      "Epoch: 899/2000... Step: 899... Loss: 0.00035... Val Loss: 0.00208\n",
      "EarlyStopping counter: 471 out of 30\n",
      "Epoch: 900/2000... Step: 900... Loss: 0.00034... Val Loss: 0.00232\n",
      "EarlyStopping counter: 472 out of 30\n",
      "Epoch: 901/2000... Step: 901... Loss: 0.00041... Val Loss: 0.00192\n",
      "EarlyStopping counter: 473 out of 30\n",
      "Epoch: 902/2000... Step: 902... Loss: 0.00042... Val Loss: 0.00235\n",
      "EarlyStopping counter: 474 out of 30\n",
      "Epoch: 903/2000... Step: 903... Loss: 0.00037... Val Loss: 0.00193\n",
      "EarlyStopping counter: 475 out of 30\n",
      "Epoch: 904/2000... Step: 904... Loss: 0.00031... Val Loss: 0.00209\n",
      "EarlyStopping counter: 476 out of 30\n",
      "Epoch: 905/2000... Step: 905... Loss: 0.00034... Val Loss: 0.00210\n",
      "EarlyStopping counter: 477 out of 30\n",
      "Epoch: 906/2000... Step: 906... Loss: 0.00042... Val Loss: 0.00209\n",
      "EarlyStopping counter: 478 out of 30\n",
      "Epoch: 907/2000... Step: 907... Loss: 0.00046... Val Loss: 0.00219\n",
      "EarlyStopping counter: 479 out of 30\n",
      "Epoch: 908/2000... Step: 908... Loss: 0.00051... Val Loss: 0.00234\n",
      "EarlyStopping counter: 480 out of 30\n",
      "Epoch: 909/2000... Step: 909... Loss: 0.00059... Val Loss: 0.00239\n",
      "EarlyStopping counter: 481 out of 30\n",
      "Epoch: 910/2000... Step: 910... Loss: 0.00081... Val Loss: 0.00329\n",
      "EarlyStopping counter: 482 out of 30\n",
      "Epoch: 911/2000... Step: 911... Loss: 0.00128... Val Loss: 0.00326\n",
      "EarlyStopping counter: 483 out of 30\n",
      "Epoch: 912/2000... Step: 912... Loss: 0.00193... Val Loss: 0.00537\n",
      "EarlyStopping counter: 484 out of 30\n",
      "Epoch: 913/2000... Step: 913... Loss: 0.00309... Val Loss: 0.00558\n",
      "EarlyStopping counter: 485 out of 30\n",
      "Epoch: 914/2000... Step: 914... Loss: 0.00439... Val Loss: 0.00819\n",
      "EarlyStopping counter: 486 out of 30\n",
      "Epoch: 915/2000... Step: 915... Loss: 0.00616... Val Loss: 0.00717\n",
      "EarlyStopping counter: 487 out of 30\n",
      "Epoch: 916/2000... Step: 916... Loss: 0.00593... Val Loss: 0.00604\n",
      "EarlyStopping counter: 488 out of 30\n",
      "Epoch: 917/2000... Step: 917... Loss: 0.00438... Val Loss: 0.00359\n",
      "EarlyStopping counter: 489 out of 30\n",
      "Epoch: 918/2000... Step: 918... Loss: 0.00153... Val Loss: 0.00205\n",
      "EarlyStopping counter: 490 out of 30\n",
      "Epoch: 919/2000... Step: 919... Loss: 0.00084... Val Loss: 0.00386\n",
      "EarlyStopping counter: 491 out of 30\n",
      "Epoch: 920/2000... Step: 920... Loss: 0.00222... Val Loss: 0.00494\n",
      "EarlyStopping counter: 492 out of 30\n",
      "Epoch: 921/2000... Step: 921... Loss: 0.00304... Val Loss: 0.00389\n",
      "EarlyStopping counter: 493 out of 30\n",
      "Epoch: 922/2000... Step: 922... Loss: 0.00246... Val Loss: 0.00354\n",
      "EarlyStopping counter: 494 out of 30\n",
      "Epoch: 923/2000... Step: 923... Loss: 0.00127... Val Loss: 0.00218\n",
      "EarlyStopping counter: 495 out of 30\n",
      "Epoch: 924/2000... Step: 924... Loss: 0.00075... Val Loss: 0.00308\n",
      "EarlyStopping counter: 496 out of 30\n",
      "Epoch: 925/2000... Step: 925... Loss: 0.00135... Val Loss: 0.00410\n",
      "EarlyStopping counter: 497 out of 30\n",
      "Epoch: 926/2000... Step: 926... Loss: 0.00193... Val Loss: 0.00321\n",
      "EarlyStopping counter: 498 out of 30\n",
      "Epoch: 927/2000... Step: 927... Loss: 0.00175... Val Loss: 0.00287\n",
      "EarlyStopping counter: 499 out of 30\n",
      "Epoch: 928/2000... Step: 928... Loss: 0.00119... Val Loss: 0.00249\n",
      "EarlyStopping counter: 500 out of 30\n",
      "Epoch: 929/2000... Step: 929... Loss: 0.00060... Val Loss: 0.00250\n",
      "EarlyStopping counter: 501 out of 30\n",
      "Epoch: 930/2000... Step: 930... Loss: 0.00083... Val Loss: 0.00324\n",
      "EarlyStopping counter: 502 out of 30\n",
      "Epoch: 931/2000... Step: 931... Loss: 0.00113... Val Loss: 0.00245\n",
      "EarlyStopping counter: 503 out of 30\n",
      "Epoch: 932/2000... Step: 932... Loss: 0.00102... Val Loss: 0.00242\n",
      "EarlyStopping counter: 504 out of 30\n",
      "Epoch: 933/2000... Step: 933... Loss: 0.00063... Val Loss: 0.00273\n",
      "EarlyStopping counter: 505 out of 30\n",
      "Epoch: 934/2000... Step: 934... Loss: 0.00061... Val Loss: 0.00220\n",
      "EarlyStopping counter: 506 out of 30\n",
      "Epoch: 935/2000... Step: 935... Loss: 0.00074... Val Loss: 0.00269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 507 out of 30\n",
      "Epoch: 936/2000... Step: 936... Loss: 0.00058... Val Loss: 0.00259\n",
      "EarlyStopping counter: 508 out of 30\n",
      "Epoch: 937/2000... Step: 937... Loss: 0.00055... Val Loss: 0.00213\n",
      "EarlyStopping counter: 509 out of 30\n",
      "Epoch: 938/2000... Step: 938... Loss: 0.00061... Val Loss: 0.00252\n",
      "EarlyStopping counter: 510 out of 30\n",
      "Epoch: 939/2000... Step: 939... Loss: 0.00053... Val Loss: 0.00222\n",
      "EarlyStopping counter: 511 out of 30\n",
      "Epoch: 940/2000... Step: 940... Loss: 0.00047... Val Loss: 0.00221\n",
      "EarlyStopping counter: 512 out of 30\n",
      "Epoch: 941/2000... Step: 941... Loss: 0.00042... Val Loss: 0.00229\n",
      "EarlyStopping counter: 513 out of 30\n",
      "Epoch: 942/2000... Step: 942... Loss: 0.00048... Val Loss: 0.00202\n",
      "EarlyStopping counter: 514 out of 30\n",
      "Epoch: 943/2000... Step: 943... Loss: 0.00045... Val Loss: 0.00222\n",
      "EarlyStopping counter: 515 out of 30\n",
      "Epoch: 944/2000... Step: 944... Loss: 0.00039... Val Loss: 0.00211\n",
      "EarlyStopping counter: 516 out of 30\n",
      "Epoch: 945/2000... Step: 945... Loss: 0.00036... Val Loss: 0.00212\n",
      "EarlyStopping counter: 517 out of 30\n",
      "Epoch: 946/2000... Step: 946... Loss: 0.00036... Val Loss: 0.00220\n",
      "EarlyStopping counter: 518 out of 30\n",
      "Epoch: 947/2000... Step: 947... Loss: 0.00037... Val Loss: 0.00207\n",
      "EarlyStopping counter: 519 out of 30\n",
      "Epoch: 948/2000... Step: 948... Loss: 0.00030... Val Loss: 0.00208\n",
      "EarlyStopping counter: 520 out of 30\n",
      "Epoch: 949/2000... Step: 949... Loss: 0.00031... Val Loss: 0.00222\n",
      "EarlyStopping counter: 521 out of 30\n",
      "Epoch: 950/2000... Step: 950... Loss: 0.00032... Val Loss: 0.00200\n",
      "EarlyStopping counter: 522 out of 30\n",
      "Epoch: 951/2000... Step: 951... Loss: 0.00029... Val Loss: 0.00200\n",
      "EarlyStopping counter: 523 out of 30\n",
      "Epoch: 952/2000... Step: 952... Loss: 0.00030... Val Loss: 0.00218\n",
      "EarlyStopping counter: 524 out of 30\n",
      "Epoch: 953/2000... Step: 953... Loss: 0.00030... Val Loss: 0.00199\n",
      "EarlyStopping counter: 525 out of 30\n",
      "Epoch: 954/2000... Step: 954... Loss: 0.00031... Val Loss: 0.00221\n",
      "EarlyStopping counter: 526 out of 30\n",
      "Epoch: 955/2000... Step: 955... Loss: 0.00031... Val Loss: 0.00195\n",
      "EarlyStopping counter: 527 out of 30\n",
      "Epoch: 956/2000... Step: 956... Loss: 0.00029... Val Loss: 0.00216\n",
      "EarlyStopping counter: 528 out of 30\n",
      "Epoch: 957/2000... Step: 957... Loss: 0.00028... Val Loss: 0.00208\n",
      "EarlyStopping counter: 529 out of 30\n",
      "Epoch: 958/2000... Step: 958... Loss: 0.00027... Val Loss: 0.00196\n",
      "EarlyStopping counter: 530 out of 30\n",
      "Epoch: 959/2000... Step: 959... Loss: 0.00027... Val Loss: 0.00216\n",
      "EarlyStopping counter: 531 out of 30\n",
      "Epoch: 960/2000... Step: 960... Loss: 0.00025... Val Loss: 0.00202\n",
      "EarlyStopping counter: 532 out of 30\n",
      "Epoch: 961/2000... Step: 961... Loss: 0.00025... Val Loss: 0.00205\n",
      "EarlyStopping counter: 533 out of 30\n",
      "Epoch: 962/2000... Step: 962... Loss: 0.00026... Val Loss: 0.00203\n",
      "EarlyStopping counter: 534 out of 30\n",
      "Epoch: 963/2000... Step: 963... Loss: 0.00026... Val Loss: 0.00211\n",
      "EarlyStopping counter: 535 out of 30\n",
      "Epoch: 964/2000... Step: 964... Loss: 0.00026... Val Loss: 0.00198\n",
      "EarlyStopping counter: 536 out of 30\n",
      "Epoch: 965/2000... Step: 965... Loss: 0.00026... Val Loss: 0.00213\n",
      "EarlyStopping counter: 537 out of 30\n",
      "Epoch: 966/2000... Step: 966... Loss: 0.00027... Val Loss: 0.00201\n",
      "EarlyStopping counter: 538 out of 30\n",
      "Epoch: 967/2000... Step: 967... Loss: 0.00030... Val Loss: 0.00223\n",
      "EarlyStopping counter: 539 out of 30\n",
      "Epoch: 968/2000... Step: 968... Loss: 0.00034... Val Loss: 0.00211\n",
      "EarlyStopping counter: 540 out of 30\n",
      "Epoch: 969/2000... Step: 969... Loss: 0.00039... Val Loss: 0.00230\n",
      "EarlyStopping counter: 541 out of 30\n",
      "Epoch: 970/2000... Step: 970... Loss: 0.00048... Val Loss: 0.00232\n",
      "EarlyStopping counter: 542 out of 30\n",
      "Epoch: 971/2000... Step: 971... Loss: 0.00055... Val Loss: 0.00225\n",
      "EarlyStopping counter: 543 out of 30\n",
      "Epoch: 972/2000... Step: 972... Loss: 0.00067... Val Loss: 0.00262\n",
      "EarlyStopping counter: 544 out of 30\n",
      "Epoch: 973/2000... Step: 973... Loss: 0.00073... Val Loss: 0.00239\n",
      "EarlyStopping counter: 545 out of 30\n",
      "Epoch: 974/2000... Step: 974... Loss: 0.00084... Val Loss: 0.00295\n",
      "EarlyStopping counter: 546 out of 30\n",
      "Epoch: 975/2000... Step: 975... Loss: 0.00089... Val Loss: 0.00264\n",
      "EarlyStopping counter: 547 out of 30\n",
      "Epoch: 976/2000... Step: 976... Loss: 0.00102... Val Loss: 0.00346\n",
      "EarlyStopping counter: 548 out of 30\n",
      "Epoch: 977/2000... Step: 977... Loss: 0.00119... Val Loss: 0.00279\n",
      "EarlyStopping counter: 549 out of 30\n",
      "Epoch: 978/2000... Step: 978... Loss: 0.00138... Val Loss: 0.00391\n",
      "EarlyStopping counter: 550 out of 30\n",
      "Epoch: 979/2000... Step: 979... Loss: 0.00156... Val Loss: 0.00298\n",
      "EarlyStopping counter: 551 out of 30\n",
      "Epoch: 980/2000... Step: 980... Loss: 0.00164... Val Loss: 0.00374\n",
      "EarlyStopping counter: 552 out of 30\n",
      "Epoch: 981/2000... Step: 981... Loss: 0.00175... Val Loss: 0.00297\n",
      "EarlyStopping counter: 553 out of 30\n",
      "Epoch: 982/2000... Step: 982... Loss: 0.00167... Val Loss: 0.00319\n",
      "EarlyStopping counter: 554 out of 30\n",
      "Epoch: 983/2000... Step: 983... Loss: 0.00160... Val Loss: 0.00284\n",
      "EarlyStopping counter: 555 out of 30\n",
      "Epoch: 984/2000... Step: 984... Loss: 0.00115... Val Loss: 0.00226\n",
      "EarlyStopping counter: 556 out of 30\n",
      "Epoch: 985/2000... Step: 985... Loss: 0.00067... Val Loss: 0.00255\n",
      "EarlyStopping counter: 557 out of 30\n",
      "Epoch: 986/2000... Step: 986... Loss: 0.00047... Val Loss: 0.00195\n",
      "EarlyStopping counter: 558 out of 30\n",
      "Epoch: 987/2000... Step: 987... Loss: 0.00059... Val Loss: 0.00296\n",
      "EarlyStopping counter: 559 out of 30\n",
      "Epoch: 988/2000... Step: 988... Loss: 0.00077... Val Loss: 0.00240\n",
      "EarlyStopping counter: 560 out of 30\n",
      "Epoch: 989/2000... Step: 989... Loss: 0.00079... Val Loss: 0.00270\n",
      "EarlyStopping counter: 561 out of 30\n",
      "Epoch: 990/2000... Step: 990... Loss: 0.00079... Val Loss: 0.00271\n",
      "EarlyStopping counter: 562 out of 30\n",
      "Epoch: 991/2000... Step: 991... Loss: 0.00070... Val Loss: 0.00226\n",
      "EarlyStopping counter: 563 out of 30\n",
      "Epoch: 992/2000... Step: 992... Loss: 0.00058... Val Loss: 0.00231\n",
      "EarlyStopping counter: 564 out of 30\n",
      "Epoch: 993/2000... Step: 993... Loss: 0.00043... Val Loss: 0.00231\n",
      "EarlyStopping counter: 565 out of 30\n",
      "Epoch: 994/2000... Step: 994... Loss: 0.00042... Val Loss: 0.00216\n",
      "EarlyStopping counter: 566 out of 30\n",
      "Epoch: 995/2000... Step: 995... Loss: 0.00056... Val Loss: 0.00249\n",
      "EarlyStopping counter: 567 out of 30\n",
      "Epoch: 996/2000... Step: 996... Loss: 0.00055... Val Loss: 0.00220\n",
      "EarlyStopping counter: 568 out of 30\n",
      "Epoch: 997/2000... Step: 997... Loss: 0.00042... Val Loss: 0.00210\n",
      "EarlyStopping counter: 569 out of 30\n",
      "Epoch: 998/2000... Step: 998... Loss: 0.00028... Val Loss: 0.00233\n",
      "EarlyStopping counter: 570 out of 30\n",
      "Epoch: 999/2000... Step: 999... Loss: 0.00028... Val Loss: 0.00192\n",
      "EarlyStopping counter: 571 out of 30\n",
      "Epoch: 1000/2000... Step: 1000... Loss: 0.00036... Val Loss: 0.00235\n",
      "EarlyStopping counter: 572 out of 30\n",
      "Epoch: 1001/2000... Step: 1001... Loss: 0.00039... Val Loss: 0.00207\n",
      "EarlyStopping counter: 573 out of 30\n",
      "Epoch: 1002/2000... Step: 1002... Loss: 0.00038... Val Loss: 0.00215\n",
      "EarlyStopping counter: 574 out of 30\n",
      "Epoch: 1003/2000... Step: 1003... Loss: 0.00036... Val Loss: 0.00227\n",
      "EarlyStopping counter: 575 out of 30\n",
      "Epoch: 1004/2000... Step: 1004... Loss: 0.00036... Val Loss: 0.00206\n",
      "EarlyStopping counter: 576 out of 30\n",
      "Epoch: 1005/2000... Step: 1005... Loss: 0.00035... Val Loss: 0.00225\n",
      "EarlyStopping counter: 577 out of 30\n",
      "Epoch: 1006/2000... Step: 1006... Loss: 0.00034... Val Loss: 0.00217\n",
      "EarlyStopping counter: 578 out of 30\n",
      "Epoch: 1007/2000... Step: 1007... Loss: 0.00037... Val Loss: 0.00206\n",
      "EarlyStopping counter: 579 out of 30\n",
      "Epoch: 1008/2000... Step: 1008... Loss: 0.00039... Val Loss: 0.00242\n",
      "EarlyStopping counter: 580 out of 30\n",
      "Epoch: 1009/2000... Step: 1009... Loss: 0.00042... Val Loss: 0.00201\n",
      "EarlyStopping counter: 581 out of 30\n",
      "Epoch: 1010/2000... Step: 1010... Loss: 0.00044... Val Loss: 0.00263\n",
      "EarlyStopping counter: 582 out of 30\n",
      "Epoch: 1011/2000... Step: 1011... Loss: 0.00051... Val Loss: 0.00223\n",
      "EarlyStopping counter: 583 out of 30\n",
      "Epoch: 1012/2000... Step: 1012... Loss: 0.00062... Val Loss: 0.00287\n",
      "EarlyStopping counter: 584 out of 30\n",
      "Epoch: 1013/2000... Step: 1013... Loss: 0.00076... Val Loss: 0.00259\n",
      "EarlyStopping counter: 585 out of 30\n",
      "Epoch: 1014/2000... Step: 1014... Loss: 0.00096... Val Loss: 0.00336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 586 out of 30\n",
      "Epoch: 1015/2000... Step: 1015... Loss: 0.00123... Val Loss: 0.00312\n",
      "EarlyStopping counter: 587 out of 30\n",
      "Epoch: 1016/2000... Step: 1016... Loss: 0.00162... Val Loss: 0.00421\n",
      "EarlyStopping counter: 588 out of 30\n",
      "Epoch: 1017/2000... Step: 1017... Loss: 0.00224... Val Loss: 0.00413\n",
      "EarlyStopping counter: 589 out of 30\n",
      "Epoch: 1018/2000... Step: 1018... Loss: 0.00276... Val Loss: 0.00503\n",
      "EarlyStopping counter: 590 out of 30\n",
      "Epoch: 1019/2000... Step: 1019... Loss: 0.00323... Val Loss: 0.00474\n",
      "EarlyStopping counter: 591 out of 30\n",
      "Epoch: 1020/2000... Step: 1020... Loss: 0.00298... Val Loss: 0.00421\n",
      "EarlyStopping counter: 592 out of 30\n",
      "Epoch: 1021/2000... Step: 1021... Loss: 0.00233... Val Loss: 0.00307\n",
      "EarlyStopping counter: 593 out of 30\n",
      "Epoch: 1022/2000... Step: 1022... Loss: 0.00112... Val Loss: 0.00231\n",
      "EarlyStopping counter: 594 out of 30\n",
      "Epoch: 1023/2000... Step: 1023... Loss: 0.00050... Val Loss: 0.00221\n",
      "EarlyStopping counter: 595 out of 30\n",
      "Epoch: 1024/2000... Step: 1024... Loss: 0.00063... Val Loss: 0.00328\n",
      "EarlyStopping counter: 596 out of 30\n",
      "Epoch: 1025/2000... Step: 1025... Loss: 0.00120... Val Loss: 0.00337\n",
      "EarlyStopping counter: 597 out of 30\n",
      "Epoch: 1026/2000... Step: 1026... Loss: 0.00168... Val Loss: 0.00359\n",
      "EarlyStopping counter: 598 out of 30\n",
      "Epoch: 1027/2000... Step: 1027... Loss: 0.00153... Val Loss: 0.00302\n",
      "EarlyStopping counter: 599 out of 30\n",
      "Epoch: 1028/2000... Step: 1028... Loss: 0.00106... Val Loss: 0.00217\n",
      "EarlyStopping counter: 600 out of 30\n",
      "Epoch: 1029/2000... Step: 1029... Loss: 0.00063... Val Loss: 0.00299\n",
      "EarlyStopping counter: 601 out of 30\n",
      "Epoch: 1030/2000... Step: 1030... Loss: 0.00062... Val Loss: 0.00221\n",
      "EarlyStopping counter: 602 out of 30\n",
      "Epoch: 1031/2000... Step: 1031... Loss: 0.00080... Val Loss: 0.00308\n",
      "EarlyStopping counter: 603 out of 30\n",
      "Epoch: 1032/2000... Step: 1032... Loss: 0.00082... Val Loss: 0.00243\n",
      "EarlyStopping counter: 604 out of 30\n",
      "Epoch: 1033/2000... Step: 1033... Loss: 0.00068... Val Loss: 0.00224\n",
      "EarlyStopping counter: 605 out of 30\n",
      "Epoch: 1034/2000... Step: 1034... Loss: 0.00056... Val Loss: 0.00288\n",
      "EarlyStopping counter: 606 out of 30\n",
      "Epoch: 1035/2000... Step: 1035... Loss: 0.00058... Val Loss: 0.00200\n",
      "EarlyStopping counter: 607 out of 30\n",
      "Epoch: 1036/2000... Step: 1036... Loss: 0.00056... Val Loss: 0.00286\n",
      "EarlyStopping counter: 608 out of 30\n",
      "Epoch: 1037/2000... Step: 1037... Loss: 0.00055... Val Loss: 0.00218\n",
      "EarlyStopping counter: 609 out of 30\n",
      "Epoch: 1038/2000... Step: 1038... Loss: 0.00052... Val Loss: 0.00234\n",
      "EarlyStopping counter: 610 out of 30\n",
      "Epoch: 1039/2000... Step: 1039... Loss: 0.00043... Val Loss: 0.00235\n",
      "EarlyStopping counter: 611 out of 30\n",
      "Epoch: 1040/2000... Step: 1040... Loss: 0.00033... Val Loss: 0.00199\n",
      "EarlyStopping counter: 612 out of 30\n",
      "Epoch: 1041/2000... Step: 1041... Loss: 0.00033... Val Loss: 0.00262\n",
      "EarlyStopping counter: 613 out of 30\n",
      "Epoch: 1042/2000... Step: 1042... Loss: 0.00048... Val Loss: 0.00215\n",
      "EarlyStopping counter: 614 out of 30\n",
      "Epoch: 1043/2000... Step: 1043... Loss: 0.00058... Val Loss: 0.00265\n",
      "EarlyStopping counter: 615 out of 30\n",
      "Epoch: 1044/2000... Step: 1044... Loss: 0.00051... Val Loss: 0.00191\n",
      "EarlyStopping counter: 616 out of 30\n",
      "Epoch: 1045/2000... Step: 1045... Loss: 0.00036... Val Loss: 0.00232\n",
      "EarlyStopping counter: 617 out of 30\n",
      "Epoch: 1046/2000... Step: 1046... Loss: 0.00029... Val Loss: 0.00208\n",
      "EarlyStopping counter: 618 out of 30\n",
      "Epoch: 1047/2000... Step: 1047... Loss: 0.00030... Val Loss: 0.00211\n",
      "EarlyStopping counter: 619 out of 30\n",
      "Epoch: 1048/2000... Step: 1048... Loss: 0.00032... Val Loss: 0.00236\n",
      "EarlyStopping counter: 620 out of 30\n",
      "Epoch: 1049/2000... Step: 1049... Loss: 0.00033... Val Loss: 0.00202\n",
      "EarlyStopping counter: 621 out of 30\n",
      "Epoch: 1050/2000... Step: 1050... Loss: 0.00034... Val Loss: 0.00236\n",
      "EarlyStopping counter: 622 out of 30\n",
      "Epoch: 1051/2000... Step: 1051... Loss: 0.00033... Val Loss: 0.00200\n",
      "EarlyStopping counter: 623 out of 30\n",
      "Epoch: 1052/2000... Step: 1052... Loss: 0.00029... Val Loss: 0.00216\n",
      "EarlyStopping counter: 624 out of 30\n",
      "Epoch: 1053/2000... Step: 1053... Loss: 0.00025... Val Loss: 0.00216\n",
      "EarlyStopping counter: 625 out of 30\n",
      "Epoch: 1054/2000... Step: 1054... Loss: 0.00023... Val Loss: 0.00193\n",
      "EarlyStopping counter: 626 out of 30\n",
      "Epoch: 1055/2000... Step: 1055... Loss: 0.00026... Val Loss: 0.00237\n",
      "EarlyStopping counter: 627 out of 30\n",
      "Epoch: 1056/2000... Step: 1056... Loss: 0.00028... Val Loss: 0.00198\n",
      "EarlyStopping counter: 628 out of 30\n",
      "Epoch: 1057/2000... Step: 1057... Loss: 0.00027... Val Loss: 0.00224\n",
      "EarlyStopping counter: 629 out of 30\n",
      "Epoch: 1058/2000... Step: 1058... Loss: 0.00027... Val Loss: 0.00207\n",
      "EarlyStopping counter: 630 out of 30\n",
      "Epoch: 1059/2000... Step: 1059... Loss: 0.00027... Val Loss: 0.00211\n",
      "EarlyStopping counter: 631 out of 30\n",
      "Epoch: 1060/2000... Step: 1060... Loss: 0.00025... Val Loss: 0.00214\n",
      "EarlyStopping counter: 632 out of 30\n",
      "Epoch: 1061/2000... Step: 1061... Loss: 0.00023... Val Loss: 0.00204\n",
      "EarlyStopping counter: 633 out of 30\n",
      "Epoch: 1062/2000... Step: 1062... Loss: 0.00023... Val Loss: 0.00218\n",
      "EarlyStopping counter: 634 out of 30\n",
      "Epoch: 1063/2000... Step: 1063... Loss: 0.00023... Val Loss: 0.00200\n",
      "EarlyStopping counter: 635 out of 30\n",
      "Epoch: 1064/2000... Step: 1064... Loss: 0.00024... Val Loss: 0.00222\n",
      "EarlyStopping counter: 636 out of 30\n",
      "Epoch: 1065/2000... Step: 1065... Loss: 0.00025... Val Loss: 0.00200\n",
      "EarlyStopping counter: 637 out of 30\n",
      "Epoch: 1066/2000... Step: 1066... Loss: 0.00025... Val Loss: 0.00229\n",
      "EarlyStopping counter: 638 out of 30\n",
      "Epoch: 1067/2000... Step: 1067... Loss: 0.00026... Val Loss: 0.00199\n",
      "EarlyStopping counter: 639 out of 30\n",
      "Epoch: 1068/2000... Step: 1068... Loss: 0.00027... Val Loss: 0.00235\n",
      "EarlyStopping counter: 640 out of 30\n",
      "Epoch: 1069/2000... Step: 1069... Loss: 0.00030... Val Loss: 0.00203\n",
      "EarlyStopping counter: 641 out of 30\n",
      "Epoch: 1070/2000... Step: 1070... Loss: 0.00032... Val Loss: 0.00249\n",
      "EarlyStopping counter: 642 out of 30\n",
      "Epoch: 1071/2000... Step: 1071... Loss: 0.00038... Val Loss: 0.00218\n",
      "EarlyStopping counter: 643 out of 30\n",
      "Epoch: 1072/2000... Step: 1072... Loss: 0.00049... Val Loss: 0.00292\n",
      "EarlyStopping counter: 644 out of 30\n",
      "Epoch: 1073/2000... Step: 1073... Loss: 0.00072... Val Loss: 0.00269\n",
      "EarlyStopping counter: 645 out of 30\n",
      "Epoch: 1074/2000... Step: 1074... Loss: 0.00111... Val Loss: 0.00437\n",
      "EarlyStopping counter: 646 out of 30\n",
      "Epoch: 1075/2000... Step: 1075... Loss: 0.00190... Val Loss: 0.00442\n",
      "EarlyStopping counter: 647 out of 30\n",
      "Epoch: 1076/2000... Step: 1076... Loss: 0.00317... Val Loss: 0.00808\n",
      "EarlyStopping counter: 648 out of 30\n",
      "Epoch: 1077/2000... Step: 1077... Loss: 0.00534... Val Loss: 0.00842\n",
      "EarlyStopping counter: 649 out of 30\n",
      "Epoch: 1078/2000... Step: 1078... Loss: 0.00762... Val Loss: 0.01112\n",
      "EarlyStopping counter: 650 out of 30\n",
      "Epoch: 1079/2000... Step: 1079... Loss: 0.00857... Val Loss: 0.00687\n",
      "EarlyStopping counter: 651 out of 30\n",
      "Epoch: 1080/2000... Step: 1080... Loss: 0.00524... Val Loss: 0.00305\n",
      "EarlyStopping counter: 652 out of 30\n",
      "Epoch: 1081/2000... Step: 1081... Loss: 0.00176... Val Loss: 0.00423\n",
      "EarlyStopping counter: 653 out of 30\n",
      "Epoch: 1082/2000... Step: 1082... Loss: 0.00193... Val Loss: 0.00562\n",
      "EarlyStopping counter: 654 out of 30\n",
      "Epoch: 1083/2000... Step: 1083... Loss: 0.00413... Val Loss: 0.00555\n",
      "EarlyStopping counter: 655 out of 30\n",
      "Epoch: 1084/2000... Step: 1084... Loss: 0.00411... Val Loss: 0.00499\n",
      "EarlyStopping counter: 656 out of 30\n",
      "Epoch: 1085/2000... Step: 1085... Loss: 0.00266... Val Loss: 0.00285\n",
      "EarlyStopping counter: 657 out of 30\n",
      "Epoch: 1086/2000... Step: 1086... Loss: 0.00200... Val Loss: 0.00431\n",
      "EarlyStopping counter: 658 out of 30\n",
      "Epoch: 1087/2000... Step: 1087... Loss: 0.00204... Val Loss: 0.00411\n",
      "EarlyStopping counter: 659 out of 30\n",
      "Epoch: 1088/2000... Step: 1088... Loss: 0.00209... Val Loss: 0.00299\n",
      "EarlyStopping counter: 660 out of 30\n",
      "Epoch: 1089/2000... Step: 1089... Loss: 0.00186... Val Loss: 0.00356\n",
      "EarlyStopping counter: 661 out of 30\n",
      "Epoch: 1090/2000... Step: 1090... Loss: 0.00141... Val Loss: 0.00253\n",
      "EarlyStopping counter: 662 out of 30\n",
      "Epoch: 1091/2000... Step: 1091... Loss: 0.00137... Val Loss: 0.00368\n",
      "EarlyStopping counter: 663 out of 30\n",
      "Epoch: 1092/2000... Step: 1092... Loss: 0.00179... Val Loss: 0.00390\n",
      "EarlyStopping counter: 664 out of 30\n",
      "Epoch: 1093/2000... Step: 1093... Loss: 0.00174... Val Loss: 0.00215\n",
      "EarlyStopping counter: 665 out of 30\n",
      "Epoch: 1094/2000... Step: 1094... Loss: 0.00081... Val Loss: 0.00267\n",
      "EarlyStopping counter: 666 out of 30\n",
      "Epoch: 1095/2000... Step: 1095... Loss: 0.00080... Val Loss: 0.00343\n",
      "EarlyStopping counter: 667 out of 30\n",
      "Epoch: 1096/2000... Step: 1096... Loss: 0.00151... Val Loss: 0.00312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 668 out of 30\n",
      "Epoch: 1097/2000... Step: 1097... Loss: 0.00138... Val Loss: 0.00245\n",
      "EarlyStopping counter: 669 out of 30\n",
      "Epoch: 1098/2000... Step: 1098... Loss: 0.00061... Val Loss: 0.00239\n",
      "EarlyStopping counter: 670 out of 30\n",
      "Epoch: 1099/2000... Step: 1099... Loss: 0.00058... Val Loss: 0.00276\n",
      "EarlyStopping counter: 671 out of 30\n",
      "Epoch: 1100/2000... Step: 1100... Loss: 0.00106... Val Loss: 0.00286\n",
      "EarlyStopping counter: 672 out of 30\n",
      "Epoch: 1101/2000... Step: 1101... Loss: 0.00092... Val Loss: 0.00274\n",
      "EarlyStopping counter: 673 out of 30\n",
      "Epoch: 1102/2000... Step: 1102... Loss: 0.00060... Val Loss: 0.00220\n",
      "EarlyStopping counter: 674 out of 30\n",
      "Epoch: 1103/2000... Step: 1103... Loss: 0.00061... Val Loss: 0.00274\n",
      "EarlyStopping counter: 675 out of 30\n",
      "Epoch: 1104/2000... Step: 1104... Loss: 0.00069... Val Loss: 0.00238\n",
      "EarlyStopping counter: 676 out of 30\n",
      "Epoch: 1105/2000... Step: 1105... Loss: 0.00055... Val Loss: 0.00203\n",
      "EarlyStopping counter: 677 out of 30\n",
      "Epoch: 1106/2000... Step: 1106... Loss: 0.00056... Val Loss: 0.00270\n",
      "EarlyStopping counter: 678 out of 30\n",
      "Epoch: 1107/2000... Step: 1107... Loss: 0.00066... Val Loss: 0.00238\n",
      "EarlyStopping counter: 679 out of 30\n",
      "Epoch: 1108/2000... Step: 1108... Loss: 0.00053... Val Loss: 0.00201\n",
      "EarlyStopping counter: 680 out of 30\n",
      "Epoch: 1109/2000... Step: 1109... Loss: 0.00040... Val Loss: 0.00248\n",
      "EarlyStopping counter: 681 out of 30\n",
      "Epoch: 1110/2000... Step: 1110... Loss: 0.00044... Val Loss: 0.00201\n",
      "EarlyStopping counter: 682 out of 30\n",
      "Epoch: 1111/2000... Step: 1111... Loss: 0.00048... Val Loss: 0.00237\n",
      "EarlyStopping counter: 683 out of 30\n",
      "Epoch: 1112/2000... Step: 1112... Loss: 0.00037... Val Loss: 0.00227\n",
      "EarlyStopping counter: 684 out of 30\n",
      "Epoch: 1113/2000... Step: 1113... Loss: 0.00034... Val Loss: 0.00213\n",
      "EarlyStopping counter: 685 out of 30\n",
      "Epoch: 1114/2000... Step: 1114... Loss: 0.00042... Val Loss: 0.00262\n",
      "EarlyStopping counter: 686 out of 30\n",
      "Epoch: 1115/2000... Step: 1115... Loss: 0.00041... Val Loss: 0.00198\n",
      "EarlyStopping counter: 687 out of 30\n",
      "Epoch: 1116/2000... Step: 1116... Loss: 0.00033... Val Loss: 0.00228\n",
      "EarlyStopping counter: 688 out of 30\n",
      "Epoch: 1117/2000... Step: 1117... Loss: 0.00031... Val Loss: 0.00230\n",
      "EarlyStopping counter: 689 out of 30\n",
      "Epoch: 1118/2000... Step: 1118... Loss: 0.00032... Val Loss: 0.00206\n",
      "EarlyStopping counter: 690 out of 30\n",
      "Epoch: 1119/2000... Step: 1119... Loss: 0.00033... Val Loss: 0.00232\n",
      "EarlyStopping counter: 691 out of 30\n",
      "Epoch: 1120/2000... Step: 1120... Loss: 0.00032... Val Loss: 0.00226\n",
      "EarlyStopping counter: 692 out of 30\n",
      "Epoch: 1121/2000... Step: 1121... Loss: 0.00031... Val Loss: 0.00228\n",
      "EarlyStopping counter: 693 out of 30\n",
      "Epoch: 1122/2000... Step: 1122... Loss: 0.00030... Val Loss: 0.00215\n",
      "EarlyStopping counter: 694 out of 30\n",
      "Epoch: 1123/2000... Step: 1123... Loss: 0.00025... Val Loss: 0.00209\n",
      "EarlyStopping counter: 695 out of 30\n",
      "Epoch: 1124/2000... Step: 1124... Loss: 0.00023... Val Loss: 0.00215\n",
      "EarlyStopping counter: 696 out of 30\n",
      "Epoch: 1125/2000... Step: 1125... Loss: 0.00027... Val Loss: 0.00213\n",
      "EarlyStopping counter: 697 out of 30\n",
      "Epoch: 1126/2000... Step: 1126... Loss: 0.00028... Val Loss: 0.00214\n",
      "EarlyStopping counter: 698 out of 30\n",
      "Epoch: 1127/2000... Step: 1127... Loss: 0.00026... Val Loss: 0.00216\n",
      "EarlyStopping counter: 699 out of 30\n",
      "Epoch: 1128/2000... Step: 1128... Loss: 0.00025... Val Loss: 0.00213\n",
      "EarlyStopping counter: 700 out of 30\n",
      "Epoch: 1129/2000... Step: 1129... Loss: 0.00025... Val Loss: 0.00224\n",
      "EarlyStopping counter: 701 out of 30\n",
      "Epoch: 1130/2000... Step: 1130... Loss: 0.00028... Val Loss: 0.00206\n",
      "EarlyStopping counter: 702 out of 30\n",
      "Epoch: 1131/2000... Step: 1131... Loss: 0.00033... Val Loss: 0.00257\n",
      "EarlyStopping counter: 703 out of 30\n",
      "Epoch: 1132/2000... Step: 1132... Loss: 0.00042... Val Loss: 0.00211\n",
      "EarlyStopping counter: 704 out of 30\n",
      "Epoch: 1133/2000... Step: 1133... Loss: 0.00051... Val Loss: 0.00264\n",
      "EarlyStopping counter: 705 out of 30\n",
      "Epoch: 1134/2000... Step: 1134... Loss: 0.00063... Val Loss: 0.00234\n",
      "EarlyStopping counter: 706 out of 30\n",
      "Epoch: 1135/2000... Step: 1135... Loss: 0.00069... Val Loss: 0.00274\n",
      "EarlyStopping counter: 707 out of 30\n",
      "Epoch: 1136/2000... Step: 1136... Loss: 0.00080... Val Loss: 0.00247\n",
      "EarlyStopping counter: 708 out of 30\n",
      "Epoch: 1137/2000... Step: 1137... Loss: 0.00076... Val Loss: 0.00257\n",
      "EarlyStopping counter: 709 out of 30\n",
      "Epoch: 1138/2000... Step: 1138... Loss: 0.00069... Val Loss: 0.00236\n",
      "EarlyStopping counter: 710 out of 30\n",
      "Epoch: 1139/2000... Step: 1139... Loss: 0.00047... Val Loss: 0.00216\n",
      "EarlyStopping counter: 711 out of 30\n",
      "Epoch: 1140/2000... Step: 1140... Loss: 0.00035... Val Loss: 0.00240\n",
      "EarlyStopping counter: 712 out of 30\n",
      "Epoch: 1141/2000... Step: 1141... Loss: 0.00036... Val Loss: 0.00211\n",
      "EarlyStopping counter: 713 out of 30\n",
      "Epoch: 1142/2000... Step: 1142... Loss: 0.00043... Val Loss: 0.00256\n",
      "EarlyStopping counter: 714 out of 30\n",
      "Epoch: 1143/2000... Step: 1143... Loss: 0.00048... Val Loss: 0.00206\n",
      "EarlyStopping counter: 715 out of 30\n",
      "Epoch: 1144/2000... Step: 1144... Loss: 0.00037... Val Loss: 0.00233\n",
      "EarlyStopping counter: 716 out of 30\n",
      "Epoch: 1145/2000... Step: 1145... Loss: 0.00027... Val Loss: 0.00209\n",
      "EarlyStopping counter: 717 out of 30\n",
      "Epoch: 1146/2000... Step: 1146... Loss: 0.00023... Val Loss: 0.00206\n",
      "EarlyStopping counter: 718 out of 30\n",
      "Epoch: 1147/2000... Step: 1147... Loss: 0.00026... Val Loss: 0.00242\n",
      "EarlyStopping counter: 719 out of 30\n",
      "Epoch: 1148/2000... Step: 1148... Loss: 0.00032... Val Loss: 0.00203\n",
      "EarlyStopping counter: 720 out of 30\n",
      "Epoch: 1149/2000... Step: 1149... Loss: 0.00032... Val Loss: 0.00233\n",
      "EarlyStopping counter: 721 out of 30\n",
      "Epoch: 1150/2000... Step: 1150... Loss: 0.00028... Val Loss: 0.00207\n",
      "EarlyStopping counter: 722 out of 30\n",
      "Epoch: 1151/2000... Step: 1151... Loss: 0.00024... Val Loss: 0.00215\n",
      "EarlyStopping counter: 723 out of 30\n",
      "Epoch: 1152/2000... Step: 1152... Loss: 0.00025... Val Loss: 0.00228\n",
      "EarlyStopping counter: 724 out of 30\n",
      "Epoch: 1153/2000... Step: 1153... Loss: 0.00029... Val Loss: 0.00215\n",
      "EarlyStopping counter: 725 out of 30\n",
      "Epoch: 1154/2000... Step: 1154... Loss: 0.00033... Val Loss: 0.00239\n",
      "EarlyStopping counter: 726 out of 30\n",
      "Epoch: 1155/2000... Step: 1155... Loss: 0.00036... Val Loss: 0.00223\n",
      "EarlyStopping counter: 727 out of 30\n",
      "Epoch: 1156/2000... Step: 1156... Loss: 0.00036... Val Loss: 0.00226\n",
      "EarlyStopping counter: 728 out of 30\n",
      "Epoch: 1157/2000... Step: 1157... Loss: 0.00036... Val Loss: 0.00237\n",
      "EarlyStopping counter: 729 out of 30\n",
      "Epoch: 1158/2000... Step: 1158... Loss: 0.00036... Val Loss: 0.00217\n",
      "EarlyStopping counter: 730 out of 30\n",
      "Epoch: 1159/2000... Step: 1159... Loss: 0.00038... Val Loss: 0.00259\n",
      "EarlyStopping counter: 731 out of 30\n",
      "Epoch: 1160/2000... Step: 1160... Loss: 0.00044... Val Loss: 0.00222\n",
      "EarlyStopping counter: 732 out of 30\n",
      "Epoch: 1161/2000... Step: 1161... Loss: 0.00050... Val Loss: 0.00283\n",
      "EarlyStopping counter: 733 out of 30\n",
      "Epoch: 1162/2000... Step: 1162... Loss: 0.00059... Val Loss: 0.00236\n",
      "EarlyStopping counter: 734 out of 30\n",
      "Epoch: 1163/2000... Step: 1163... Loss: 0.00064... Val Loss: 0.00295\n",
      "EarlyStopping counter: 735 out of 30\n",
      "Epoch: 1164/2000... Step: 1164... Loss: 0.00072... Val Loss: 0.00247\n",
      "EarlyStopping counter: 736 out of 30\n",
      "Epoch: 1165/2000... Step: 1165... Loss: 0.00077... Val Loss: 0.00300\n",
      "EarlyStopping counter: 737 out of 30\n",
      "Epoch: 1166/2000... Step: 1166... Loss: 0.00088... Val Loss: 0.00263\n",
      "EarlyStopping counter: 738 out of 30\n",
      "Epoch: 1167/2000... Step: 1167... Loss: 0.00095... Val Loss: 0.00305\n",
      "EarlyStopping counter: 739 out of 30\n",
      "Epoch: 1168/2000... Step: 1168... Loss: 0.00107... Val Loss: 0.00278\n",
      "EarlyStopping counter: 740 out of 30\n",
      "Epoch: 1169/2000... Step: 1169... Loss: 0.00108... Val Loss: 0.00296\n",
      "EarlyStopping counter: 741 out of 30\n",
      "Epoch: 1170/2000... Step: 1170... Loss: 0.00107... Val Loss: 0.00282\n",
      "EarlyStopping counter: 742 out of 30\n",
      "Epoch: 1171/2000... Step: 1171... Loss: 0.00090... Val Loss: 0.00259\n",
      "EarlyStopping counter: 743 out of 30\n",
      "Epoch: 1172/2000... Step: 1172... Loss: 0.00072... Val Loss: 0.00251\n",
      "EarlyStopping counter: 744 out of 30\n",
      "Epoch: 1173/2000... Step: 1173... Loss: 0.00051... Val Loss: 0.00216\n",
      "EarlyStopping counter: 745 out of 30\n",
      "Epoch: 1174/2000... Step: 1174... Loss: 0.00031... Val Loss: 0.00216\n",
      "EarlyStopping counter: 746 out of 30\n",
      "Epoch: 1175/2000... Step: 1175... Loss: 0.00021... Val Loss: 0.00222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 747 out of 30\n",
      "Epoch: 1176/2000... Step: 1176... Loss: 0.00024... Val Loss: 0.00220\n",
      "EarlyStopping counter: 748 out of 30\n",
      "Epoch: 1177/2000... Step: 1177... Loss: 0.00035... Val Loss: 0.00251\n",
      "EarlyStopping counter: 749 out of 30\n",
      "Epoch: 1178/2000... Step: 1178... Loss: 0.00046... Val Loss: 0.00239\n",
      "EarlyStopping counter: 750 out of 30\n",
      "Epoch: 1179/2000... Step: 1179... Loss: 0.00052... Val Loss: 0.00248\n",
      "EarlyStopping counter: 751 out of 30\n",
      "Epoch: 1180/2000... Step: 1180... Loss: 0.00052... Val Loss: 0.00244\n",
      "EarlyStopping counter: 752 out of 30\n",
      "Epoch: 1181/2000... Step: 1181... Loss: 0.00049... Val Loss: 0.00222\n",
      "EarlyStopping counter: 753 out of 30\n",
      "Epoch: 1182/2000... Step: 1182... Loss: 0.00042... Val Loss: 0.00247\n",
      "EarlyStopping counter: 754 out of 30\n",
      "Epoch: 1183/2000... Step: 1183... Loss: 0.00036... Val Loss: 0.00206\n",
      "EarlyStopping counter: 755 out of 30\n",
      "Epoch: 1184/2000... Step: 1184... Loss: 0.00031... Val Loss: 0.00250\n",
      "EarlyStopping counter: 756 out of 30\n",
      "Epoch: 1185/2000... Step: 1185... Loss: 0.00030... Val Loss: 0.00208\n",
      "EarlyStopping counter: 757 out of 30\n",
      "Epoch: 1186/2000... Step: 1186... Loss: 0.00030... Val Loss: 0.00248\n",
      "EarlyStopping counter: 758 out of 30\n",
      "Epoch: 1187/2000... Step: 1187... Loss: 0.00030... Val Loss: 0.00208\n",
      "EarlyStopping counter: 759 out of 30\n",
      "Epoch: 1188/2000... Step: 1188... Loss: 0.00029... Val Loss: 0.00244\n",
      "EarlyStopping counter: 760 out of 30\n",
      "Epoch: 1189/2000... Step: 1189... Loss: 0.00027... Val Loss: 0.00207\n",
      "EarlyStopping counter: 761 out of 30\n",
      "Epoch: 1190/2000... Step: 1190... Loss: 0.00025... Val Loss: 0.00226\n",
      "EarlyStopping counter: 762 out of 30\n",
      "Epoch: 1191/2000... Step: 1191... Loss: 0.00023... Val Loss: 0.00220\n",
      "EarlyStopping counter: 763 out of 30\n",
      "Epoch: 1192/2000... Step: 1192... Loss: 0.00023... Val Loss: 0.00212\n",
      "EarlyStopping counter: 764 out of 30\n",
      "Epoch: 1193/2000... Step: 1193... Loss: 0.00025... Val Loss: 0.00242\n",
      "EarlyStopping counter: 765 out of 30\n",
      "Epoch: 1194/2000... Step: 1194... Loss: 0.00028... Val Loss: 0.00207\n",
      "EarlyStopping counter: 766 out of 30\n",
      "Epoch: 1195/2000... Step: 1195... Loss: 0.00033... Val Loss: 0.00268\n",
      "EarlyStopping counter: 767 out of 30\n",
      "Epoch: 1196/2000... Step: 1196... Loss: 0.00039... Val Loss: 0.00211\n",
      "EarlyStopping counter: 768 out of 30\n",
      "Epoch: 1197/2000... Step: 1197... Loss: 0.00046... Val Loss: 0.00296\n",
      "EarlyStopping counter: 769 out of 30\n",
      "Epoch: 1198/2000... Step: 1198... Loss: 0.00057... Val Loss: 0.00231\n",
      "EarlyStopping counter: 770 out of 30\n",
      "Epoch: 1199/2000... Step: 1199... Loss: 0.00072... Val Loss: 0.00338\n",
      "EarlyStopping counter: 771 out of 30\n",
      "Epoch: 1200/2000... Step: 1200... Loss: 0.00094... Val Loss: 0.00285\n",
      "EarlyStopping counter: 772 out of 30\n",
      "Epoch: 1201/2000... Step: 1201... Loss: 0.00125... Val Loss: 0.00410\n",
      "EarlyStopping counter: 773 out of 30\n",
      "Epoch: 1202/2000... Step: 1202... Loss: 0.00170... Val Loss: 0.00372\n",
      "EarlyStopping counter: 774 out of 30\n",
      "Epoch: 1203/2000... Step: 1203... Loss: 0.00221... Val Loss: 0.00517\n",
      "EarlyStopping counter: 775 out of 30\n",
      "Epoch: 1204/2000... Step: 1204... Loss: 0.00297... Val Loss: 0.00478\n",
      "EarlyStopping counter: 776 out of 30\n",
      "Epoch: 1205/2000... Step: 1205... Loss: 0.00350... Val Loss: 0.00521\n",
      "EarlyStopping counter: 777 out of 30\n",
      "Epoch: 1206/2000... Step: 1206... Loss: 0.00352... Val Loss: 0.00443\n",
      "EarlyStopping counter: 778 out of 30\n",
      "Epoch: 1207/2000... Step: 1207... Loss: 0.00261... Val Loss: 0.00330\n",
      "EarlyStopping counter: 779 out of 30\n",
      "Epoch: 1208/2000... Step: 1208... Loss: 0.00183... Val Loss: 0.00352\n",
      "EarlyStopping counter: 780 out of 30\n",
      "Epoch: 1209/2000... Step: 1209... Loss: 0.00132... Val Loss: 0.00346\n",
      "EarlyStopping counter: 781 out of 30\n",
      "Epoch: 1210/2000... Step: 1210... Loss: 0.00157... Val Loss: 0.00330\n",
      "EarlyStopping counter: 782 out of 30\n",
      "Epoch: 1211/2000... Step: 1211... Loss: 0.00137... Val Loss: 0.00323\n",
      "EarlyStopping counter: 783 out of 30\n",
      "Epoch: 1212/2000... Step: 1212... Loss: 0.00126... Val Loss: 0.00336\n",
      "EarlyStopping counter: 784 out of 30\n",
      "Epoch: 1213/2000... Step: 1213... Loss: 0.00143... Val Loss: 0.00323\n",
      "EarlyStopping counter: 785 out of 30\n",
      "Epoch: 1214/2000... Step: 1214... Loss: 0.00107... Val Loss: 0.00242\n",
      "EarlyStopping counter: 786 out of 30\n",
      "Epoch: 1215/2000... Step: 1215... Loss: 0.00070... Val Loss: 0.00232\n",
      "EarlyStopping counter: 787 out of 30\n",
      "Epoch: 1216/2000... Step: 1216... Loss: 0.00053... Val Loss: 0.00289\n",
      "EarlyStopping counter: 788 out of 30\n",
      "Epoch: 1217/2000... Step: 1217... Loss: 0.00084... Val Loss: 0.00281\n",
      "EarlyStopping counter: 789 out of 30\n",
      "Epoch: 1218/2000... Step: 1218... Loss: 0.00107... Val Loss: 0.00262\n",
      "EarlyStopping counter: 790 out of 30\n",
      "Epoch: 1219/2000... Step: 1219... Loss: 0.00080... Val Loss: 0.00287\n",
      "EarlyStopping counter: 791 out of 30\n",
      "Epoch: 1220/2000... Step: 1220... Loss: 0.00057... Val Loss: 0.00202\n",
      "EarlyStopping counter: 792 out of 30\n",
      "Epoch: 1221/2000... Step: 1221... Loss: 0.00055... Val Loss: 0.00321\n",
      "EarlyStopping counter: 793 out of 30\n",
      "Epoch: 1222/2000... Step: 1222... Loss: 0.00072... Val Loss: 0.00233\n",
      "EarlyStopping counter: 794 out of 30\n",
      "Epoch: 1223/2000... Step: 1223... Loss: 0.00062... Val Loss: 0.00241\n",
      "EarlyStopping counter: 795 out of 30\n",
      "Epoch: 1224/2000... Step: 1224... Loss: 0.00037... Val Loss: 0.00251\n",
      "EarlyStopping counter: 796 out of 30\n",
      "Epoch: 1225/2000... Step: 1225... Loss: 0.00038... Val Loss: 0.00204\n",
      "EarlyStopping counter: 797 out of 30\n",
      "Epoch: 1226/2000... Step: 1226... Loss: 0.00048... Val Loss: 0.00297\n",
      "EarlyStopping counter: 798 out of 30\n",
      "Epoch: 1227/2000... Step: 1227... Loss: 0.00054... Val Loss: 0.00209\n",
      "EarlyStopping counter: 799 out of 30\n",
      "Epoch: 1228/2000... Step: 1228... Loss: 0.00044... Val Loss: 0.00228\n",
      "EarlyStopping counter: 800 out of 30\n",
      "Epoch: 1229/2000... Step: 1229... Loss: 0.00031... Val Loss: 0.00243\n",
      "EarlyStopping counter: 801 out of 30\n",
      "Epoch: 1230/2000... Step: 1230... Loss: 0.00033... Val Loss: 0.00209\n",
      "EarlyStopping counter: 802 out of 30\n",
      "Epoch: 1231/2000... Step: 1231... Loss: 0.00035... Val Loss: 0.00248\n",
      "EarlyStopping counter: 803 out of 30\n",
      "Epoch: 1232/2000... Step: 1232... Loss: 0.00034... Val Loss: 0.00216\n",
      "EarlyStopping counter: 804 out of 30\n",
      "Epoch: 1233/2000... Step: 1233... Loss: 0.00031... Val Loss: 0.00226\n",
      "EarlyStopping counter: 805 out of 30\n",
      "Epoch: 1234/2000... Step: 1234... Loss: 0.00029... Val Loss: 0.00223\n",
      "EarlyStopping counter: 806 out of 30\n",
      "Epoch: 1235/2000... Step: 1235... Loss: 0.00029... Val Loss: 0.00211\n",
      "EarlyStopping counter: 807 out of 30\n",
      "Epoch: 1236/2000... Step: 1236... Loss: 0.00029... Val Loss: 0.00226\n",
      "EarlyStopping counter: 808 out of 30\n",
      "Epoch: 1237/2000... Step: 1237... Loss: 0.00029... Val Loss: 0.00224\n",
      "EarlyStopping counter: 809 out of 30\n",
      "Epoch: 1238/2000... Step: 1238... Loss: 0.00031... Val Loss: 0.00213\n",
      "EarlyStopping counter: 810 out of 30\n",
      "Epoch: 1239/2000... Step: 1239... Loss: 0.00031... Val Loss: 0.00241\n",
      "EarlyStopping counter: 811 out of 30\n",
      "Epoch: 1240/2000... Step: 1240... Loss: 0.00032... Val Loss: 0.00209\n",
      "EarlyStopping counter: 812 out of 30\n",
      "Epoch: 1241/2000... Step: 1241... Loss: 0.00034... Val Loss: 0.00242\n",
      "EarlyStopping counter: 813 out of 30\n",
      "Epoch: 1242/2000... Step: 1242... Loss: 0.00035... Val Loss: 0.00227\n",
      "EarlyStopping counter: 814 out of 30\n",
      "Epoch: 1243/2000... Step: 1243... Loss: 0.00037... Val Loss: 0.00243\n",
      "EarlyStopping counter: 815 out of 30\n",
      "Epoch: 1244/2000... Step: 1244... Loss: 0.00039... Val Loss: 0.00223\n",
      "EarlyStopping counter: 816 out of 30\n",
      "Epoch: 1245/2000... Step: 1245... Loss: 0.00044... Val Loss: 0.00269\n",
      "EarlyStopping counter: 817 out of 30\n",
      "Epoch: 1246/2000... Step: 1246... Loss: 0.00053... Val Loss: 0.00231\n",
      "EarlyStopping counter: 818 out of 30\n",
      "Epoch: 1247/2000... Step: 1247... Loss: 0.00056... Val Loss: 0.00290\n",
      "EarlyStopping counter: 819 out of 30\n",
      "Epoch: 1248/2000... Step: 1248... Loss: 0.00062... Val Loss: 0.00230\n",
      "EarlyStopping counter: 820 out of 30\n",
      "Epoch: 1249/2000... Step: 1249... Loss: 0.00071... Val Loss: 0.00332\n",
      "EarlyStopping counter: 821 out of 30\n",
      "Epoch: 1250/2000... Step: 1250... Loss: 0.00085... Val Loss: 0.00254\n",
      "EarlyStopping counter: 822 out of 30\n",
      "Epoch: 1251/2000... Step: 1251... Loss: 0.00100... Val Loss: 0.00364\n",
      "EarlyStopping counter: 823 out of 30\n",
      "Epoch: 1252/2000... Step: 1252... Loss: 0.00111... Val Loss: 0.00278\n",
      "EarlyStopping counter: 824 out of 30\n",
      "Epoch: 1253/2000... Step: 1253... Loss: 0.00119... Val Loss: 0.00361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 825 out of 30\n",
      "Epoch: 1254/2000... Step: 1254... Loss: 0.00117... Val Loss: 0.00279\n",
      "EarlyStopping counter: 826 out of 30\n",
      "Epoch: 1255/2000... Step: 1255... Loss: 0.00108... Val Loss: 0.00311\n",
      "EarlyStopping counter: 827 out of 30\n",
      "Epoch: 1256/2000... Step: 1256... Loss: 0.00105... Val Loss: 0.00285\n",
      "EarlyStopping counter: 828 out of 30\n",
      "Epoch: 1257/2000... Step: 1257... Loss: 0.00094... Val Loss: 0.00263\n",
      "EarlyStopping counter: 829 out of 30\n",
      "Epoch: 1258/2000... Step: 1258... Loss: 0.00084... Val Loss: 0.00269\n",
      "EarlyStopping counter: 830 out of 30\n",
      "Epoch: 1259/2000... Step: 1259... Loss: 0.00070... Val Loss: 0.00229\n",
      "EarlyStopping counter: 831 out of 30\n",
      "Epoch: 1260/2000... Step: 1260... Loss: 0.00048... Val Loss: 0.00243\n",
      "EarlyStopping counter: 832 out of 30\n",
      "Epoch: 1261/2000... Step: 1261... Loss: 0.00034... Val Loss: 0.00211\n",
      "EarlyStopping counter: 833 out of 30\n",
      "Epoch: 1262/2000... Step: 1262... Loss: 0.00026... Val Loss: 0.00233\n",
      "EarlyStopping counter: 834 out of 30\n",
      "Epoch: 1263/2000... Step: 1263... Loss: 0.00031... Val Loss: 0.00240\n",
      "EarlyStopping counter: 835 out of 30\n",
      "Epoch: 1264/2000... Step: 1264... Loss: 0.00041... Val Loss: 0.00238\n",
      "EarlyStopping counter: 836 out of 30\n",
      "Epoch: 1265/2000... Step: 1265... Loss: 0.00051... Val Loss: 0.00263\n",
      "EarlyStopping counter: 837 out of 30\n",
      "Epoch: 1266/2000... Step: 1266... Loss: 0.00056... Val Loss: 0.00252\n",
      "EarlyStopping counter: 838 out of 30\n",
      "Epoch: 1267/2000... Step: 1267... Loss: 0.00052... Val Loss: 0.00224\n",
      "EarlyStopping counter: 839 out of 30\n",
      "Epoch: 1268/2000... Step: 1268... Loss: 0.00046... Val Loss: 0.00263\n",
      "EarlyStopping counter: 840 out of 30\n",
      "Epoch: 1269/2000... Step: 1269... Loss: 0.00039... Val Loss: 0.00210\n",
      "EarlyStopping counter: 841 out of 30\n",
      "Epoch: 1270/2000... Step: 1270... Loss: 0.00036... Val Loss: 0.00267\n",
      "EarlyStopping counter: 842 out of 30\n",
      "Epoch: 1271/2000... Step: 1271... Loss: 0.00037... Val Loss: 0.00208\n",
      "EarlyStopping counter: 843 out of 30\n",
      "Epoch: 1272/2000... Step: 1272... Loss: 0.00036... Val Loss: 0.00249\n",
      "EarlyStopping counter: 844 out of 30\n",
      "Epoch: 1273/2000... Step: 1273... Loss: 0.00034... Val Loss: 0.00216\n",
      "EarlyStopping counter: 845 out of 30\n",
      "Epoch: 1274/2000... Step: 1274... Loss: 0.00027... Val Loss: 0.00215\n",
      "EarlyStopping counter: 846 out of 30\n",
      "Epoch: 1275/2000... Step: 1275... Loss: 0.00022... Val Loss: 0.00226\n",
      "EarlyStopping counter: 847 out of 30\n",
      "Epoch: 1276/2000... Step: 1276... Loss: 0.00020... Val Loss: 0.00209\n",
      "EarlyStopping counter: 848 out of 30\n",
      "Epoch: 1277/2000... Step: 1277... Loss: 0.00021... Val Loss: 0.00233\n",
      "EarlyStopping counter: 849 out of 30\n",
      "Epoch: 1278/2000... Step: 1278... Loss: 0.00024... Val Loss: 0.00211\n",
      "EarlyStopping counter: 850 out of 30\n",
      "Epoch: 1279/2000... Step: 1279... Loss: 0.00025... Val Loss: 0.00230\n",
      "EarlyStopping counter: 851 out of 30\n",
      "Epoch: 1280/2000... Step: 1280... Loss: 0.00025... Val Loss: 0.00216\n",
      "EarlyStopping counter: 852 out of 30\n",
      "Epoch: 1281/2000... Step: 1281... Loss: 0.00024... Val Loss: 0.00227\n",
      "EarlyStopping counter: 853 out of 30\n",
      "Epoch: 1282/2000... Step: 1282... Loss: 0.00023... Val Loss: 0.00218\n",
      "EarlyStopping counter: 854 out of 30\n",
      "Epoch: 1283/2000... Step: 1283... Loss: 0.00024... Val Loss: 0.00239\n",
      "EarlyStopping counter: 855 out of 30\n",
      "Epoch: 1284/2000... Step: 1284... Loss: 0.00028... Val Loss: 0.00221\n",
      "EarlyStopping counter: 856 out of 30\n",
      "Epoch: 1285/2000... Step: 1285... Loss: 0.00036... Val Loss: 0.00263\n",
      "EarlyStopping counter: 857 out of 30\n",
      "Epoch: 1286/2000... Step: 1286... Loss: 0.00048... Val Loss: 0.00244\n",
      "EarlyStopping counter: 858 out of 30\n",
      "Epoch: 1287/2000... Step: 1287... Loss: 0.00068... Val Loss: 0.00318\n",
      "EarlyStopping counter: 859 out of 30\n",
      "Epoch: 1288/2000... Step: 1288... Loss: 0.00099... Val Loss: 0.00318\n",
      "EarlyStopping counter: 860 out of 30\n",
      "Epoch: 1289/2000... Step: 1289... Loss: 0.00148... Val Loss: 0.00471\n",
      "EarlyStopping counter: 861 out of 30\n",
      "Epoch: 1290/2000... Step: 1290... Loss: 0.00237... Val Loss: 0.00487\n",
      "EarlyStopping counter: 862 out of 30\n",
      "Epoch: 1291/2000... Step: 1291... Loss: 0.00341... Val Loss: 0.00745\n",
      "EarlyStopping counter: 863 out of 30\n",
      "Epoch: 1292/2000... Step: 1292... Loss: 0.00502... Val Loss: 0.00677\n",
      "EarlyStopping counter: 864 out of 30\n",
      "Epoch: 1293/2000... Step: 1293... Loss: 0.00549... Val Loss: 0.00755\n",
      "EarlyStopping counter: 865 out of 30\n",
      "Epoch: 1294/2000... Step: 1294... Loss: 0.00553... Val Loss: 0.00409\n",
      "EarlyStopping counter: 866 out of 30\n",
      "Epoch: 1295/2000... Step: 1295... Loss: 0.00268... Val Loss: 0.00277\n",
      "EarlyStopping counter: 867 out of 30\n",
      "Epoch: 1296/2000... Step: 1296... Loss: 0.00090... Val Loss: 0.00307\n",
      "EarlyStopping counter: 868 out of 30\n",
      "Epoch: 1297/2000... Step: 1297... Loss: 0.00145... Val Loss: 0.00475\n",
      "EarlyStopping counter: 869 out of 30\n",
      "Epoch: 1298/2000... Step: 1298... Loss: 0.00289... Val Loss: 0.00509\n",
      "EarlyStopping counter: 870 out of 30\n",
      "Epoch: 1299/2000... Step: 1299... Loss: 0.00315... Val Loss: 0.00370\n",
      "EarlyStopping counter: 871 out of 30\n",
      "Epoch: 1300/2000... Step: 1300... Loss: 0.00174... Val Loss: 0.00291\n",
      "EarlyStopping counter: 872 out of 30\n",
      "Epoch: 1301/2000... Step: 1301... Loss: 0.00098... Val Loss: 0.00289\n",
      "EarlyStopping counter: 873 out of 30\n",
      "Epoch: 1302/2000... Step: 1302... Loss: 0.00118... Val Loss: 0.00368\n",
      "EarlyStopping counter: 874 out of 30\n",
      "Epoch: 1303/2000... Step: 1303... Loss: 0.00165... Val Loss: 0.00331\n",
      "EarlyStopping counter: 875 out of 30\n",
      "Epoch: 1304/2000... Step: 1304... Loss: 0.00160... Val Loss: 0.00316\n",
      "EarlyStopping counter: 876 out of 30\n",
      "Epoch: 1305/2000... Step: 1305... Loss: 0.00105... Val Loss: 0.00234\n",
      "EarlyStopping counter: 877 out of 30\n",
      "Epoch: 1306/2000... Step: 1306... Loss: 0.00070... Val Loss: 0.00282\n",
      "EarlyStopping counter: 878 out of 30\n",
      "Epoch: 1307/2000... Step: 1307... Loss: 0.00087... Val Loss: 0.00327\n",
      "EarlyStopping counter: 879 out of 30\n",
      "Epoch: 1308/2000... Step: 1308... Loss: 0.00114... Val Loss: 0.00300\n",
      "EarlyStopping counter: 880 out of 30\n",
      "Epoch: 1309/2000... Step: 1309... Loss: 0.00116... Val Loss: 0.00272\n",
      "EarlyStopping counter: 881 out of 30\n",
      "Epoch: 1310/2000... Step: 1310... Loss: 0.00072... Val Loss: 0.00236\n",
      "EarlyStopping counter: 882 out of 30\n",
      "Epoch: 1311/2000... Step: 1311... Loss: 0.00048... Val Loss: 0.00270\n",
      "EarlyStopping counter: 883 out of 30\n",
      "Epoch: 1312/2000... Step: 1312... Loss: 0.00073... Val Loss: 0.00300\n",
      "EarlyStopping counter: 884 out of 30\n",
      "Epoch: 1313/2000... Step: 1313... Loss: 0.00095... Val Loss: 0.00284\n",
      "EarlyStopping counter: 885 out of 30\n",
      "Epoch: 1314/2000... Step: 1314... Loss: 0.00077... Val Loss: 0.00236\n",
      "EarlyStopping counter: 886 out of 30\n",
      "Epoch: 1315/2000... Step: 1315... Loss: 0.00040... Val Loss: 0.00246\n",
      "EarlyStopping counter: 887 out of 30\n",
      "Epoch: 1316/2000... Step: 1316... Loss: 0.00032... Val Loss: 0.00247\n",
      "EarlyStopping counter: 888 out of 30\n",
      "Epoch: 1317/2000... Step: 1317... Loss: 0.00056... Val Loss: 0.00270\n",
      "EarlyStopping counter: 889 out of 30\n",
      "Epoch: 1318/2000... Step: 1318... Loss: 0.00066... Val Loss: 0.00259\n",
      "EarlyStopping counter: 890 out of 30\n",
      "Epoch: 1319/2000... Step: 1319... Loss: 0.00054... Val Loss: 0.00220\n",
      "EarlyStopping counter: 891 out of 30\n",
      "Epoch: 1320/2000... Step: 1320... Loss: 0.00034... Val Loss: 0.00257\n",
      "EarlyStopping counter: 892 out of 30\n",
      "Epoch: 1321/2000... Step: 1321... Loss: 0.00033... Val Loss: 0.00227\n",
      "EarlyStopping counter: 893 out of 30\n",
      "Epoch: 1322/2000... Step: 1322... Loss: 0.00042... Val Loss: 0.00268\n",
      "EarlyStopping counter: 894 out of 30\n",
      "Epoch: 1323/2000... Step: 1323... Loss: 0.00044... Val Loss: 0.00227\n",
      "EarlyStopping counter: 895 out of 30\n",
      "Epoch: 1324/2000... Step: 1324... Loss: 0.00034... Val Loss: 0.00214\n",
      "EarlyStopping counter: 896 out of 30\n",
      "Epoch: 1325/2000... Step: 1325... Loss: 0.00029... Val Loss: 0.00271\n",
      "EarlyStopping counter: 897 out of 30\n",
      "Epoch: 1326/2000... Step: 1326... Loss: 0.00037... Val Loss: 0.00209\n",
      "EarlyStopping counter: 898 out of 30\n",
      "Epoch: 1327/2000... Step: 1327... Loss: 0.00043... Val Loss: 0.00254\n",
      "EarlyStopping counter: 899 out of 30\n",
      "Epoch: 1328/2000... Step: 1328... Loss: 0.00035... Val Loss: 0.00210\n",
      "EarlyStopping counter: 900 out of 30\n",
      "Epoch: 1329/2000... Step: 1329... Loss: 0.00023... Val Loss: 0.00217\n",
      "EarlyStopping counter: 901 out of 30\n",
      "Epoch: 1330/2000... Step: 1330... Loss: 0.00021... Val Loss: 0.00246\n",
      "EarlyStopping counter: 902 out of 30\n",
      "Epoch: 1331/2000... Step: 1331... Loss: 0.00028... Val Loss: 0.00207\n",
      "EarlyStopping counter: 903 out of 30\n",
      "Epoch: 1332/2000... Step: 1332... Loss: 0.00031... Val Loss: 0.00245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 904 out of 30\n",
      "Epoch: 1333/2000... Step: 1333... Loss: 0.00027... Val Loss: 0.00210\n",
      "EarlyStopping counter: 905 out of 30\n",
      "Epoch: 1334/2000... Step: 1334... Loss: 0.00022... Val Loss: 0.00223\n",
      "EarlyStopping counter: 906 out of 30\n",
      "Epoch: 1335/2000... Step: 1335... Loss: 0.00023... Val Loss: 0.00230\n",
      "EarlyStopping counter: 907 out of 30\n",
      "Epoch: 1336/2000... Step: 1336... Loss: 0.00025... Val Loss: 0.00217\n",
      "EarlyStopping counter: 908 out of 30\n",
      "Epoch: 1337/2000... Step: 1337... Loss: 0.00025... Val Loss: 0.00217\n",
      "EarlyStopping counter: 909 out of 30\n",
      "Epoch: 1338/2000... Step: 1338... Loss: 0.00021... Val Loss: 0.00222\n",
      "EarlyStopping counter: 910 out of 30\n",
      "Epoch: 1339/2000... Step: 1339... Loss: 0.00019... Val Loss: 0.00210\n",
      "EarlyStopping counter: 911 out of 30\n",
      "Epoch: 1340/2000... Step: 1340... Loss: 0.00020... Val Loss: 0.00235\n",
      "EarlyStopping counter: 912 out of 30\n",
      "Epoch: 1341/2000... Step: 1341... Loss: 0.00022... Val Loss: 0.00213\n",
      "EarlyStopping counter: 913 out of 30\n",
      "Epoch: 1342/2000... Step: 1342... Loss: 0.00021... Val Loss: 0.00228\n",
      "EarlyStopping counter: 914 out of 30\n",
      "Epoch: 1343/2000... Step: 1343... Loss: 0.00019... Val Loss: 0.00220\n",
      "EarlyStopping counter: 915 out of 30\n",
      "Epoch: 1344/2000... Step: 1344... Loss: 0.00019... Val Loss: 0.00212\n",
      "EarlyStopping counter: 916 out of 30\n",
      "Epoch: 1345/2000... Step: 1345... Loss: 0.00020... Val Loss: 0.00230\n",
      "EarlyStopping counter: 917 out of 30\n",
      "Epoch: 1346/2000... Step: 1346... Loss: 0.00021... Val Loss: 0.00209\n",
      "EarlyStopping counter: 918 out of 30\n",
      "Epoch: 1347/2000... Step: 1347... Loss: 0.00021... Val Loss: 0.00228\n",
      "EarlyStopping counter: 919 out of 30\n",
      "Epoch: 1348/2000... Step: 1348... Loss: 0.00020... Val Loss: 0.00222\n",
      "EarlyStopping counter: 920 out of 30\n",
      "Epoch: 1349/2000... Step: 1349... Loss: 0.00021... Val Loss: 0.00223\n",
      "EarlyStopping counter: 921 out of 30\n",
      "Epoch: 1350/2000... Step: 1350... Loss: 0.00023... Val Loss: 0.00232\n",
      "EarlyStopping counter: 922 out of 30\n",
      "Epoch: 1351/2000... Step: 1351... Loss: 0.00027... Val Loss: 0.00224\n",
      "EarlyStopping counter: 923 out of 30\n",
      "Epoch: 1352/2000... Step: 1352... Loss: 0.00031... Val Loss: 0.00246\n",
      "EarlyStopping counter: 924 out of 30\n",
      "Epoch: 1353/2000... Step: 1353... Loss: 0.00038... Val Loss: 0.00231\n",
      "EarlyStopping counter: 925 out of 30\n",
      "Epoch: 1354/2000... Step: 1354... Loss: 0.00046... Val Loss: 0.00257\n",
      "EarlyStopping counter: 926 out of 30\n",
      "Epoch: 1355/2000... Step: 1355... Loss: 0.00061... Val Loss: 0.00252\n",
      "EarlyStopping counter: 927 out of 30\n",
      "Epoch: 1356/2000... Step: 1356... Loss: 0.00069... Val Loss: 0.00274\n",
      "EarlyStopping counter: 928 out of 30\n",
      "Epoch: 1357/2000... Step: 1357... Loss: 0.00082... Val Loss: 0.00265\n",
      "EarlyStopping counter: 929 out of 30\n",
      "Epoch: 1358/2000... Step: 1358... Loss: 0.00083... Val Loss: 0.00284\n",
      "EarlyStopping counter: 930 out of 30\n",
      "Epoch: 1359/2000... Step: 1359... Loss: 0.00084... Val Loss: 0.00252\n",
      "EarlyStopping counter: 931 out of 30\n",
      "Epoch: 1360/2000... Step: 1360... Loss: 0.00069... Val Loss: 0.00266\n",
      "EarlyStopping counter: 932 out of 30\n",
      "Epoch: 1361/2000... Step: 1361... Loss: 0.00053... Val Loss: 0.00229\n",
      "EarlyStopping counter: 933 out of 30\n",
      "Epoch: 1362/2000... Step: 1362... Loss: 0.00040... Val Loss: 0.00225\n",
      "EarlyStopping counter: 934 out of 30\n",
      "Epoch: 1363/2000... Step: 1363... Loss: 0.00031... Val Loss: 0.00243\n",
      "EarlyStopping counter: 935 out of 30\n",
      "Epoch: 1364/2000... Step: 1364... Loss: 0.00029... Val Loss: 0.00205\n",
      "EarlyStopping counter: 936 out of 30\n",
      "Epoch: 1365/2000... Step: 1365... Loss: 0.00032... Val Loss: 0.00284\n",
      "EarlyStopping counter: 937 out of 30\n",
      "Epoch: 1366/2000... Step: 1366... Loss: 0.00042... Val Loss: 0.00206\n",
      "EarlyStopping counter: 938 out of 30\n",
      "Epoch: 1367/2000... Step: 1367... Loss: 0.00049... Val Loss: 0.00302\n",
      "EarlyStopping counter: 939 out of 30\n",
      "Epoch: 1368/2000... Step: 1368... Loss: 0.00053... Val Loss: 0.00203\n",
      "EarlyStopping counter: 940 out of 30\n",
      "Epoch: 1369/2000... Step: 1369... Loss: 0.00050... Val Loss: 0.00289\n",
      "EarlyStopping counter: 941 out of 30\n",
      "Epoch: 1370/2000... Step: 1370... Loss: 0.00047... Val Loss: 0.00215\n",
      "EarlyStopping counter: 942 out of 30\n",
      "Epoch: 1371/2000... Step: 1371... Loss: 0.00052... Val Loss: 0.00304\n",
      "EarlyStopping counter: 943 out of 30\n",
      "Epoch: 1372/2000... Step: 1372... Loss: 0.00070... Val Loss: 0.00276\n",
      "EarlyStopping counter: 944 out of 30\n",
      "Epoch: 1373/2000... Step: 1373... Loss: 0.00096... Val Loss: 0.00365\n",
      "EarlyStopping counter: 945 out of 30\n",
      "Epoch: 1374/2000... Step: 1374... Loss: 0.00130... Val Loss: 0.00332\n",
      "EarlyStopping counter: 946 out of 30\n",
      "Epoch: 1375/2000... Step: 1375... Loss: 0.00161... Val Loss: 0.00455\n",
      "EarlyStopping counter: 947 out of 30\n",
      "Epoch: 1376/2000... Step: 1376... Loss: 0.00200... Val Loss: 0.00358\n",
      "EarlyStopping counter: 948 out of 30\n",
      "Epoch: 1377/2000... Step: 1377... Loss: 0.00218... Val Loss: 0.00486\n",
      "EarlyStopping counter: 949 out of 30\n",
      "Epoch: 1378/2000... Step: 1378... Loss: 0.00226... Val Loss: 0.00313\n",
      "EarlyStopping counter: 950 out of 30\n",
      "Epoch: 1379/2000... Step: 1379... Loss: 0.00189... Val Loss: 0.00370\n",
      "EarlyStopping counter: 951 out of 30\n",
      "Epoch: 1380/2000... Step: 1380... Loss: 0.00133... Val Loss: 0.00229\n",
      "EarlyStopping counter: 952 out of 30\n",
      "Epoch: 1381/2000... Step: 1381... Loss: 0.00070... Val Loss: 0.00261\n",
      "EarlyStopping counter: 953 out of 30\n",
      "Epoch: 1382/2000... Step: 1382... Loss: 0.00047... Val Loss: 0.00256\n",
      "EarlyStopping counter: 954 out of 30\n",
      "Epoch: 1383/2000... Step: 1383... Loss: 0.00065... Val Loss: 0.00269\n",
      "EarlyStopping counter: 955 out of 30\n",
      "Epoch: 1384/2000... Step: 1384... Loss: 0.00101... Val Loss: 0.00344\n",
      "EarlyStopping counter: 956 out of 30\n",
      "Epoch: 1385/2000... Step: 1385... Loss: 0.00117... Val Loss: 0.00255\n",
      "EarlyStopping counter: 957 out of 30\n",
      "Epoch: 1386/2000... Step: 1386... Loss: 0.00103... Val Loss: 0.00312\n",
      "EarlyStopping counter: 958 out of 30\n",
      "Epoch: 1387/2000... Step: 1387... Loss: 0.00074... Val Loss: 0.00225\n",
      "EarlyStopping counter: 959 out of 30\n",
      "Epoch: 1388/2000... Step: 1388... Loss: 0.00052... Val Loss: 0.00251\n",
      "EarlyStopping counter: 960 out of 30\n",
      "Epoch: 1389/2000... Step: 1389... Loss: 0.00046... Val Loss: 0.00228\n",
      "EarlyStopping counter: 961 out of 30\n",
      "Epoch: 1390/2000... Step: 1390... Loss: 0.00048... Val Loss: 0.00256\n",
      "EarlyStopping counter: 962 out of 30\n",
      "Epoch: 1391/2000... Step: 1391... Loss: 0.00054... Val Loss: 0.00266\n",
      "EarlyStopping counter: 963 out of 30\n",
      "Epoch: 1392/2000... Step: 1392... Loss: 0.00058... Val Loss: 0.00242\n",
      "EarlyStopping counter: 964 out of 30\n",
      "Epoch: 1393/2000... Step: 1393... Loss: 0.00056... Val Loss: 0.00282\n",
      "EarlyStopping counter: 965 out of 30\n",
      "Epoch: 1394/2000... Step: 1394... Loss: 0.00048... Val Loss: 0.00216\n",
      "EarlyStopping counter: 966 out of 30\n",
      "Epoch: 1395/2000... Step: 1395... Loss: 0.00038... Val Loss: 0.00277\n",
      "EarlyStopping counter: 967 out of 30\n",
      "Epoch: 1396/2000... Step: 1396... Loss: 0.00034... Val Loss: 0.00212\n",
      "EarlyStopping counter: 968 out of 30\n",
      "Epoch: 1397/2000... Step: 1397... Loss: 0.00035... Val Loss: 0.00260\n",
      "EarlyStopping counter: 969 out of 30\n",
      "Epoch: 1398/2000... Step: 1398... Loss: 0.00035... Val Loss: 0.00235\n",
      "EarlyStopping counter: 970 out of 30\n",
      "Epoch: 1399/2000... Step: 1399... Loss: 0.00033... Val Loss: 0.00218\n",
      "EarlyStopping counter: 971 out of 30\n",
      "Epoch: 1400/2000... Step: 1400... Loss: 0.00030... Val Loss: 0.00257\n",
      "EarlyStopping counter: 972 out of 30\n",
      "Epoch: 1401/2000... Step: 1401... Loss: 0.00032... Val Loss: 0.00202\n",
      "EarlyStopping counter: 973 out of 30\n",
      "Epoch: 1402/2000... Step: 1402... Loss: 0.00035... Val Loss: 0.00266\n",
      "EarlyStopping counter: 974 out of 30\n",
      "Epoch: 1403/2000... Step: 1403... Loss: 0.00035... Val Loss: 0.00204\n",
      "EarlyStopping counter: 975 out of 30\n",
      "Epoch: 1404/2000... Step: 1404... Loss: 0.00030... Val Loss: 0.00248\n",
      "EarlyStopping counter: 976 out of 30\n",
      "Epoch: 1405/2000... Step: 1405... Loss: 0.00025... Val Loss: 0.00220\n",
      "EarlyStopping counter: 977 out of 30\n",
      "Epoch: 1406/2000... Step: 1406... Loss: 0.00021... Val Loss: 0.00219\n",
      "EarlyStopping counter: 978 out of 30\n",
      "Epoch: 1407/2000... Step: 1407... Loss: 0.00022... Val Loss: 0.00242\n",
      "EarlyStopping counter: 979 out of 30\n",
      "Epoch: 1408/2000... Step: 1408... Loss: 0.00023... Val Loss: 0.00204\n",
      "EarlyStopping counter: 980 out of 30\n",
      "Epoch: 1409/2000... Step: 1409... Loss: 0.00023... Val Loss: 0.00242\n",
      "EarlyStopping counter: 981 out of 30\n",
      "Epoch: 1410/2000... Step: 1410... Loss: 0.00021... Val Loss: 0.00211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 982 out of 30\n",
      "Epoch: 1411/2000... Step: 1411... Loss: 0.00019... Val Loss: 0.00226\n",
      "EarlyStopping counter: 983 out of 30\n",
      "Epoch: 1412/2000... Step: 1412... Loss: 0.00020... Val Loss: 0.00233\n",
      "EarlyStopping counter: 984 out of 30\n",
      "Epoch: 1413/2000... Step: 1413... Loss: 0.00023... Val Loss: 0.00221\n",
      "EarlyStopping counter: 985 out of 30\n",
      "Epoch: 1414/2000... Step: 1414... Loss: 0.00024... Val Loss: 0.00229\n",
      "EarlyStopping counter: 986 out of 30\n",
      "Epoch: 1415/2000... Step: 1415... Loss: 0.00022... Val Loss: 0.00220\n",
      "EarlyStopping counter: 987 out of 30\n",
      "Epoch: 1416/2000... Step: 1416... Loss: 0.00020... Val Loss: 0.00212\n",
      "EarlyStopping counter: 988 out of 30\n",
      "Epoch: 1417/2000... Step: 1417... Loss: 0.00018... Val Loss: 0.00235\n",
      "EarlyStopping counter: 989 out of 30\n",
      "Epoch: 1418/2000... Step: 1418... Loss: 0.00019... Val Loss: 0.00209\n",
      "EarlyStopping counter: 990 out of 30\n",
      "Epoch: 1419/2000... Step: 1419... Loss: 0.00021... Val Loss: 0.00239\n",
      "EarlyStopping counter: 991 out of 30\n",
      "Epoch: 1420/2000... Step: 1420... Loss: 0.00022... Val Loss: 0.00214\n",
      "EarlyStopping counter: 992 out of 30\n",
      "Epoch: 1421/2000... Step: 1421... Loss: 0.00022... Val Loss: 0.00235\n",
      "EarlyStopping counter: 993 out of 30\n",
      "Epoch: 1422/2000... Step: 1422... Loss: 0.00025... Val Loss: 0.00226\n",
      "EarlyStopping counter: 994 out of 30\n",
      "Epoch: 1423/2000... Step: 1423... Loss: 0.00031... Val Loss: 0.00244\n",
      "EarlyStopping counter: 995 out of 30\n",
      "Epoch: 1424/2000... Step: 1424... Loss: 0.00041... Val Loss: 0.00258\n",
      "EarlyStopping counter: 996 out of 30\n",
      "Epoch: 1425/2000... Step: 1425... Loss: 0.00055... Val Loss: 0.00284\n",
      "EarlyStopping counter: 997 out of 30\n",
      "Epoch: 1426/2000... Step: 1426... Loss: 0.00078... Val Loss: 0.00296\n",
      "EarlyStopping counter: 998 out of 30\n",
      "Epoch: 1427/2000... Step: 1427... Loss: 0.00109... Val Loss: 0.00377\n",
      "EarlyStopping counter: 999 out of 30\n",
      "Epoch: 1428/2000... Step: 1428... Loss: 0.00161... Val Loss: 0.00378\n",
      "EarlyStopping counter: 1000 out of 30\n",
      "Epoch: 1429/2000... Step: 1429... Loss: 0.00221... Val Loss: 0.00534\n",
      "EarlyStopping counter: 1001 out of 30\n",
      "Epoch: 1430/2000... Step: 1430... Loss: 0.00302... Val Loss: 0.00475\n",
      "EarlyStopping counter: 1002 out of 30\n",
      "Epoch: 1431/2000... Step: 1431... Loss: 0.00339... Val Loss: 0.00574\n",
      "EarlyStopping counter: 1003 out of 30\n",
      "Epoch: 1432/2000... Step: 1432... Loss: 0.00354... Val Loss: 0.00399\n",
      "EarlyStopping counter: 1004 out of 30\n",
      "Epoch: 1433/2000... Step: 1433... Loss: 0.00253... Val Loss: 0.00379\n",
      "EarlyStopping counter: 1005 out of 30\n",
      "Epoch: 1434/2000... Step: 1434... Loss: 0.00176... Val Loss: 0.00300\n",
      "EarlyStopping counter: 1006 out of 30\n",
      "Epoch: 1435/2000... Step: 1435... Loss: 0.00129... Val Loss: 0.00395\n",
      "EarlyStopping counter: 1007 out of 30\n",
      "Epoch: 1436/2000... Step: 1436... Loss: 0.00179... Val Loss: 0.00384\n",
      "EarlyStopping counter: 1008 out of 30\n",
      "Epoch: 1437/2000... Step: 1437... Loss: 0.00186... Val Loss: 0.00384\n",
      "EarlyStopping counter: 1009 out of 30\n",
      "Epoch: 1438/2000... Step: 1438... Loss: 0.00183... Val Loss: 0.00377\n",
      "EarlyStopping counter: 1010 out of 30\n",
      "Epoch: 1439/2000... Step: 1439... Loss: 0.00154... Val Loss: 0.00297\n",
      "EarlyStopping counter: 1011 out of 30\n",
      "Epoch: 1440/2000... Step: 1440... Loss: 0.00111... Val Loss: 0.00306\n",
      "EarlyStopping counter: 1012 out of 30\n",
      "Epoch: 1441/2000... Step: 1441... Loss: 0.00092... Val Loss: 0.00253\n",
      "EarlyStopping counter: 1013 out of 30\n",
      "Epoch: 1442/2000... Step: 1442... Loss: 0.00091... Val Loss: 0.00337\n",
      "EarlyStopping counter: 1014 out of 30\n",
      "Epoch: 1443/2000... Step: 1443... Loss: 0.00112... Val Loss: 0.00287\n",
      "EarlyStopping counter: 1015 out of 30\n",
      "Epoch: 1444/2000... Step: 1444... Loss: 0.00116... Val Loss: 0.00286\n",
      "EarlyStopping counter: 1016 out of 30\n",
      "Epoch: 1445/2000... Step: 1445... Loss: 0.00088... Val Loss: 0.00265\n",
      "EarlyStopping counter: 1017 out of 30\n",
      "Epoch: 1446/2000... Step: 1446... Loss: 0.00054... Val Loss: 0.00215\n",
      "EarlyStopping counter: 1018 out of 30\n",
      "Epoch: 1447/2000... Step: 1447... Loss: 0.00044... Val Loss: 0.00294\n",
      "EarlyStopping counter: 1019 out of 30\n",
      "Epoch: 1448/2000... Step: 1448... Loss: 0.00076... Val Loss: 0.00270\n",
      "EarlyStopping counter: 1020 out of 30\n",
      "Epoch: 1449/2000... Step: 1449... Loss: 0.00091... Val Loss: 0.00290\n",
      "EarlyStopping counter: 1021 out of 30\n",
      "Epoch: 1450/2000... Step: 1450... Loss: 0.00074... Val Loss: 0.00253\n",
      "EarlyStopping counter: 1022 out of 30\n",
      "Epoch: 1451/2000... Step: 1451... Loss: 0.00042... Val Loss: 0.00206\n",
      "EarlyStopping counter: 1023 out of 30\n",
      "Epoch: 1452/2000... Step: 1452... Loss: 0.00039... Val Loss: 0.00287\n",
      "EarlyStopping counter: 1024 out of 30\n",
      "Epoch: 1453/2000... Step: 1453... Loss: 0.00056... Val Loss: 0.00216\n",
      "EarlyStopping counter: 1025 out of 30\n",
      "Epoch: 1454/2000... Step: 1454... Loss: 0.00046... Val Loss: 0.00239\n",
      "EarlyStopping counter: 1026 out of 30\n",
      "Epoch: 1455/2000... Step: 1455... Loss: 0.00031... Val Loss: 0.00238\n",
      "EarlyStopping counter: 1027 out of 30\n",
      "Epoch: 1456/2000... Step: 1456... Loss: 0.00027... Val Loss: 0.00212\n",
      "EarlyStopping counter: 1028 out of 30\n",
      "Epoch: 1457/2000... Step: 1457... Loss: 0.00039... Val Loss: 0.00265\n",
      "EarlyStopping counter: 1029 out of 30\n",
      "Epoch: 1458/2000... Step: 1458... Loss: 0.00044... Val Loss: 0.00209\n",
      "EarlyStopping counter: 1030 out of 30\n",
      "Epoch: 1459/2000... Step: 1459... Loss: 0.00032... Val Loss: 0.00255\n",
      "EarlyStopping counter: 1031 out of 30\n",
      "Epoch: 1460/2000... Step: 1460... Loss: 0.00028... Val Loss: 0.00214\n",
      "EarlyStopping counter: 1032 out of 30\n",
      "Epoch: 1461/2000... Step: 1461... Loss: 0.00031... Val Loss: 0.00233\n",
      "EarlyStopping counter: 1033 out of 30\n",
      "Epoch: 1462/2000... Step: 1462... Loss: 0.00032... Val Loss: 0.00223\n",
      "EarlyStopping counter: 1034 out of 30\n",
      "Epoch: 1463/2000... Step: 1463... Loss: 0.00026... Val Loss: 0.00226\n",
      "EarlyStopping counter: 1035 out of 30\n",
      "Epoch: 1464/2000... Step: 1464... Loss: 0.00021... Val Loss: 0.00213\n",
      "EarlyStopping counter: 1036 out of 30\n",
      "Epoch: 1465/2000... Step: 1465... Loss: 0.00021... Val Loss: 0.00230\n",
      "EarlyStopping counter: 1037 out of 30\n",
      "Epoch: 1466/2000... Step: 1466... Loss: 0.00023... Val Loss: 0.00227\n",
      "EarlyStopping counter: 1038 out of 30\n",
      "Epoch: 1467/2000... Step: 1467... Loss: 0.00022... Val Loss: 0.00209\n",
      "EarlyStopping counter: 1039 out of 30\n",
      "Epoch: 1468/2000... Step: 1468... Loss: 0.00023... Val Loss: 0.00252\n",
      "EarlyStopping counter: 1040 out of 30\n",
      "Epoch: 1469/2000... Step: 1469... Loss: 0.00025... Val Loss: 0.00207\n",
      "EarlyStopping counter: 1041 out of 30\n",
      "Epoch: 1470/2000... Step: 1470... Loss: 0.00026... Val Loss: 0.00247\n",
      "EarlyStopping counter: 1042 out of 30\n",
      "Epoch: 1471/2000... Step: 1471... Loss: 0.00024... Val Loss: 0.00206\n",
      "EarlyStopping counter: 1043 out of 30\n",
      "Epoch: 1472/2000... Step: 1472... Loss: 0.00024... Val Loss: 0.00246\n",
      "EarlyStopping counter: 1044 out of 30\n",
      "Epoch: 1473/2000... Step: 1473... Loss: 0.00025... Val Loss: 0.00214\n",
      "EarlyStopping counter: 1045 out of 30\n",
      "Epoch: 1474/2000... Step: 1474... Loss: 0.00026... Val Loss: 0.00236\n",
      "EarlyStopping counter: 1046 out of 30\n",
      "Epoch: 1475/2000... Step: 1475... Loss: 0.00025... Val Loss: 0.00220\n",
      "EarlyStopping counter: 1047 out of 30\n",
      "Epoch: 1476/2000... Step: 1476... Loss: 0.00025... Val Loss: 0.00236\n",
      "EarlyStopping counter: 1048 out of 30\n",
      "Epoch: 1477/2000... Step: 1477... Loss: 0.00027... Val Loss: 0.00224\n",
      "EarlyStopping counter: 1049 out of 30\n",
      "Epoch: 1478/2000... Step: 1478... Loss: 0.00030... Val Loss: 0.00239\n",
      "EarlyStopping counter: 1050 out of 30\n",
      "Epoch: 1479/2000... Step: 1479... Loss: 0.00034... Val Loss: 0.00229\n",
      "EarlyStopping counter: 1051 out of 30\n",
      "Epoch: 1480/2000... Step: 1480... Loss: 0.00035... Val Loss: 0.00243\n",
      "EarlyStopping counter: 1052 out of 30\n",
      "Epoch: 1481/2000... Step: 1481... Loss: 0.00039... Val Loss: 0.00237\n",
      "EarlyStopping counter: 1053 out of 30\n",
      "Epoch: 1482/2000... Step: 1482... Loss: 0.00043... Val Loss: 0.00262\n",
      "EarlyStopping counter: 1054 out of 30\n",
      "Epoch: 1483/2000... Step: 1483... Loss: 0.00049... Val Loss: 0.00239\n",
      "EarlyStopping counter: 1055 out of 30\n",
      "Epoch: 1484/2000... Step: 1484... Loss: 0.00055... Val Loss: 0.00290\n",
      "EarlyStopping counter: 1056 out of 30\n",
      "Epoch: 1485/2000... Step: 1485... Loss: 0.00062... Val Loss: 0.00242\n",
      "EarlyStopping counter: 1057 out of 30\n",
      "Epoch: 1486/2000... Step: 1486... Loss: 0.00069... Val Loss: 0.00321\n",
      "EarlyStopping counter: 1058 out of 30\n",
      "Epoch: 1487/2000... Step: 1487... Loss: 0.00075... Val Loss: 0.00242\n",
      "EarlyStopping counter: 1059 out of 30\n",
      "Epoch: 1488/2000... Step: 1488... Loss: 0.00081... Val Loss: 0.00347\n",
      "EarlyStopping counter: 1060 out of 30\n",
      "Epoch: 1489/2000... Step: 1489... Loss: 0.00086... Val Loss: 0.00246\n",
      "EarlyStopping counter: 1061 out of 30\n",
      "Epoch: 1490/2000... Step: 1490... Loss: 0.00092... Val Loss: 0.00351\n",
      "EarlyStopping counter: 1062 out of 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1491/2000... Step: 1491... Loss: 0.00092... Val Loss: 0.00244\n",
      "EarlyStopping counter: 1063 out of 30\n",
      "Epoch: 1492/2000... Step: 1492... Loss: 0.00092... Val Loss: 0.00337\n",
      "EarlyStopping counter: 1064 out of 30\n",
      "Epoch: 1493/2000... Step: 1493... Loss: 0.00086... Val Loss: 0.00225\n",
      "EarlyStopping counter: 1065 out of 30\n",
      "Epoch: 1494/2000... Step: 1494... Loss: 0.00074... Val Loss: 0.00298\n",
      "EarlyStopping counter: 1066 out of 30\n",
      "Epoch: 1495/2000... Step: 1495... Loss: 0.00061... Val Loss: 0.00213\n",
      "EarlyStopping counter: 1067 out of 30\n",
      "Epoch: 1496/2000... Step: 1496... Loss: 0.00040... Val Loss: 0.00242\n",
      "EarlyStopping counter: 1068 out of 30\n",
      "Epoch: 1497/2000... Step: 1497... Loss: 0.00029... Val Loss: 0.00241\n",
      "EarlyStopping counter: 1069 out of 30\n",
      "Epoch: 1498/2000... Step: 1498... Loss: 0.00033... Val Loss: 0.00224\n",
      "EarlyStopping counter: 1070 out of 30\n",
      "Epoch: 1499/2000... Step: 1499... Loss: 0.00048... Val Loss: 0.00311\n",
      "EarlyStopping counter: 1071 out of 30\n",
      "Epoch: 1500/2000... Step: 1500... Loss: 0.00065... Val Loss: 0.00225\n",
      "EarlyStopping counter: 1072 out of 30\n",
      "Epoch: 1501/2000... Step: 1501... Loss: 0.00069... Val Loss: 0.00312\n",
      "EarlyStopping counter: 1073 out of 30\n",
      "Epoch: 1502/2000... Step: 1502... Loss: 0.00066... Val Loss: 0.00220\n",
      "EarlyStopping counter: 1074 out of 30\n",
      "Epoch: 1503/2000... Step: 1503... Loss: 0.00056... Val Loss: 0.00264\n",
      "EarlyStopping counter: 1075 out of 30\n",
      "Epoch: 1504/2000... Step: 1504... Loss: 0.00050... Val Loss: 0.00244\n",
      "EarlyStopping counter: 1076 out of 30\n",
      "Epoch: 1505/2000... Step: 1505... Loss: 0.00048... Val Loss: 0.00239\n",
      "EarlyStopping counter: 1077 out of 30\n",
      "Epoch: 1506/2000... Step: 1506... Loss: 0.00050... Val Loss: 0.00267\n",
      "EarlyStopping counter: 1078 out of 30\n",
      "Epoch: 1507/2000... Step: 1507... Loss: 0.00050... Val Loss: 0.00228\n",
      "EarlyStopping counter: 1079 out of 30\n",
      "Epoch: 1508/2000... Step: 1508... Loss: 0.00042... Val Loss: 0.00256\n",
      "EarlyStopping counter: 1080 out of 30\n",
      "Epoch: 1509/2000... Step: 1509... Loss: 0.00030... Val Loss: 0.00218\n",
      "EarlyStopping counter: 1081 out of 30\n",
      "Epoch: 1510/2000... Step: 1510... Loss: 0.00024... Val Loss: 0.00216\n",
      "EarlyStopping counter: 1082 out of 30\n",
      "Epoch: 1511/2000... Step: 1511... Loss: 0.00023... Val Loss: 0.00241\n",
      "EarlyStopping counter: 1083 out of 30\n",
      "Epoch: 1512/2000... Step: 1512... Loss: 0.00026... Val Loss: 0.00209\n",
      "EarlyStopping counter: 1084 out of 30\n",
      "Epoch: 1513/2000... Step: 1513... Loss: 0.00025... Val Loss: 0.00243\n",
      "EarlyStopping counter: 1085 out of 30\n",
      "Epoch: 1514/2000... Step: 1514... Loss: 0.00022... Val Loss: 0.00210\n",
      "EarlyStopping counter: 1086 out of 30\n",
      "Epoch: 1515/2000... Step: 1515... Loss: 0.00019... Val Loss: 0.00222\n",
      "EarlyStopping counter: 1087 out of 30\n",
      "Epoch: 1516/2000... Step: 1516... Loss: 0.00019... Val Loss: 0.00230\n",
      "EarlyStopping counter: 1088 out of 30\n",
      "Epoch: 1517/2000... Step: 1517... Loss: 0.00022... Val Loss: 0.00216\n",
      "EarlyStopping counter: 1089 out of 30\n",
      "Epoch: 1518/2000... Step: 1518... Loss: 0.00023... Val Loss: 0.00236\n",
      "EarlyStopping counter: 1090 out of 30\n",
      "Epoch: 1519/2000... Step: 1519... Loss: 0.00023... Val Loss: 0.00216\n",
      "EarlyStopping counter: 1091 out of 30\n",
      "Epoch: 1520/2000... Step: 1520... Loss: 0.00021... Val Loss: 0.00234\n",
      "EarlyStopping counter: 1092 out of 30\n",
      "Epoch: 1521/2000... Step: 1521... Loss: 0.00021... Val Loss: 0.00222\n",
      "EarlyStopping counter: 1093 out of 30\n",
      "Epoch: 1522/2000... Step: 1522... Loss: 0.00023... Val Loss: 0.00237\n",
      "EarlyStopping counter: 1094 out of 30\n",
      "Epoch: 1523/2000... Step: 1523... Loss: 0.00027... Val Loss: 0.00228\n",
      "EarlyStopping counter: 1095 out of 30\n",
      "Epoch: 1524/2000... Step: 1524... Loss: 0.00035... Val Loss: 0.00252\n",
      "EarlyStopping counter: 1096 out of 30\n",
      "Epoch: 1525/2000... Step: 1525... Loss: 0.00044... Val Loss: 0.00244\n",
      "EarlyStopping counter: 1097 out of 30\n",
      "Epoch: 1526/2000... Step: 1526... Loss: 0.00058... Val Loss: 0.00294\n",
      "EarlyStopping counter: 1098 out of 30\n",
      "Epoch: 1527/2000... Step: 1527... Loss: 0.00080... Val Loss: 0.00292\n",
      "EarlyStopping counter: 1099 out of 30\n",
      "Epoch: 1528/2000... Step: 1528... Loss: 0.00111... Val Loss: 0.00394\n",
      "EarlyStopping counter: 1100 out of 30\n",
      "Epoch: 1529/2000... Step: 1529... Loss: 0.00170... Val Loss: 0.00403\n",
      "EarlyStopping counter: 1101 out of 30\n",
      "Epoch: 1530/2000... Step: 1530... Loss: 0.00231... Val Loss: 0.00584\n",
      "EarlyStopping counter: 1102 out of 30\n",
      "Epoch: 1531/2000... Step: 1531... Loss: 0.00351... Val Loss: 0.00607\n",
      "EarlyStopping counter: 1103 out of 30\n",
      "Epoch: 1532/2000... Step: 1532... Loss: 0.00410... Val Loss: 0.00739\n",
      "EarlyStopping counter: 1104 out of 30\n",
      "Epoch: 1533/2000... Step: 1533... Loss: 0.00501... Val Loss: 0.00550\n",
      "EarlyStopping counter: 1105 out of 30\n",
      "Epoch: 1534/2000... Step: 1534... Loss: 0.00329... Val Loss: 0.00348\n",
      "EarlyStopping counter: 1106 out of 30\n",
      "Epoch: 1535/2000... Step: 1535... Loss: 0.00148... Val Loss: 0.00227\n",
      "EarlyStopping counter: 1107 out of 30\n",
      "Epoch: 1536/2000... Step: 1536... Loss: 0.00035... Val Loss: 0.00307\n",
      "EarlyStopping counter: 1108 out of 30\n",
      "Epoch: 1537/2000... Step: 1537... Loss: 0.00101... Val Loss: 0.00429\n",
      "EarlyStopping counter: 1109 out of 30\n",
      "Epoch: 1538/2000... Step: 1538... Loss: 0.00234... Val Loss: 0.00420\n",
      "EarlyStopping counter: 1110 out of 30\n",
      "Epoch: 1539/2000... Step: 1539... Loss: 0.00219... Val Loss: 0.00330\n",
      "EarlyStopping counter: 1111 out of 30\n",
      "Epoch: 1540/2000... Step: 1540... Loss: 0.00126... Val Loss: 0.00223\n",
      "EarlyStopping counter: 1112 out of 30\n",
      "Epoch: 1541/2000... Step: 1541... Loss: 0.00034... Val Loss: 0.00253\n",
      "EarlyStopping counter: 1113 out of 30\n",
      "Epoch: 1542/2000... Step: 1542... Loss: 0.00060... Val Loss: 0.00325\n",
      "EarlyStopping counter: 1114 out of 30\n",
      "Epoch: 1543/2000... Step: 1543... Loss: 0.00143... Val Loss: 0.00354\n",
      "EarlyStopping counter: 1115 out of 30\n",
      "Epoch: 1544/2000... Step: 1544... Loss: 0.00140... Val Loss: 0.00275\n",
      "EarlyStopping counter: 1116 out of 30\n",
      "Epoch: 1545/2000... Step: 1545... Loss: 0.00083... Val Loss: 0.00226\n",
      "EarlyStopping counter: 1117 out of 30\n",
      "Epoch: 1546/2000... Step: 1546... Loss: 0.00030... Val Loss: 0.00259\n",
      "EarlyStopping counter: 1118 out of 30\n",
      "Epoch: 1547/2000... Step: 1547... Loss: 0.00048... Val Loss: 0.00286\n",
      "EarlyStopping counter: 1119 out of 30\n",
      "Epoch: 1548/2000... Step: 1548... Loss: 0.00092... Val Loss: 0.00282\n",
      "EarlyStopping counter: 1120 out of 30\n",
      "Epoch: 1549/2000... Step: 1549... Loss: 0.00092... Val Loss: 0.00277\n",
      "EarlyStopping counter: 1121 out of 30\n",
      "Epoch: 1550/2000... Step: 1550... Loss: 0.00062... Val Loss: 0.00219\n",
      "EarlyStopping counter: 1122 out of 30\n",
      "Epoch: 1551/2000... Step: 1551... Loss: 0.00041... Val Loss: 0.00262\n",
      "EarlyStopping counter: 1123 out of 30\n",
      "Epoch: 1552/2000... Step: 1552... Loss: 0.00049... Val Loss: 0.00249\n",
      "EarlyStopping counter: 1124 out of 30\n",
      "Epoch: 1553/2000... Step: 1553... Loss: 0.00060... Val Loss: 0.00257\n",
      "EarlyStopping counter: 1125 out of 30\n",
      "Epoch: 1554/2000... Step: 1554... Loss: 0.00052... Val Loss: 0.00242\n",
      "EarlyStopping counter: 1126 out of 30\n",
      "Epoch: 1555/2000... Step: 1555... Loss: 0.00041... Val Loss: 0.00222\n",
      "EarlyStopping counter: 1127 out of 30\n",
      "Epoch: 1556/2000... Step: 1556... Loss: 0.00040... Val Loss: 0.00250\n",
      "EarlyStopping counter: 1128 out of 30\n",
      "Epoch: 1557/2000... Step: 1557... Loss: 0.00048... Val Loss: 0.00222\n",
      "EarlyStopping counter: 1129 out of 30\n",
      "Epoch: 1558/2000... Step: 1558... Loss: 0.00047... Val Loss: 0.00237\n",
      "EarlyStopping counter: 1130 out of 30\n",
      "Epoch: 1559/2000... Step: 1559... Loss: 0.00032... Val Loss: 0.00231\n",
      "EarlyStopping counter: 1131 out of 30\n",
      "Epoch: 1560/2000... Step: 1560... Loss: 0.00023... Val Loss: 0.00206\n",
      "EarlyStopping counter: 1132 out of 30\n",
      "Epoch: 1561/2000... Step: 1561... Loss: 0.00027... Val Loss: 0.00247\n",
      "EarlyStopping counter: 1133 out of 30\n",
      "Epoch: 1562/2000... Step: 1562... Loss: 0.00034... Val Loss: 0.00199\n",
      "EarlyStopping counter: 1134 out of 30\n",
      "Epoch: 1563/2000... Step: 1563... Loss: 0.00030... Val Loss: 0.00234\n",
      "EarlyStopping counter: 1135 out of 30\n",
      "Epoch: 1564/2000... Step: 1564... Loss: 0.00023... Val Loss: 0.00219\n",
      "EarlyStopping counter: 1136 out of 30\n",
      "Epoch: 1565/2000... Step: 1565... Loss: 0.00022... Val Loss: 0.00222\n",
      "EarlyStopping counter: 1137 out of 30\n",
      "Epoch: 1566/2000... Step: 1566... Loss: 0.00024... Val Loss: 0.00232\n",
      "EarlyStopping counter: 1138 out of 30\n",
      "Epoch: 1567/2000... Step: 1567... Loss: 0.00028... Val Loss: 0.00222\n",
      "EarlyStopping counter: 1139 out of 30\n",
      "Epoch: 1568/2000... Step: 1568... Loss: 0.00021... Val Loss: 0.00219\n",
      "EarlyStopping counter: 1140 out of 30\n",
      "Epoch: 1569/2000... Step: 1569... Loss: 0.00017... Val Loss: 0.00225\n",
      "EarlyStopping counter: 1141 out of 30\n",
      "Epoch: 1570/2000... Step: 1570... Loss: 0.00019... Val Loss: 0.00209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1142 out of 30\n",
      "Epoch: 1571/2000... Step: 1571... Loss: 0.00022... Val Loss: 0.00227\n",
      "EarlyStopping counter: 1143 out of 30\n",
      "Epoch: 1572/2000... Step: 1572... Loss: 0.00021... Val Loss: 0.00215\n",
      "EarlyStopping counter: 1144 out of 30\n",
      "Epoch: 1573/2000... Step: 1573... Loss: 0.00018... Val Loss: 0.00217\n",
      "EarlyStopping counter: 1145 out of 30\n",
      "Epoch: 1574/2000... Step: 1574... Loss: 0.00016... Val Loss: 0.00227\n",
      "EarlyStopping counter: 1146 out of 30\n",
      "Epoch: 1575/2000... Step: 1575... Loss: 0.00018... Val Loss: 0.00215\n",
      "EarlyStopping counter: 1147 out of 30\n",
      "Epoch: 1576/2000... Step: 1576... Loss: 0.00019... Val Loss: 0.00226\n",
      "EarlyStopping counter: 1148 out of 30\n",
      "Epoch: 1577/2000... Step: 1577... Loss: 0.00018... Val Loss: 0.00219\n",
      "EarlyStopping counter: 1149 out of 30\n",
      "Epoch: 1578/2000... Step: 1578... Loss: 0.00017... Val Loss: 0.00222\n",
      "EarlyStopping counter: 1150 out of 30\n",
      "Epoch: 1579/2000... Step: 1579... Loss: 0.00017... Val Loss: 0.00220\n",
      "EarlyStopping counter: 1151 out of 30\n",
      "Epoch: 1580/2000... Step: 1580... Loss: 0.00018... Val Loss: 0.00226\n",
      "EarlyStopping counter: 1152 out of 30\n",
      "Epoch: 1581/2000... Step: 1581... Loss: 0.00019... Val Loss: 0.00221\n",
      "EarlyStopping counter: 1153 out of 30\n",
      "Epoch: 1582/2000... Step: 1582... Loss: 0.00019... Val Loss: 0.00235\n",
      "EarlyStopping counter: 1154 out of 30\n",
      "Epoch: 1583/2000... Step: 1583... Loss: 0.00019... Val Loss: 0.00210\n",
      "EarlyStopping counter: 1155 out of 30\n",
      "Epoch: 1584/2000... Step: 1584... Loss: 0.00021... Val Loss: 0.00250\n",
      "EarlyStopping counter: 1156 out of 30\n",
      "Epoch: 1585/2000... Step: 1585... Loss: 0.00025... Val Loss: 0.00206\n",
      "EarlyStopping counter: 1157 out of 30\n",
      "Epoch: 1586/2000... Step: 1586... Loss: 0.00030... Val Loss: 0.00274\n",
      "EarlyStopping counter: 1158 out of 30\n",
      "Epoch: 1587/2000... Step: 1587... Loss: 0.00037... Val Loss: 0.00212\n",
      "EarlyStopping counter: 1159 out of 30\n",
      "Epoch: 1588/2000... Step: 1588... Loss: 0.00047... Val Loss: 0.00317\n",
      "EarlyStopping counter: 1160 out of 30\n",
      "Epoch: 1589/2000... Step: 1589... Loss: 0.00063... Val Loss: 0.00258\n",
      "EarlyStopping counter: 1161 out of 30\n",
      "Epoch: 1590/2000... Step: 1590... Loss: 0.00096... Val Loss: 0.00408\n",
      "EarlyStopping counter: 1162 out of 30\n",
      "Epoch: 1591/2000... Step: 1591... Loss: 0.00128... Val Loss: 0.00331\n",
      "EarlyStopping counter: 1163 out of 30\n",
      "Epoch: 1592/2000... Step: 1592... Loss: 0.00187... Val Loss: 0.00535\n",
      "EarlyStopping counter: 1164 out of 30\n",
      "Epoch: 1593/2000... Step: 1593... Loss: 0.00219... Val Loss: 0.00395\n",
      "EarlyStopping counter: 1165 out of 30\n",
      "Epoch: 1594/2000... Step: 1594... Loss: 0.00256... Val Loss: 0.00528\n",
      "EarlyStopping counter: 1166 out of 30\n",
      "Epoch: 1595/2000... Step: 1595... Loss: 0.00221... Val Loss: 0.00319\n",
      "EarlyStopping counter: 1167 out of 30\n",
      "Epoch: 1596/2000... Step: 1596... Loss: 0.00177... Val Loss: 0.00331\n",
      "EarlyStopping counter: 1168 out of 30\n",
      "Epoch: 1597/2000... Step: 1597... Loss: 0.00099... Val Loss: 0.00232\n",
      "EarlyStopping counter: 1169 out of 30\n",
      "Epoch: 1598/2000... Step: 1598... Loss: 0.00056... Val Loss: 0.00246\n",
      "EarlyStopping counter: 1170 out of 30\n",
      "Epoch: 1599/2000... Step: 1599... Loss: 0.00066... Val Loss: 0.00310\n",
      "EarlyStopping counter: 1171 out of 30\n",
      "Epoch: 1600/2000... Step: 1600... Loss: 0.00091... Val Loss: 0.00299\n",
      "EarlyStopping counter: 1172 out of 30\n",
      "Epoch: 1601/2000... Step: 1601... Loss: 0.00118... Val Loss: 0.00336\n",
      "EarlyStopping counter: 1173 out of 30\n",
      "Epoch: 1602/2000... Step: 1602... Loss: 0.00109... Val Loss: 0.00294\n",
      "EarlyStopping counter: 1174 out of 30\n",
      "Epoch: 1603/2000... Step: 1603... Loss: 0.00093... Val Loss: 0.00263\n",
      "EarlyStopping counter: 1175 out of 30\n",
      "Epoch: 1604/2000... Step: 1604... Loss: 0.00072... Val Loss: 0.00255\n",
      "EarlyStopping counter: 1176 out of 30\n",
      "Epoch: 1605/2000... Step: 1605... Loss: 0.00058... Val Loss: 0.00240\n",
      "EarlyStopping counter: 1177 out of 30\n",
      "Epoch: 1606/2000... Step: 1606... Loss: 0.00066... Val Loss: 0.00265\n",
      "EarlyStopping counter: 1178 out of 30\n",
      "Epoch: 1607/2000... Step: 1607... Loss: 0.00063... Val Loss: 0.00250\n",
      "EarlyStopping counter: 1179 out of 30\n",
      "Epoch: 1608/2000... Step: 1608... Loss: 0.00065... Val Loss: 0.00244\n",
      "EarlyStopping counter: 1180 out of 30\n",
      "Epoch: 1609/2000... Step: 1609... Loss: 0.00044... Val Loss: 0.00250\n",
      "EarlyStopping counter: 1181 out of 30\n",
      "Epoch: 1610/2000... Step: 1610... Loss: 0.00035... Val Loss: 0.00236\n",
      "EarlyStopping counter: 1182 out of 30\n",
      "Epoch: 1611/2000... Step: 1611... Loss: 0.00049... Val Loss: 0.00262\n",
      "EarlyStopping counter: 1183 out of 30\n",
      "Epoch: 1612/2000... Step: 1612... Loss: 0.00061... Val Loss: 0.00256\n",
      "EarlyStopping counter: 1184 out of 30\n",
      "Epoch: 1613/2000... Step: 1613... Loss: 0.00061... Val Loss: 0.00239\n",
      "EarlyStopping counter: 1185 out of 30\n",
      "Epoch: 1614/2000... Step: 1614... Loss: 0.00041... Val Loss: 0.00220\n",
      "EarlyStopping counter: 1186 out of 30\n",
      "Epoch: 1615/2000... Step: 1615... Loss: 0.00028... Val Loss: 0.00254\n",
      "EarlyStopping counter: 1187 out of 30\n",
      "Epoch: 1616/2000... Step: 1616... Loss: 0.00035... Val Loss: 0.00237\n",
      "EarlyStopping counter: 1188 out of 30\n",
      "Epoch: 1617/2000... Step: 1617... Loss: 0.00038... Val Loss: 0.00237\n",
      "EarlyStopping counter: 1189 out of 30\n",
      "Epoch: 1618/2000... Step: 1618... Loss: 0.00032... Val Loss: 0.00236\n",
      "EarlyStopping counter: 1190 out of 30\n",
      "Epoch: 1619/2000... Step: 1619... Loss: 0.00020... Val Loss: 0.00212\n",
      "EarlyStopping counter: 1191 out of 30\n",
      "Epoch: 1620/2000... Step: 1620... Loss: 0.00022... Val Loss: 0.00244\n",
      "EarlyStopping counter: 1192 out of 30\n",
      "Epoch: 1621/2000... Step: 1621... Loss: 0.00029... Val Loss: 0.00217\n",
      "EarlyStopping counter: 1193 out of 30\n",
      "Epoch: 1622/2000... Step: 1622... Loss: 0.00027... Val Loss: 0.00228\n",
      "EarlyStopping counter: 1194 out of 30\n",
      "Epoch: 1623/2000... Step: 1623... Loss: 0.00025... Val Loss: 0.00223\n",
      "EarlyStopping counter: 1195 out of 30\n",
      "Epoch: 1624/2000... Step: 1624... Loss: 0.00025... Val Loss: 0.00220\n",
      "EarlyStopping counter: 1196 out of 30\n",
      "Epoch: 1625/2000... Step: 1625... Loss: 0.00031... Val Loss: 0.00230\n",
      "EarlyStopping counter: 1197 out of 30\n",
      "Epoch: 1626/2000... Step: 1626... Loss: 0.00029... Val Loss: 0.00226\n",
      "EarlyStopping counter: 1198 out of 30\n",
      "Epoch: 1627/2000... Step: 1627... Loss: 0.00024... Val Loss: 0.00222\n",
      "EarlyStopping counter: 1199 out of 30\n",
      "Epoch: 1628/2000... Step: 1628... Loss: 0.00025... Val Loss: 0.00236\n",
      "EarlyStopping counter: 1200 out of 30\n",
      "Epoch: 1629/2000... Step: 1629... Loss: 0.00029... Val Loss: 0.00236\n",
      "EarlyStopping counter: 1201 out of 30\n",
      "Epoch: 1630/2000... Step: 1630... Loss: 0.00029... Val Loss: 0.00220\n",
      "EarlyStopping counter: 1202 out of 30\n",
      "Epoch: 1631/2000... Step: 1631... Loss: 0.00028... Val Loss: 0.00250\n",
      "EarlyStopping counter: 1203 out of 30\n",
      "Epoch: 1632/2000... Step: 1632... Loss: 0.00030... Val Loss: 0.00212\n",
      "EarlyStopping counter: 1204 out of 30\n",
      "Epoch: 1633/2000... Step: 1633... Loss: 0.00033... Val Loss: 0.00262\n",
      "EarlyStopping counter: 1205 out of 30\n",
      "Epoch: 1634/2000... Step: 1634... Loss: 0.00038... Val Loss: 0.00214\n",
      "EarlyStopping counter: 1206 out of 30\n",
      "Epoch: 1635/2000... Step: 1635... Loss: 0.00036... Val Loss: 0.00258\n",
      "EarlyStopping counter: 1207 out of 30\n",
      "Epoch: 1636/2000... Step: 1636... Loss: 0.00036... Val Loss: 0.00234\n",
      "EarlyStopping counter: 1208 out of 30\n",
      "Epoch: 1637/2000... Step: 1637... Loss: 0.00036... Val Loss: 0.00258\n",
      "EarlyStopping counter: 1209 out of 30\n",
      "Epoch: 1638/2000... Step: 1638... Loss: 0.00039... Val Loss: 0.00243\n",
      "EarlyStopping counter: 1210 out of 30\n",
      "Epoch: 1639/2000... Step: 1639... Loss: 0.00039... Val Loss: 0.00250\n",
      "EarlyStopping counter: 1211 out of 30\n",
      "Epoch: 1640/2000... Step: 1640... Loss: 0.00038... Val Loss: 0.00234\n",
      "EarlyStopping counter: 1212 out of 30\n",
      "Epoch: 1641/2000... Step: 1641... Loss: 0.00040... Val Loss: 0.00253\n",
      "EarlyStopping counter: 1213 out of 30\n",
      "Epoch: 1642/2000... Step: 1642... Loss: 0.00044... Val Loss: 0.00238\n",
      "EarlyStopping counter: 1214 out of 30\n",
      "Epoch: 1643/2000... Step: 1643... Loss: 0.00051... Val Loss: 0.00255\n",
      "EarlyStopping counter: 1215 out of 30\n",
      "Epoch: 1644/2000... Step: 1644... Loss: 0.00055... Val Loss: 0.00264\n",
      "EarlyStopping counter: 1216 out of 30\n",
      "Epoch: 1645/2000... Step: 1645... Loss: 0.00061... Val Loss: 0.00258\n",
      "EarlyStopping counter: 1217 out of 30\n",
      "Epoch: 1646/2000... Step: 1646... Loss: 0.00066... Val Loss: 0.00287\n",
      "EarlyStopping counter: 1218 out of 30\n",
      "Epoch: 1647/2000... Step: 1647... Loss: 0.00075... Val Loss: 0.00254\n",
      "EarlyStopping counter: 1219 out of 30\n",
      "Epoch: 1648/2000... Step: 1648... Loss: 0.00080... Val Loss: 0.00323\n",
      "EarlyStopping counter: 1220 out of 30\n",
      "Epoch: 1649/2000... Step: 1649... Loss: 0.00084... Val Loss: 0.00251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1221 out of 30\n",
      "Epoch: 1650/2000... Step: 1650... Loss: 0.00087... Val Loss: 0.00325\n",
      "EarlyStopping counter: 1222 out of 30\n",
      "Epoch: 1651/2000... Step: 1651... Loss: 0.00079... Val Loss: 0.00236\n",
      "EarlyStopping counter: 1223 out of 30\n",
      "Epoch: 1652/2000... Step: 1652... Loss: 0.00064... Val Loss: 0.00264\n",
      "EarlyStopping counter: 1224 out of 30\n",
      "Epoch: 1653/2000... Step: 1653... Loss: 0.00041... Val Loss: 0.00230\n",
      "EarlyStopping counter: 1225 out of 30\n",
      "Epoch: 1654/2000... Step: 1654... Loss: 0.00028... Val Loss: 0.00211\n",
      "EarlyStopping counter: 1226 out of 30\n",
      "Epoch: 1655/2000... Step: 1655... Loss: 0.00025... Val Loss: 0.00270\n",
      "EarlyStopping counter: 1227 out of 30\n",
      "Epoch: 1656/2000... Step: 1656... Loss: 0.00034... Val Loss: 0.00209\n",
      "EarlyStopping counter: 1228 out of 30\n",
      "Epoch: 1657/2000... Step: 1657... Loss: 0.00043... Val Loss: 0.00282\n",
      "EarlyStopping counter: 1229 out of 30\n",
      "Epoch: 1658/2000... Step: 1658... Loss: 0.00043... Val Loss: 0.00213\n",
      "EarlyStopping counter: 1230 out of 30\n",
      "Epoch: 1659/2000... Step: 1659... Loss: 0.00034... Val Loss: 0.00229\n",
      "EarlyStopping counter: 1231 out of 30\n",
      "Epoch: 1660/2000... Step: 1660... Loss: 0.00021... Val Loss: 0.00236\n",
      "EarlyStopping counter: 1232 out of 30\n",
      "Epoch: 1661/2000... Step: 1661... Loss: 0.00018... Val Loss: 0.00208\n",
      "EarlyStopping counter: 1233 out of 30\n",
      "Epoch: 1662/2000... Step: 1662... Loss: 0.00025... Val Loss: 0.00272\n",
      "EarlyStopping counter: 1234 out of 30\n",
      "Epoch: 1663/2000... Step: 1663... Loss: 0.00033... Val Loss: 0.00209\n",
      "EarlyStopping counter: 1235 out of 30\n",
      "Epoch: 1664/2000... Step: 1664... Loss: 0.00033... Val Loss: 0.00262\n",
      "EarlyStopping counter: 1236 out of 30\n",
      "Epoch: 1665/2000... Step: 1665... Loss: 0.00028... Val Loss: 0.00213\n",
      "EarlyStopping counter: 1237 out of 30\n",
      "Epoch: 1666/2000... Step: 1666... Loss: 0.00026... Val Loss: 0.00241\n",
      "EarlyStopping counter: 1238 out of 30\n",
      "Epoch: 1667/2000... Step: 1667... Loss: 0.00032... Val Loss: 0.00246\n",
      "EarlyStopping counter: 1239 out of 30\n",
      "Epoch: 1668/2000... Step: 1668... Loss: 0.00042... Val Loss: 0.00255\n",
      "EarlyStopping counter: 1240 out of 30\n",
      "Epoch: 1669/2000... Step: 1669... Loss: 0.00057... Val Loss: 0.00269\n",
      "EarlyStopping counter: 1241 out of 30\n",
      "Epoch: 1670/2000... Step: 1670... Loss: 0.00073... Val Loss: 0.00331\n",
      "EarlyStopping counter: 1242 out of 30\n",
      "Epoch: 1671/2000... Step: 1671... Loss: 0.00110... Val Loss: 0.00326\n",
      "EarlyStopping counter: 1243 out of 30\n",
      "Epoch: 1672/2000... Step: 1672... Loss: 0.00167... Val Loss: 0.00508\n",
      "EarlyStopping counter: 1244 out of 30\n",
      "Epoch: 1673/2000... Step: 1673... Loss: 0.00259... Val Loss: 0.00521\n",
      "EarlyStopping counter: 1245 out of 30\n",
      "Epoch: 1674/2000... Step: 1674... Loss: 0.00373... Val Loss: 0.00773\n",
      "EarlyStopping counter: 1246 out of 30\n",
      "Epoch: 1675/2000... Step: 1675... Loss: 0.00487... Val Loss: 0.00675\n",
      "EarlyStopping counter: 1247 out of 30\n",
      "Epoch: 1676/2000... Step: 1676... Loss: 0.00492... Val Loss: 0.00602\n",
      "EarlyStopping counter: 1248 out of 30\n",
      "Epoch: 1677/2000... Step: 1677... Loss: 0.00394... Val Loss: 0.00385\n",
      "EarlyStopping counter: 1249 out of 30\n",
      "Epoch: 1678/2000... Step: 1678... Loss: 0.00152... Val Loss: 0.00243\n",
      "EarlyStopping counter: 1250 out of 30\n",
      "Epoch: 1679/2000... Step: 1679... Loss: 0.00078... Val Loss: 0.00403\n",
      "EarlyStopping counter: 1251 out of 30\n",
      "Epoch: 1680/2000... Step: 1680... Loss: 0.00176... Val Loss: 0.00495\n",
      "EarlyStopping counter: 1252 out of 30\n",
      "Epoch: 1681/2000... Step: 1681... Loss: 0.00278... Val Loss: 0.00450\n",
      "EarlyStopping counter: 1253 out of 30\n",
      "Epoch: 1682/2000... Step: 1682... Loss: 0.00256... Val Loss: 0.00340\n",
      "EarlyStopping counter: 1254 out of 30\n",
      "Epoch: 1683/2000... Step: 1683... Loss: 0.00119... Val Loss: 0.00231\n",
      "EarlyStopping counter: 1255 out of 30\n",
      "Epoch: 1684/2000... Step: 1684... Loss: 0.00054... Val Loss: 0.00324\n",
      "EarlyStopping counter: 1256 out of 30\n",
      "Epoch: 1685/2000... Step: 1685... Loss: 0.00119... Val Loss: 0.00392\n",
      "EarlyStopping counter: 1257 out of 30\n",
      "Epoch: 1686/2000... Step: 1686... Loss: 0.00210... Val Loss: 0.00393\n",
      "EarlyStopping counter: 1258 out of 30\n",
      "Epoch: 1687/2000... Step: 1687... Loss: 0.00179... Val Loss: 0.00257\n",
      "EarlyStopping counter: 1259 out of 30\n",
      "Epoch: 1688/2000... Step: 1688... Loss: 0.00081... Val Loss: 0.00227\n",
      "EarlyStopping counter: 1260 out of 30\n",
      "Epoch: 1689/2000... Step: 1689... Loss: 0.00034... Val Loss: 0.00290\n",
      "EarlyStopping counter: 1261 out of 30\n",
      "Epoch: 1690/2000... Step: 1690... Loss: 0.00091... Val Loss: 0.00332\n",
      "EarlyStopping counter: 1262 out of 30\n",
      "Epoch: 1691/2000... Step: 1691... Loss: 0.00140... Val Loss: 0.00305\n",
      "EarlyStopping counter: 1263 out of 30\n",
      "Epoch: 1692/2000... Step: 1692... Loss: 0.00106... Val Loss: 0.00251\n",
      "EarlyStopping counter: 1264 out of 30\n",
      "Epoch: 1693/2000... Step: 1693... Loss: 0.00043... Val Loss: 0.00242\n",
      "EarlyStopping counter: 1265 out of 30\n",
      "Epoch: 1694/2000... Step: 1694... Loss: 0.00035... Val Loss: 0.00276\n",
      "EarlyStopping counter: 1266 out of 30\n",
      "Epoch: 1695/2000... Step: 1695... Loss: 0.00073... Val Loss: 0.00290\n",
      "EarlyStopping counter: 1267 out of 30\n",
      "Epoch: 1696/2000... Step: 1696... Loss: 0.00088... Val Loss: 0.00283\n",
      "EarlyStopping counter: 1268 out of 30\n",
      "Epoch: 1697/2000... Step: 1697... Loss: 0.00063... Val Loss: 0.00222\n",
      "EarlyStopping counter: 1269 out of 30\n",
      "Epoch: 1698/2000... Step: 1698... Loss: 0.00055... Val Loss: 0.00270\n",
      "EarlyStopping counter: 1270 out of 30\n",
      "Epoch: 1699/2000... Step: 1699... Loss: 0.00055... Val Loss: 0.00247\n",
      "EarlyStopping counter: 1271 out of 30\n",
      "Epoch: 1700/2000... Step: 1700... Loss: 0.00048... Val Loss: 0.00226\n",
      "EarlyStopping counter: 1272 out of 30\n",
      "Epoch: 1701/2000... Step: 1701... Loss: 0.00046... Val Loss: 0.00257\n",
      "EarlyStopping counter: 1273 out of 30\n",
      "Epoch: 1702/2000... Step: 1702... Loss: 0.00032... Val Loss: 0.00231\n",
      "EarlyStopping counter: 1274 out of 30\n",
      "Epoch: 1703/2000... Step: 1703... Loss: 0.00035... Val Loss: 0.00235\n",
      "EarlyStopping counter: 1275 out of 30\n",
      "Epoch: 1704/2000... Step: 1704... Loss: 0.00042... Val Loss: 0.00237\n",
      "EarlyStopping counter: 1276 out of 30\n",
      "Epoch: 1705/2000... Step: 1705... Loss: 0.00035... Val Loss: 0.00218\n",
      "EarlyStopping counter: 1277 out of 30\n",
      "Epoch: 1706/2000... Step: 1706... Loss: 0.00024... Val Loss: 0.00252\n",
      "EarlyStopping counter: 1278 out of 30\n",
      "Epoch: 1707/2000... Step: 1707... Loss: 0.00029... Val Loss: 0.00215\n",
      "EarlyStopping counter: 1279 out of 30\n",
      "Epoch: 1708/2000... Step: 1708... Loss: 0.00031... Val Loss: 0.00249\n",
      "EarlyStopping counter: 1280 out of 30\n",
      "Epoch: 1709/2000... Step: 1709... Loss: 0.00027... Val Loss: 0.00219\n",
      "EarlyStopping counter: 1281 out of 30\n",
      "Epoch: 1710/2000... Step: 1710... Loss: 0.00021... Val Loss: 0.00217\n",
      "EarlyStopping counter: 1282 out of 30\n",
      "Epoch: 1711/2000... Step: 1711... Loss: 0.00024... Val Loss: 0.00249\n",
      "EarlyStopping counter: 1283 out of 30\n",
      "Epoch: 1712/2000... Step: 1712... Loss: 0.00025... Val Loss: 0.00212\n",
      "EarlyStopping counter: 1284 out of 30\n",
      "Epoch: 1713/2000... Step: 1713... Loss: 0.00024... Val Loss: 0.00225\n",
      "EarlyStopping counter: 1285 out of 30\n",
      "Epoch: 1714/2000... Step: 1714... Loss: 0.00021... Val Loss: 0.00214\n",
      "EarlyStopping counter: 1286 out of 30\n",
      "Epoch: 1715/2000... Step: 1715... Loss: 0.00020... Val Loss: 0.00225\n",
      "EarlyStopping counter: 1287 out of 30\n",
      "Epoch: 1716/2000... Step: 1716... Loss: 0.00023... Val Loss: 0.00229\n",
      "EarlyStopping counter: 1288 out of 30\n",
      "Epoch: 1717/2000... Step: 1717... Loss: 0.00021... Val Loss: 0.00217\n",
      "EarlyStopping counter: 1289 out of 30\n",
      "Epoch: 1718/2000... Step: 1718... Loss: 0.00017... Val Loss: 0.00224\n",
      "EarlyStopping counter: 1290 out of 30\n",
      "Epoch: 1719/2000... Step: 1719... Loss: 0.00020... Val Loss: 0.00229\n",
      "EarlyStopping counter: 1291 out of 30\n",
      "Epoch: 1720/2000... Step: 1720... Loss: 0.00021... Val Loss: 0.00215\n",
      "EarlyStopping counter: 1292 out of 30\n",
      "Epoch: 1721/2000... Step: 1721... Loss: 0.00017... Val Loss: 0.00217\n",
      "EarlyStopping counter: 1293 out of 30\n",
      "Epoch: 1722/2000... Step: 1722... Loss: 0.00018... Val Loss: 0.00231\n",
      "EarlyStopping counter: 1294 out of 30\n",
      "Epoch: 1723/2000... Step: 1723... Loss: 0.00018... Val Loss: 0.00219\n",
      "EarlyStopping counter: 1295 out of 30\n",
      "Epoch: 1724/2000... Step: 1724... Loss: 0.00018... Val Loss: 0.00226\n",
      "EarlyStopping counter: 1296 out of 30\n",
      "Epoch: 1725/2000... Step: 1725... Loss: 0.00018... Val Loss: 0.00223\n",
      "EarlyStopping counter: 1297 out of 30\n",
      "Epoch: 1726/2000... Step: 1726... Loss: 0.00018... Val Loss: 0.00219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1298 out of 30\n",
      "Epoch: 1727/2000... Step: 1727... Loss: 0.00016... Val Loss: 0.00218\n",
      "EarlyStopping counter: 1299 out of 30\n",
      "Epoch: 1728/2000... Step: 1728... Loss: 0.00016... Val Loss: 0.00230\n",
      "EarlyStopping counter: 1300 out of 30\n",
      "Epoch: 1729/2000... Step: 1729... Loss: 0.00018... Val Loss: 0.00212\n",
      "EarlyStopping counter: 1301 out of 30\n",
      "Epoch: 1730/2000... Step: 1730... Loss: 0.00016... Val Loss: 0.00230\n",
      "EarlyStopping counter: 1302 out of 30\n",
      "Epoch: 1731/2000... Step: 1731... Loss: 0.00016... Val Loss: 0.00218\n",
      "EarlyStopping counter: 1303 out of 30\n",
      "Epoch: 1732/2000... Step: 1732... Loss: 0.00017... Val Loss: 0.00230\n",
      "EarlyStopping counter: 1304 out of 30\n",
      "Epoch: 1733/2000... Step: 1733... Loss: 0.00018... Val Loss: 0.00212\n",
      "EarlyStopping counter: 1305 out of 30\n",
      "Epoch: 1734/2000... Step: 1734... Loss: 0.00017... Val Loss: 0.00233\n",
      "EarlyStopping counter: 1306 out of 30\n",
      "Epoch: 1735/2000... Step: 1735... Loss: 0.00020... Val Loss: 0.00217\n",
      "EarlyStopping counter: 1307 out of 30\n",
      "Epoch: 1736/2000... Step: 1736... Loss: 0.00022... Val Loss: 0.00230\n",
      "EarlyStopping counter: 1308 out of 30\n",
      "Epoch: 1737/2000... Step: 1737... Loss: 0.00026... Val Loss: 0.00223\n",
      "EarlyStopping counter: 1309 out of 30\n",
      "Epoch: 1738/2000... Step: 1738... Loss: 0.00031... Val Loss: 0.00238\n",
      "EarlyStopping counter: 1310 out of 30\n",
      "Epoch: 1739/2000... Step: 1739... Loss: 0.00041... Val Loss: 0.00227\n",
      "EarlyStopping counter: 1311 out of 30\n",
      "Epoch: 1740/2000... Step: 1740... Loss: 0.00045... Val Loss: 0.00261\n",
      "EarlyStopping counter: 1312 out of 30\n",
      "Epoch: 1741/2000... Step: 1741... Loss: 0.00058... Val Loss: 0.00247\n",
      "EarlyStopping counter: 1313 out of 30\n",
      "Epoch: 1742/2000... Step: 1742... Loss: 0.00068... Val Loss: 0.00299\n",
      "EarlyStopping counter: 1314 out of 30\n",
      "Epoch: 1743/2000... Step: 1743... Loss: 0.00084... Val Loss: 0.00258\n",
      "EarlyStopping counter: 1315 out of 30\n",
      "Epoch: 1744/2000... Step: 1744... Loss: 0.00094... Val Loss: 0.00339\n",
      "EarlyStopping counter: 1316 out of 30\n",
      "Epoch: 1745/2000... Step: 1745... Loss: 0.00100... Val Loss: 0.00252\n",
      "EarlyStopping counter: 1317 out of 30\n",
      "Epoch: 1746/2000... Step: 1746... Loss: 0.00091... Val Loss: 0.00330\n",
      "EarlyStopping counter: 1318 out of 30\n",
      "Epoch: 1747/2000... Step: 1747... Loss: 0.00077... Val Loss: 0.00234\n",
      "EarlyStopping counter: 1319 out of 30\n",
      "Epoch: 1748/2000... Step: 1748... Loss: 0.00062... Val Loss: 0.00267\n",
      "EarlyStopping counter: 1320 out of 30\n",
      "Epoch: 1749/2000... Step: 1749... Loss: 0.00044... Val Loss: 0.00240\n",
      "EarlyStopping counter: 1321 out of 30\n",
      "Epoch: 1750/2000... Step: 1750... Loss: 0.00033... Val Loss: 0.00207\n",
      "EarlyStopping counter: 1322 out of 30\n",
      "Epoch: 1751/2000... Step: 1751... Loss: 0.00033... Val Loss: 0.00297\n",
      "EarlyStopping counter: 1323 out of 30\n",
      "Epoch: 1752/2000... Step: 1752... Loss: 0.00046... Val Loss: 0.00205\n",
      "EarlyStopping counter: 1324 out of 30\n",
      "Epoch: 1753/2000... Step: 1753... Loss: 0.00050... Val Loss: 0.00283\n",
      "EarlyStopping counter: 1325 out of 30\n",
      "Epoch: 1754/2000... Step: 1754... Loss: 0.00042... Val Loss: 0.00206\n",
      "EarlyStopping counter: 1326 out of 30\n",
      "Epoch: 1755/2000... Step: 1755... Loss: 0.00032... Val Loss: 0.00227\n",
      "EarlyStopping counter: 1327 out of 30\n",
      "Epoch: 1756/2000... Step: 1756... Loss: 0.00030... Val Loss: 0.00260\n",
      "EarlyStopping counter: 1328 out of 30\n",
      "Epoch: 1757/2000... Step: 1757... Loss: 0.00039... Val Loss: 0.00235\n",
      "EarlyStopping counter: 1329 out of 30\n",
      "Epoch: 1758/2000... Step: 1758... Loss: 0.00046... Val Loss: 0.00259\n",
      "EarlyStopping counter: 1330 out of 30\n",
      "Epoch: 1759/2000... Step: 1759... Loss: 0.00046... Val Loss: 0.00263\n",
      "EarlyStopping counter: 1331 out of 30\n",
      "Epoch: 1760/2000... Step: 1760... Loss: 0.00052... Val Loss: 0.00228\n",
      "EarlyStopping counter: 1332 out of 30\n",
      "Epoch: 1761/2000... Step: 1761... Loss: 0.00062... Val Loss: 0.00324\n",
      "EarlyStopping counter: 1333 out of 30\n",
      "Epoch: 1762/2000... Step: 1762... Loss: 0.00084... Val Loss: 0.00257\n",
      "EarlyStopping counter: 1334 out of 30\n",
      "Epoch: 1763/2000... Step: 1763... Loss: 0.00106... Val Loss: 0.00395\n",
      "EarlyStopping counter: 1335 out of 30\n",
      "Epoch: 1764/2000... Step: 1764... Loss: 0.00135... Val Loss: 0.00334\n",
      "EarlyStopping counter: 1336 out of 30\n",
      "Epoch: 1765/2000... Step: 1765... Loss: 0.00166... Val Loss: 0.00449\n",
      "EarlyStopping counter: 1337 out of 30\n",
      "Epoch: 1766/2000... Step: 1766... Loss: 0.00197... Val Loss: 0.00385\n",
      "EarlyStopping counter: 1338 out of 30\n",
      "Epoch: 1767/2000... Step: 1767... Loss: 0.00195... Val Loss: 0.00400\n",
      "EarlyStopping counter: 1339 out of 30\n",
      "Epoch: 1768/2000... Step: 1768... Loss: 0.00172... Val Loss: 0.00295\n",
      "EarlyStopping counter: 1340 out of 30\n",
      "Epoch: 1769/2000... Step: 1769... Loss: 0.00106... Val Loss: 0.00264\n",
      "EarlyStopping counter: 1341 out of 30\n",
      "Epoch: 1770/2000... Step: 1770... Loss: 0.00064... Val Loss: 0.00234\n",
      "EarlyStopping counter: 1342 out of 30\n",
      "Epoch: 1771/2000... Step: 1771... Loss: 0.00038... Val Loss: 0.00236\n",
      "EarlyStopping counter: 1343 out of 30\n",
      "Epoch: 1772/2000... Step: 1772... Loss: 0.00038... Val Loss: 0.00272\n",
      "EarlyStopping counter: 1344 out of 30\n",
      "Epoch: 1773/2000... Step: 1773... Loss: 0.00049... Val Loss: 0.00249\n",
      "EarlyStopping counter: 1345 out of 30\n",
      "Epoch: 1774/2000... Step: 1774... Loss: 0.00069... Val Loss: 0.00308\n",
      "EarlyStopping counter: 1346 out of 30\n",
      "Epoch: 1775/2000... Step: 1775... Loss: 0.00089... Val Loss: 0.00253\n",
      "EarlyStopping counter: 1347 out of 30\n",
      "Epoch: 1776/2000... Step: 1776... Loss: 0.00086... Val Loss: 0.00281\n",
      "EarlyStopping counter: 1348 out of 30\n",
      "Epoch: 1777/2000... Step: 1777... Loss: 0.00076... Val Loss: 0.00231\n",
      "EarlyStopping counter: 1349 out of 30\n",
      "Epoch: 1778/2000... Step: 1778... Loss: 0.00049... Val Loss: 0.00230\n",
      "EarlyStopping counter: 1350 out of 30\n",
      "Epoch: 1779/2000... Step: 1779... Loss: 0.00024... Val Loss: 0.00238\n",
      "EarlyStopping counter: 1351 out of 30\n",
      "Epoch: 1780/2000... Step: 1780... Loss: 0.00021... Val Loss: 0.00221\n",
      "EarlyStopping counter: 1352 out of 30\n",
      "Epoch: 1781/2000... Step: 1781... Loss: 0.00031... Val Loss: 0.00261\n",
      "EarlyStopping counter: 1353 out of 30\n",
      "Epoch: 1782/2000... Step: 1782... Loss: 0.00038... Val Loss: 0.00238\n",
      "EarlyStopping counter: 1354 out of 30\n",
      "Epoch: 1783/2000... Step: 1783... Loss: 0.00038... Val Loss: 0.00261\n",
      "EarlyStopping counter: 1355 out of 30\n",
      "Epoch: 1784/2000... Step: 1784... Loss: 0.00041... Val Loss: 0.00249\n",
      "EarlyStopping counter: 1356 out of 30\n",
      "Epoch: 1785/2000... Step: 1785... Loss: 0.00040... Val Loss: 0.00243\n",
      "EarlyStopping counter: 1357 out of 30\n",
      "Epoch: 1786/2000... Step: 1786... Loss: 0.00031... Val Loss: 0.00231\n",
      "EarlyStopping counter: 1358 out of 30\n",
      "Epoch: 1787/2000... Step: 1787... Loss: 0.00021... Val Loss: 0.00237\n",
      "EarlyStopping counter: 1359 out of 30\n",
      "Epoch: 1788/2000... Step: 1788... Loss: 0.00018... Val Loss: 0.00225\n",
      "EarlyStopping counter: 1360 out of 30\n",
      "Epoch: 1789/2000... Step: 1789... Loss: 0.00020... Val Loss: 0.00232\n",
      "EarlyStopping counter: 1361 out of 30\n",
      "Epoch: 1790/2000... Step: 1790... Loss: 0.00020... Val Loss: 0.00233\n",
      "EarlyStopping counter: 1362 out of 30\n",
      "Epoch: 1791/2000... Step: 1791... Loss: 0.00019... Val Loss: 0.00213\n",
      "EarlyStopping counter: 1363 out of 30\n",
      "Epoch: 1792/2000... Step: 1792... Loss: 0.00023... Val Loss: 0.00249\n",
      "EarlyStopping counter: 1364 out of 30\n",
      "Epoch: 1793/2000... Step: 1793... Loss: 0.00028... Val Loss: 0.00217\n",
      "EarlyStopping counter: 1365 out of 30\n",
      "Epoch: 1794/2000... Step: 1794... Loss: 0.00030... Val Loss: 0.00249\n",
      "EarlyStopping counter: 1366 out of 30\n",
      "Epoch: 1795/2000... Step: 1795... Loss: 0.00029... Val Loss: 0.00231\n",
      "EarlyStopping counter: 1367 out of 30\n",
      "Epoch: 1796/2000... Step: 1796... Loss: 0.00030... Val Loss: 0.00246\n",
      "EarlyStopping counter: 1368 out of 30\n",
      "Epoch: 1797/2000... Step: 1797... Loss: 0.00032... Val Loss: 0.00236\n",
      "EarlyStopping counter: 1369 out of 30\n",
      "Epoch: 1798/2000... Step: 1798... Loss: 0.00032... Val Loss: 0.00242\n",
      "EarlyStopping counter: 1370 out of 30\n",
      "Epoch: 1799/2000... Step: 1799... Loss: 0.00030... Val Loss: 0.00222\n",
      "EarlyStopping counter: 1371 out of 30\n",
      "Epoch: 1800/2000... Step: 1800... Loss: 0.00027... Val Loss: 0.00245\n",
      "EarlyStopping counter: 1372 out of 30\n",
      "Epoch: 1801/2000... Step: 1801... Loss: 0.00025... Val Loss: 0.00224\n",
      "EarlyStopping counter: 1373 out of 30\n",
      "Epoch: 1802/2000... Step: 1802... Loss: 0.00023... Val Loss: 0.00237\n",
      "EarlyStopping counter: 1374 out of 30\n",
      "Epoch: 1803/2000... Step: 1803... Loss: 0.00019... Val Loss: 0.00222\n",
      "EarlyStopping counter: 1375 out of 30\n",
      "Epoch: 1804/2000... Step: 1804... Loss: 0.00017... Val Loss: 0.00230\n",
      "EarlyStopping counter: 1376 out of 30\n",
      "Epoch: 1805/2000... Step: 1805... Loss: 0.00016... Val Loss: 0.00224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1377 out of 30\n",
      "Epoch: 1806/2000... Step: 1806... Loss: 0.00016... Val Loss: 0.00230\n",
      "EarlyStopping counter: 1378 out of 30\n",
      "Epoch: 1807/2000... Step: 1807... Loss: 0.00015... Val Loss: 0.00220\n",
      "EarlyStopping counter: 1379 out of 30\n",
      "Epoch: 1808/2000... Step: 1808... Loss: 0.00015... Val Loss: 0.00236\n",
      "EarlyStopping counter: 1380 out of 30\n",
      "Epoch: 1809/2000... Step: 1809... Loss: 0.00016... Val Loss: 0.00219\n",
      "EarlyStopping counter: 1381 out of 30\n",
      "Epoch: 1810/2000... Step: 1810... Loss: 0.00017... Val Loss: 0.00239\n",
      "EarlyStopping counter: 1382 out of 30\n",
      "Epoch: 1811/2000... Step: 1811... Loss: 0.00017... Val Loss: 0.00222\n",
      "EarlyStopping counter: 1383 out of 30\n",
      "Epoch: 1812/2000... Step: 1812... Loss: 0.00017... Val Loss: 0.00233\n",
      "EarlyStopping counter: 1384 out of 30\n",
      "Epoch: 1813/2000... Step: 1813... Loss: 0.00018... Val Loss: 0.00230\n",
      "EarlyStopping counter: 1385 out of 30\n",
      "Epoch: 1814/2000... Step: 1814... Loss: 0.00018... Val Loss: 0.00225\n",
      "EarlyStopping counter: 1386 out of 30\n",
      "Epoch: 1815/2000... Step: 1815... Loss: 0.00019... Val Loss: 0.00235\n",
      "EarlyStopping counter: 1387 out of 30\n",
      "Epoch: 1816/2000... Step: 1816... Loss: 0.00020... Val Loss: 0.00225\n",
      "EarlyStopping counter: 1388 out of 30\n",
      "Epoch: 1817/2000... Step: 1817... Loss: 0.00022... Val Loss: 0.00243\n",
      "EarlyStopping counter: 1389 out of 30\n",
      "Epoch: 1818/2000... Step: 1818... Loss: 0.00026... Val Loss: 0.00227\n",
      "EarlyStopping counter: 1390 out of 30\n",
      "Epoch: 1819/2000... Step: 1819... Loss: 0.00031... Val Loss: 0.00263\n",
      "EarlyStopping counter: 1391 out of 30\n",
      "Epoch: 1820/2000... Step: 1820... Loss: 0.00039... Val Loss: 0.00241\n",
      "EarlyStopping counter: 1392 out of 30\n",
      "Epoch: 1821/2000... Step: 1821... Loss: 0.00049... Val Loss: 0.00305\n",
      "EarlyStopping counter: 1393 out of 30\n",
      "Epoch: 1822/2000... Step: 1822... Loss: 0.00069... Val Loss: 0.00273\n",
      "EarlyStopping counter: 1394 out of 30\n",
      "Epoch: 1823/2000... Step: 1823... Loss: 0.00095... Val Loss: 0.00391\n",
      "EarlyStopping counter: 1395 out of 30\n",
      "Epoch: 1824/2000... Step: 1824... Loss: 0.00139... Val Loss: 0.00357\n",
      "EarlyStopping counter: 1396 out of 30\n",
      "Epoch: 1825/2000... Step: 1825... Loss: 0.00192... Val Loss: 0.00541\n",
      "EarlyStopping counter: 1397 out of 30\n",
      "Epoch: 1826/2000... Step: 1826... Loss: 0.00286... Val Loss: 0.00502\n",
      "EarlyStopping counter: 1398 out of 30\n",
      "Epoch: 1827/2000... Step: 1827... Loss: 0.00351... Val Loss: 0.00690\n",
      "EarlyStopping counter: 1399 out of 30\n",
      "Epoch: 1828/2000... Step: 1828... Loss: 0.00456... Val Loss: 0.00520\n",
      "EarlyStopping counter: 1400 out of 30\n",
      "Epoch: 1829/2000... Step: 1829... Loss: 0.00369... Val Loss: 0.00489\n",
      "EarlyStopping counter: 1401 out of 30\n",
      "Epoch: 1830/2000... Step: 1830... Loss: 0.00287... Val Loss: 0.00313\n",
      "EarlyStopping counter: 1402 out of 30\n",
      "Epoch: 1831/2000... Step: 1831... Loss: 0.00172... Val Loss: 0.00409\n",
      "EarlyStopping counter: 1403 out of 30\n",
      "Epoch: 1832/2000... Step: 1832... Loss: 0.00181... Val Loss: 0.00434\n",
      "EarlyStopping counter: 1404 out of 30\n",
      "Epoch: 1833/2000... Step: 1833... Loss: 0.00278... Val Loss: 0.00577\n",
      "EarlyStopping counter: 1405 out of 30\n",
      "Epoch: 1834/2000... Step: 1834... Loss: 0.00347... Val Loss: 0.00498\n",
      "EarlyStopping counter: 1406 out of 30\n",
      "Epoch: 1835/2000... Step: 1835... Loss: 0.00298... Val Loss: 0.00390\n",
      "EarlyStopping counter: 1407 out of 30\n",
      "Epoch: 1836/2000... Step: 1836... Loss: 0.00113... Val Loss: 0.00234\n",
      "EarlyStopping counter: 1408 out of 30\n",
      "Epoch: 1837/2000... Step: 1837... Loss: 0.00067... Val Loss: 0.00309\n",
      "EarlyStopping counter: 1409 out of 30\n",
      "Epoch: 1838/2000... Step: 1838... Loss: 0.00136... Val Loss: 0.00413\n",
      "EarlyStopping counter: 1410 out of 30\n",
      "Epoch: 1839/2000... Step: 1839... Loss: 0.00195... Val Loss: 0.00324\n",
      "EarlyStopping counter: 1411 out of 30\n",
      "Epoch: 1840/2000... Step: 1840... Loss: 0.00148... Val Loss: 0.00276\n",
      "EarlyStopping counter: 1412 out of 30\n",
      "Epoch: 1841/2000... Step: 1841... Loss: 0.00059... Val Loss: 0.00248\n",
      "EarlyStopping counter: 1413 out of 30\n",
      "Epoch: 1842/2000... Step: 1842... Loss: 0.00047... Val Loss: 0.00291\n",
      "EarlyStopping counter: 1414 out of 30\n",
      "Epoch: 1843/2000... Step: 1843... Loss: 0.00097... Val Loss: 0.00319\n",
      "EarlyStopping counter: 1415 out of 30\n",
      "Epoch: 1844/2000... Step: 1844... Loss: 0.00130... Val Loss: 0.00343\n",
      "EarlyStopping counter: 1416 out of 30\n",
      "Epoch: 1845/2000... Step: 1845... Loss: 0.00103... Val Loss: 0.00235\n",
      "EarlyStopping counter: 1417 out of 30\n",
      "Epoch: 1846/2000... Step: 1846... Loss: 0.00059... Val Loss: 0.00294\n",
      "EarlyStopping counter: 1418 out of 30\n",
      "Epoch: 1847/2000... Step: 1847... Loss: 0.00056... Val Loss: 0.00243\n",
      "EarlyStopping counter: 1419 out of 30\n",
      "Epoch: 1848/2000... Step: 1848... Loss: 0.00080... Val Loss: 0.00309\n",
      "EarlyStopping counter: 1420 out of 30\n",
      "Epoch: 1849/2000... Step: 1849... Loss: 0.00087... Val Loss: 0.00284\n",
      "EarlyStopping counter: 1421 out of 30\n",
      "Epoch: 1850/2000... Step: 1850... Loss: 0.00071... Val Loss: 0.00230\n",
      "EarlyStopping counter: 1422 out of 30\n",
      "Epoch: 1851/2000... Step: 1851... Loss: 0.00054... Val Loss: 0.00321\n",
      "EarlyStopping counter: 1423 out of 30\n",
      "Epoch: 1852/2000... Step: 1852... Loss: 0.00063... Val Loss: 0.00231\n",
      "EarlyStopping counter: 1424 out of 30\n",
      "Epoch: 1853/2000... Step: 1853... Loss: 0.00073... Val Loss: 0.00285\n",
      "EarlyStopping counter: 1425 out of 30\n",
      "Epoch: 1854/2000... Step: 1854... Loss: 0.00055... Val Loss: 0.00233\n",
      "EarlyStopping counter: 1426 out of 30\n",
      "Epoch: 1855/2000... Step: 1855... Loss: 0.00032... Val Loss: 0.00211\n",
      "EarlyStopping counter: 1427 out of 30\n",
      "Epoch: 1856/2000... Step: 1856... Loss: 0.00041... Val Loss: 0.00313\n",
      "EarlyStopping counter: 1428 out of 30\n",
      "Epoch: 1857/2000... Step: 1857... Loss: 0.00062... Val Loss: 0.00216\n",
      "EarlyStopping counter: 1429 out of 30\n",
      "Epoch: 1858/2000... Step: 1858... Loss: 0.00067... Val Loss: 0.00296\n",
      "EarlyStopping counter: 1430 out of 30\n",
      "Epoch: 1859/2000... Step: 1859... Loss: 0.00056... Val Loss: 0.00217\n",
      "EarlyStopping counter: 1431 out of 30\n",
      "Epoch: 1860/2000... Step: 1860... Loss: 0.00036... Val Loss: 0.00233\n",
      "EarlyStopping counter: 1432 out of 30\n",
      "Epoch: 1861/2000... Step: 1861... Loss: 0.00038... Val Loss: 0.00270\n",
      "EarlyStopping counter: 1433 out of 30\n",
      "Epoch: 1862/2000... Step: 1862... Loss: 0.00040... Val Loss: 0.00202\n",
      "EarlyStopping counter: 1434 out of 30\n",
      "Epoch: 1863/2000... Step: 1863... Loss: 0.00032... Val Loss: 0.00246\n",
      "EarlyStopping counter: 1435 out of 30\n",
      "Epoch: 1864/2000... Step: 1864... Loss: 0.00031... Val Loss: 0.00235\n",
      "EarlyStopping counter: 1436 out of 30\n",
      "Epoch: 1865/2000... Step: 1865... Loss: 0.00038... Val Loss: 0.00234\n",
      "EarlyStopping counter: 1437 out of 30\n",
      "Epoch: 1866/2000... Step: 1866... Loss: 0.00045... Val Loss: 0.00231\n",
      "EarlyStopping counter: 1438 out of 30\n",
      "Epoch: 1867/2000... Step: 1867... Loss: 0.00031... Val Loss: 0.00220\n",
      "EarlyStopping counter: 1439 out of 30\n",
      "Epoch: 1868/2000... Step: 1868... Loss: 0.00020... Val Loss: 0.00210\n",
      "EarlyStopping counter: 1440 out of 30\n",
      "Epoch: 1869/2000... Step: 1869... Loss: 0.00025... Val Loss: 0.00238\n",
      "EarlyStopping counter: 1441 out of 30\n",
      "Epoch: 1870/2000... Step: 1870... Loss: 0.00029... Val Loss: 0.00223\n",
      "EarlyStopping counter: 1442 out of 30\n",
      "Epoch: 1871/2000... Step: 1871... Loss: 0.00028... Val Loss: 0.00210\n",
      "EarlyStopping counter: 1443 out of 30\n",
      "Epoch: 1872/2000... Step: 1872... Loss: 0.00022... Val Loss: 0.00239\n",
      "EarlyStopping counter: 1444 out of 30\n",
      "Epoch: 1873/2000... Step: 1873... Loss: 0.00021... Val Loss: 0.00206\n",
      "EarlyStopping counter: 1445 out of 30\n",
      "Epoch: 1874/2000... Step: 1874... Loss: 0.00027... Val Loss: 0.00239\n",
      "EarlyStopping counter: 1446 out of 30\n",
      "Epoch: 1875/2000... Step: 1875... Loss: 0.00026... Val Loss: 0.00211\n",
      "EarlyStopping counter: 1447 out of 30\n",
      "Epoch: 1876/2000... Step: 1876... Loss: 0.00022... Val Loss: 0.00223\n",
      "EarlyStopping counter: 1448 out of 30\n",
      "Epoch: 1877/2000... Step: 1877... Loss: 0.00019... Val Loss: 0.00230\n",
      "EarlyStopping counter: 1449 out of 30\n",
      "Epoch: 1878/2000... Step: 1878... Loss: 0.00020... Val Loss: 0.00214\n",
      "EarlyStopping counter: 1450 out of 30\n",
      "Epoch: 1879/2000... Step: 1879... Loss: 0.00021... Val Loss: 0.00235\n",
      "EarlyStopping counter: 1451 out of 30\n",
      "Epoch: 1880/2000... Step: 1880... Loss: 0.00018... Val Loss: 0.00216\n",
      "EarlyStopping counter: 1452 out of 30\n",
      "Epoch: 1881/2000... Step: 1881... Loss: 0.00015... Val Loss: 0.00212\n",
      "EarlyStopping counter: 1453 out of 30\n",
      "Epoch: 1882/2000... Step: 1882... Loss: 0.00017... Val Loss: 0.00233\n",
      "EarlyStopping counter: 1454 out of 30\n",
      "Epoch: 1883/2000... Step: 1883... Loss: 0.00019... Val Loss: 0.00215\n",
      "EarlyStopping counter: 1455 out of 30\n",
      "Epoch: 1884/2000... Step: 1884... Loss: 0.00019... Val Loss: 0.00219\n",
      "EarlyStopping counter: 1456 out of 30\n",
      "Epoch: 1885/2000... Step: 1885... Loss: 0.00017... Val Loss: 0.00229\n",
      "EarlyStopping counter: 1457 out of 30\n",
      "Epoch: 1886/2000... Step: 1886... Loss: 0.00016... Val Loss: 0.00211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1458 out of 30\n",
      "Epoch: 1887/2000... Step: 1887... Loss: 0.00017... Val Loss: 0.00244\n",
      "EarlyStopping counter: 1459 out of 30\n",
      "Epoch: 1888/2000... Step: 1888... Loss: 0.00018... Val Loss: 0.00206\n",
      "EarlyStopping counter: 1460 out of 30\n",
      "Epoch: 1889/2000... Step: 1889... Loss: 0.00018... Val Loss: 0.00229\n",
      "EarlyStopping counter: 1461 out of 30\n",
      "Epoch: 1890/2000... Step: 1890... Loss: 0.00016... Val Loss: 0.00213\n",
      "EarlyStopping counter: 1462 out of 30\n",
      "Epoch: 1891/2000... Step: 1891... Loss: 0.00016... Val Loss: 0.00220\n",
      "EarlyStopping counter: 1463 out of 30\n",
      "Epoch: 1892/2000... Step: 1892... Loss: 0.00016... Val Loss: 0.00225\n",
      "EarlyStopping counter: 1464 out of 30\n",
      "Epoch: 1893/2000... Step: 1893... Loss: 0.00016... Val Loss: 0.00222\n",
      "EarlyStopping counter: 1465 out of 30\n",
      "Epoch: 1894/2000... Step: 1894... Loss: 0.00016... Val Loss: 0.00216\n",
      "EarlyStopping counter: 1466 out of 30\n",
      "Epoch: 1895/2000... Step: 1895... Loss: 0.00015... Val Loss: 0.00229\n",
      "EarlyStopping counter: 1467 out of 30\n",
      "Epoch: 1896/2000... Step: 1896... Loss: 0.00015... Val Loss: 0.00211\n",
      "EarlyStopping counter: 1468 out of 30\n",
      "Epoch: 1897/2000... Step: 1897... Loss: 0.00016... Val Loss: 0.00235\n",
      "EarlyStopping counter: 1469 out of 30\n",
      "Epoch: 1898/2000... Step: 1898... Loss: 0.00016... Val Loss: 0.00212\n",
      "EarlyStopping counter: 1470 out of 30\n",
      "Epoch: 1899/2000... Step: 1899... Loss: 0.00016... Val Loss: 0.00229\n",
      "EarlyStopping counter: 1471 out of 30\n",
      "Epoch: 1900/2000... Step: 1900... Loss: 0.00015... Val Loss: 0.00220\n",
      "EarlyStopping counter: 1472 out of 30\n",
      "Epoch: 1901/2000... Step: 1901... Loss: 0.00016... Val Loss: 0.00229\n",
      "EarlyStopping counter: 1473 out of 30\n",
      "Epoch: 1902/2000... Step: 1902... Loss: 0.00018... Val Loss: 0.00229\n",
      "EarlyStopping counter: 1474 out of 30\n",
      "Epoch: 1903/2000... Step: 1903... Loss: 0.00021... Val Loss: 0.00234\n",
      "EarlyStopping counter: 1475 out of 30\n",
      "Epoch: 1904/2000... Step: 1904... Loss: 0.00024... Val Loss: 0.00223\n",
      "EarlyStopping counter: 1476 out of 30\n",
      "Epoch: 1905/2000... Step: 1905... Loss: 0.00028... Val Loss: 0.00258\n",
      "EarlyStopping counter: 1477 out of 30\n",
      "Epoch: 1906/2000... Step: 1906... Loss: 0.00035... Val Loss: 0.00226\n",
      "EarlyStopping counter: 1478 out of 30\n",
      "Epoch: 1907/2000... Step: 1907... Loss: 0.00045... Val Loss: 0.00295\n",
      "EarlyStopping counter: 1479 out of 30\n",
      "Epoch: 1908/2000... Step: 1908... Loss: 0.00058... Val Loss: 0.00248\n",
      "EarlyStopping counter: 1480 out of 30\n",
      "Epoch: 1909/2000... Step: 1909... Loss: 0.00077... Val Loss: 0.00348\n",
      "EarlyStopping counter: 1481 out of 30\n",
      "Epoch: 1910/2000... Step: 1910... Loss: 0.00100... Val Loss: 0.00295\n",
      "EarlyStopping counter: 1482 out of 30\n",
      "Epoch: 1911/2000... Step: 1911... Loss: 0.00126... Val Loss: 0.00417\n",
      "EarlyStopping counter: 1483 out of 30\n",
      "Epoch: 1912/2000... Step: 1912... Loss: 0.00158... Val Loss: 0.00340\n",
      "EarlyStopping counter: 1484 out of 30\n",
      "Epoch: 1913/2000... Step: 1913... Loss: 0.00180... Val Loss: 0.00453\n",
      "EarlyStopping counter: 1485 out of 30\n",
      "Epoch: 1914/2000... Step: 1914... Loss: 0.00200... Val Loss: 0.00326\n",
      "EarlyStopping counter: 1486 out of 30\n",
      "Epoch: 1915/2000... Step: 1915... Loss: 0.00182... Val Loss: 0.00389\n",
      "EarlyStopping counter: 1487 out of 30\n",
      "Epoch: 1916/2000... Step: 1916... Loss: 0.00142... Val Loss: 0.00242\n",
      "EarlyStopping counter: 1488 out of 30\n",
      "Epoch: 1917/2000... Step: 1917... Loss: 0.00090... Val Loss: 0.00274\n",
      "EarlyStopping counter: 1489 out of 30\n",
      "Epoch: 1918/2000... Step: 1918... Loss: 0.00052... Val Loss: 0.00234\n",
      "EarlyStopping counter: 1490 out of 30\n",
      "Epoch: 1919/2000... Step: 1919... Loss: 0.00045... Val Loss: 0.00258\n",
      "EarlyStopping counter: 1491 out of 30\n",
      "Epoch: 1920/2000... Step: 1920... Loss: 0.00065... Val Loss: 0.00323\n",
      "EarlyStopping counter: 1492 out of 30\n",
      "Epoch: 1921/2000... Step: 1921... Loss: 0.00089... Val Loss: 0.00278\n",
      "EarlyStopping counter: 1493 out of 30\n",
      "Epoch: 1922/2000... Step: 1922... Loss: 0.00094... Val Loss: 0.00329\n",
      "EarlyStopping counter: 1494 out of 30\n",
      "Epoch: 1923/2000... Step: 1923... Loss: 0.00084... Val Loss: 0.00239\n",
      "EarlyStopping counter: 1495 out of 30\n",
      "Epoch: 1924/2000... Step: 1924... Loss: 0.00066... Val Loss: 0.00269\n",
      "EarlyStopping counter: 1496 out of 30\n",
      "Epoch: 1925/2000... Step: 1925... Loss: 0.00042... Val Loss: 0.00222\n",
      "EarlyStopping counter: 1497 out of 30\n",
      "Epoch: 1926/2000... Step: 1926... Loss: 0.00032... Val Loss: 0.00233\n",
      "EarlyStopping counter: 1498 out of 30\n",
      "Epoch: 1927/2000... Step: 1927... Loss: 0.00035... Val Loss: 0.00273\n",
      "EarlyStopping counter: 1499 out of 30\n",
      "Epoch: 1928/2000... Step: 1928... Loss: 0.00047... Val Loss: 0.00246\n",
      "EarlyStopping counter: 1500 out of 30\n",
      "Epoch: 1929/2000... Step: 1929... Loss: 0.00056... Val Loss: 0.00313\n",
      "EarlyStopping counter: 1501 out of 30\n",
      "Epoch: 1930/2000... Step: 1930... Loss: 0.00056... Val Loss: 0.00224\n",
      "EarlyStopping counter: 1502 out of 30\n",
      "Epoch: 1931/2000... Step: 1931... Loss: 0.00049... Val Loss: 0.00267\n",
      "EarlyStopping counter: 1503 out of 30\n",
      "Epoch: 1932/2000... Step: 1932... Loss: 0.00035... Val Loss: 0.00222\n",
      "EarlyStopping counter: 1504 out of 30\n",
      "Epoch: 1933/2000... Step: 1933... Loss: 0.00026... Val Loss: 0.00224\n",
      "EarlyStopping counter: 1505 out of 30\n",
      "Epoch: 1934/2000... Step: 1934... Loss: 0.00026... Val Loss: 0.00262\n",
      "EarlyStopping counter: 1506 out of 30\n",
      "Epoch: 1935/2000... Step: 1935... Loss: 0.00032... Val Loss: 0.00218\n",
      "EarlyStopping counter: 1507 out of 30\n",
      "Epoch: 1936/2000... Step: 1936... Loss: 0.00031... Val Loss: 0.00259\n",
      "EarlyStopping counter: 1508 out of 30\n",
      "Epoch: 1937/2000... Step: 1937... Loss: 0.00027... Val Loss: 0.00216\n",
      "EarlyStopping counter: 1509 out of 30\n",
      "Epoch: 1938/2000... Step: 1938... Loss: 0.00023... Val Loss: 0.00236\n",
      "EarlyStopping counter: 1510 out of 30\n",
      "Epoch: 1939/2000... Step: 1939... Loss: 0.00025... Val Loss: 0.00240\n",
      "EarlyStopping counter: 1511 out of 30\n",
      "Epoch: 1940/2000... Step: 1940... Loss: 0.00032... Val Loss: 0.00229\n",
      "EarlyStopping counter: 1512 out of 30\n",
      "Epoch: 1941/2000... Step: 1941... Loss: 0.00036... Val Loss: 0.00259\n",
      "EarlyStopping counter: 1513 out of 30\n",
      "Epoch: 1942/2000... Step: 1942... Loss: 0.00035... Val Loss: 0.00223\n",
      "EarlyStopping counter: 1514 out of 30\n",
      "Epoch: 1943/2000... Step: 1943... Loss: 0.00031... Val Loss: 0.00242\n",
      "EarlyStopping counter: 1515 out of 30\n",
      "Epoch: 1944/2000... Step: 1944... Loss: 0.00025... Val Loss: 0.00226\n",
      "EarlyStopping counter: 1516 out of 30\n",
      "Epoch: 1945/2000... Step: 1945... Loss: 0.00020... Val Loss: 0.00215\n",
      "EarlyStopping counter: 1517 out of 30\n",
      "Epoch: 1946/2000... Step: 1946... Loss: 0.00020... Val Loss: 0.00250\n",
      "EarlyStopping counter: 1518 out of 30\n",
      "Epoch: 1947/2000... Step: 1947... Loss: 0.00021... Val Loss: 0.00213\n",
      "EarlyStopping counter: 1519 out of 30\n",
      "Epoch: 1948/2000... Step: 1948... Loss: 0.00020... Val Loss: 0.00244\n",
      "EarlyStopping counter: 1520 out of 30\n",
      "Epoch: 1949/2000... Step: 1949... Loss: 0.00018... Val Loss: 0.00224\n",
      "EarlyStopping counter: 1521 out of 30\n",
      "Epoch: 1950/2000... Step: 1950... Loss: 0.00015... Val Loss: 0.00226\n",
      "EarlyStopping counter: 1522 out of 30\n",
      "Epoch: 1951/2000... Step: 1951... Loss: 0.00015... Val Loss: 0.00238\n",
      "EarlyStopping counter: 1523 out of 30\n",
      "Epoch: 1952/2000... Step: 1952... Loss: 0.00016... Val Loss: 0.00214\n",
      "EarlyStopping counter: 1524 out of 30\n",
      "Epoch: 1953/2000... Step: 1953... Loss: 0.00018... Val Loss: 0.00246\n",
      "EarlyStopping counter: 1525 out of 30\n",
      "Epoch: 1954/2000... Step: 1954... Loss: 0.00018... Val Loss: 0.00215\n",
      "EarlyStopping counter: 1526 out of 30\n",
      "Epoch: 1955/2000... Step: 1955... Loss: 0.00017... Val Loss: 0.00237\n",
      "EarlyStopping counter: 1527 out of 30\n",
      "Epoch: 1956/2000... Step: 1956... Loss: 0.00016... Val Loss: 0.00228\n",
      "EarlyStopping counter: 1528 out of 30\n",
      "Epoch: 1957/2000... Step: 1957... Loss: 0.00017... Val Loss: 0.00231\n",
      "EarlyStopping counter: 1529 out of 30\n",
      "Epoch: 1958/2000... Step: 1958... Loss: 0.00018... Val Loss: 0.00237\n",
      "EarlyStopping counter: 1530 out of 30\n",
      "Epoch: 1959/2000... Step: 1959... Loss: 0.00020... Val Loss: 0.00229\n",
      "EarlyStopping counter: 1531 out of 30\n",
      "Epoch: 1960/2000... Step: 1960... Loss: 0.00021... Val Loss: 0.00234\n",
      "EarlyStopping counter: 1532 out of 30\n",
      "Epoch: 1961/2000... Step: 1961... Loss: 0.00022... Val Loss: 0.00242\n",
      "EarlyStopping counter: 1533 out of 30\n",
      "Epoch: 1962/2000... Step: 1962... Loss: 0.00024... Val Loss: 0.00223\n",
      "EarlyStopping counter: 1534 out of 30\n",
      "Epoch: 1963/2000... Step: 1963... Loss: 0.00029... Val Loss: 0.00268\n",
      "EarlyStopping counter: 1535 out of 30\n",
      "Epoch: 1964/2000... Step: 1964... Loss: 0.00036... Val Loss: 0.00234\n",
      "EarlyStopping counter: 1536 out of 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1965/2000... Step: 1965... Loss: 0.00045... Val Loss: 0.00293\n",
      "EarlyStopping counter: 1537 out of 30\n",
      "Epoch: 1966/2000... Step: 1966... Loss: 0.00056... Val Loss: 0.00262\n",
      "EarlyStopping counter: 1538 out of 30\n",
      "Epoch: 1967/2000... Step: 1967... Loss: 0.00072... Val Loss: 0.00326\n",
      "EarlyStopping counter: 1539 out of 30\n",
      "Epoch: 1968/2000... Step: 1968... Loss: 0.00091... Val Loss: 0.00309\n",
      "EarlyStopping counter: 1540 out of 30\n",
      "Epoch: 1969/2000... Step: 1969... Loss: 0.00118... Val Loss: 0.00385\n",
      "EarlyStopping counter: 1541 out of 30\n",
      "Epoch: 1970/2000... Step: 1970... Loss: 0.00153... Val Loss: 0.00386\n",
      "EarlyStopping counter: 1542 out of 30\n",
      "Epoch: 1971/2000... Step: 1971... Loss: 0.00181... Val Loss: 0.00455\n",
      "EarlyStopping counter: 1543 out of 30\n",
      "Epoch: 1972/2000... Step: 1972... Loss: 0.00226... Val Loss: 0.00434\n",
      "EarlyStopping counter: 1544 out of 30\n",
      "Epoch: 1973/2000... Step: 1973... Loss: 0.00220... Val Loss: 0.00415\n",
      "EarlyStopping counter: 1545 out of 30\n",
      "Epoch: 1974/2000... Step: 1974... Loss: 0.00202... Val Loss: 0.00339\n",
      "EarlyStopping counter: 1546 out of 30\n",
      "Epoch: 1975/2000... Step: 1975... Loss: 0.00123... Val Loss: 0.00262\n",
      "EarlyStopping counter: 1547 out of 30\n",
      "Epoch: 1976/2000... Step: 1976... Loss: 0.00051... Val Loss: 0.00221\n",
      "EarlyStopping counter: 1548 out of 30\n",
      "Epoch: 1977/2000... Step: 1977... Loss: 0.00020... Val Loss: 0.00267\n",
      "EarlyStopping counter: 1549 out of 30\n",
      "Epoch: 1978/2000... Step: 1978... Loss: 0.00045... Val Loss: 0.00293\n",
      "EarlyStopping counter: 1550 out of 30\n",
      "Epoch: 1979/2000... Step: 1979... Loss: 0.00090... Val Loss: 0.00318\n",
      "EarlyStopping counter: 1551 out of 30\n",
      "Epoch: 1980/2000... Step: 1980... Loss: 0.00113... Val Loss: 0.00321\n",
      "EarlyStopping counter: 1552 out of 30\n",
      "Epoch: 1981/2000... Step: 1981... Loss: 0.00103... Val Loss: 0.00265\n",
      "EarlyStopping counter: 1553 out of 30\n",
      "Epoch: 1982/2000... Step: 1982... Loss: 0.00065... Val Loss: 0.00256\n",
      "EarlyStopping counter: 1554 out of 30\n",
      "Epoch: 1983/2000... Step: 1983... Loss: 0.00030... Val Loss: 0.00217\n",
      "EarlyStopping counter: 1555 out of 30\n",
      "Epoch: 1984/2000... Step: 1984... Loss: 0.00027... Val Loss: 0.00278\n",
      "EarlyStopping counter: 1556 out of 30\n",
      "Epoch: 1985/2000... Step: 1985... Loss: 0.00049... Val Loss: 0.00264\n",
      "EarlyStopping counter: 1557 out of 30\n",
      "Epoch: 1986/2000... Step: 1986... Loss: 0.00066... Val Loss: 0.00292\n",
      "EarlyStopping counter: 1558 out of 30\n",
      "Epoch: 1987/2000... Step: 1987... Loss: 0.00067... Val Loss: 0.00265\n",
      "EarlyStopping counter: 1559 out of 30\n",
      "Epoch: 1988/2000... Step: 1988... Loss: 0.00052... Val Loss: 0.00234\n",
      "EarlyStopping counter: 1560 out of 30\n",
      "Epoch: 1989/2000... Step: 1989... Loss: 0.00033... Val Loss: 0.00253\n",
      "EarlyStopping counter: 1561 out of 30\n",
      "Epoch: 1990/2000... Step: 1990... Loss: 0.00025... Val Loss: 0.00209\n",
      "EarlyStopping counter: 1562 out of 30\n",
      "Epoch: 1991/2000... Step: 1991... Loss: 0.00029... Val Loss: 0.00279\n",
      "EarlyStopping counter: 1563 out of 30\n",
      "Epoch: 1992/2000... Step: 1992... Loss: 0.00040... Val Loss: 0.00232\n",
      "EarlyStopping counter: 1564 out of 30\n",
      "Epoch: 1993/2000... Step: 1993... Loss: 0.00042... Val Loss: 0.00264\n",
      "EarlyStopping counter: 1565 out of 30\n",
      "Epoch: 1994/2000... Step: 1994... Loss: 0.00036... Val Loss: 0.00230\n",
      "EarlyStopping counter: 1566 out of 30\n",
      "Epoch: 1995/2000... Step: 1995... Loss: 0.00026... Val Loss: 0.00226\n",
      "EarlyStopping counter: 1567 out of 30\n",
      "Epoch: 1996/2000... Step: 1996... Loss: 0.00019... Val Loss: 0.00249\n",
      "EarlyStopping counter: 1568 out of 30\n",
      "Epoch: 1997/2000... Step: 1997... Loss: 0.00021... Val Loss: 0.00207\n",
      "EarlyStopping counter: 1569 out of 30\n",
      "Epoch: 1998/2000... Step: 1998... Loss: 0.00027... Val Loss: 0.00273\n",
      "EarlyStopping counter: 1570 out of 30\n",
      "Epoch: 1999/2000... Step: 1999... Loss: 0.00030... Val Loss: 0.00211\n",
      "EarlyStopping counter: 1571 out of 30\n",
      "Epoch: 2000/2000... Step: 2000... Loss: 0.00027... Val Loss: 0.00255\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "train_losess, val_losses = train(net, training_generator, validation_generator, verbose=True,\n",
    "                                                             opt_func=opt, criterion_func=criterion, epochs=num_epochs,\n",
    "                                                             lr=0.01, check_early_stopping=False, check_auc_roc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after train on the Test set:\n",
      "0.0021\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss after train on the Test set:\")\n",
    "print(infer(net, test_generator, nn.MSELoss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the plots:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_vs_epochs(train_losess, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Briefly explain graph's results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the test and the train are decreasing as we continue training the model, even after 2000 epochs.\n",
    "We can assume that the data is simple and the model understands how the data built, so it can also continue to learn and make progress on the test set. At the begining the model is learning very much from the data (the loss decreasing significantly) and after the model learning ~220 epochs, the net understands how the data generated and the loss is very low, both on the train set and the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type your answer here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How does your metric value differs between the training data and the test data and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.0002\n",
      "Test MSE: 0.0021\n"
     ]
    }
   ],
   "source": [
    "print('Train MSE: {}'.format(infer(net, training_generator, nn.MSELoss)))\n",
    "print('Test MSE: {}'.format(infer(net, test_generator, nn.MSELoss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type your answer here:**<br>\n",
    "The model performed on the train set better than test set. We can see that the train MSE is smaller as we expected. Part of the function is a simple deterministic with little noise (random * 0.1), so the model is learning good. Also, there is a component which is random, so the loss cannot be zero, however, it's very close."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
